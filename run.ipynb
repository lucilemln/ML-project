{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "try:\n",
    "    import importlib\n",
    "    importlib.reload(h)\n",
    "    importlib.reload(f)\n",
    "    importlib.reload(d)\n",
    "except NameError: # It hasn't been imported yet\n",
    "    import helpers as h\n",
    "    import implementations as f\n",
    "    import data_processing as d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing and feature selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#For this to work, the data folder needs to be one level above the project folder and the folder name needs\n",
    "#to be 'data'\n",
    "data_folder = '../data/'\n",
    "x_train, x_test, y_train, train_ids, test_ids = h.load_csv_data(data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = np.load(\"../data/x_train.npy\")\n",
    "x_test = np.load(\"../data/x_test.npy\")\n",
    "y_train = np.load(\"../data/y_train.npy\")\n",
    "train_ids = np.load(\"../data/trains_ids.npy\")\n",
    "test_ids = np.load(\"../data/test_ids.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#features_named all the features names and remove the ID column\n",
    "features_name = np.genfromtxt('../data/x_train.csv', delimiter=',', dtype=str, max_rows=1)[1:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "one paper on internet suggests to use these features : \n",
    "\n",
    " _RFHYPE5, TOLDHI2, _CHOLCHK, _BMI5, SMOKE100, CVDSTRK3, DIABETE3, _TOTINDA, _FRTLT1, _VEGLT1, _RFDRHV5, HLTHPLN1, MEDCOST, GENHLTH, MENTHLTH, PHYSHLTH, DIFFWALK, SEX, _AGEG5YR, EDUCA, and INCOME2\n",
    "\n",
    " then, iterating through them, it removes the missing values, made the data binary when possible, removed the 'don't know, not sure', and ordinal (categorical) variables ares changed to 0,1,2,..., and renamed them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a mask to get important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the important features\n",
    "features_list = ['_RFHYPE5', 'TOLDHI2', '_CHOLCHK', '_BMI5', 'SMOKE100', 'CVDSTRK3', 'DIABETE3', '_TOTINDA', '_FRTLT1', '_VEGLT1', '_RFDRHV5', \n",
    "                 'HLTHPLN1', 'MEDCOST', 'GENHLTH', 'MENTHLTH', 'PHYSHLTH', 'DIFFWALK', 'SEX', '_AGEG5YR', 'EDUCA', 'INCOME2']\n",
    "\n",
    "def masking(X, features_list):\n",
    "     #INPUT: X = (x_train, x_test), features_list: features wanted\n",
    "\n",
    "    #Create a mask to filter the data\n",
    "    mask = np.isin(features_name, features_list)\n",
    "    x_train, x_test = X\n",
    "\n",
    "    x_train_featured = x_train[:, mask]\n",
    "    x_test_featured = x_test[:, mask]\n",
    "    \n",
    "    return x_train_featured, x_test_featured\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove all missing values on X and remove corresponding lines in Y and ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all missing values on X and remove corresponding lines in Y and ids\n",
    "def cleanMissingValues(X): \n",
    "    x, y, ids = X\n",
    "    x_clean = x[~np.isnan(x).any(axis=1)]\n",
    "    #x_test_featured_clean = x_test_featured[~np.isnan(x_test_featured).any(axis=1)]\n",
    "\n",
    "    y_clean = y[~np.isnan(x).any(axis=1)]\n",
    "\n",
    "    ids_clean = ids[~np.isnan(x).any(axis=1)]\n",
    "    #test_ids_filtered = test_ids[~np.isnan(x_test_featured).any(axis=1)]\n",
    "    \n",
    "    return x_clean, y_clean, ids_clean\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace missing values by the mean of the column for the training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replaceMissingValuesMean(X):\n",
    "    #compute the mean of the column\n",
    "    mean = np.nanmean(X, axis = 0)\n",
    "\n",
    "    #replace all the NaN values by the mean\n",
    "    X = np.where(np.isnan(X), mean, X)\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing [Necessary] \n",
    "For this to work, Masking has to be done aswell\n",
    "### We want to clean the data for each feature, making them binary for yes/no, etc... and rename them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Data Processing \n",
    "\n",
    "def featureProcessing(dataMasked):\n",
    "    \n",
    "    x_train, x_test = dataMasked\n",
    "    \n",
    "    x_train_processed = d.feature_processing_test(x_train)\n",
    "\n",
    "    #Test data Processing \n",
    "    x_test_processed = d.feature_processing_test(x_test_featured)\n",
    "    \n",
    "    return x_train_processed, x_test_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can apply the processing functions to the data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainMask, testMask = masking((x_train, x_test), features_list)\n",
    "\n",
    "trainProcessed, testProcessed = featureProcessing((trainMask ,testMask))\n",
    "\n",
    "x_train_algo = replaceMissingValuesMean(trainProcessed)\n",
    "x_test_algo = replaceMissingValuesMean(testProcessed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the preprocessing has been done, we can format the data to be used by the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train = np.c_[np.ones((len(x_train_algo), 1)), x_train_algo]\n",
    "tX_test = np.c_[np.ones((len(x_test_algo), 1)), x_test_algo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation of set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_w = [random.choice([1, -1]) for i in range(len(tX_train[0]))]\n",
    "initial_w = np.ones(len(tX_train[0]))\n",
    "max_iter = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation of the dataset in a test/train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tX_train_train = tX_train[:int(len(tX_train)*0.7)]\n",
    "y_train_train = y_train[:int(len(tX_train)*0.7)]\n",
    "tX_train_test = tX_train[int(len(tX_train)*0.7):]\n",
    "y_train_test = y_train[int(len(tX_train)*0.7):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_progression(w):\n",
    "    # Plot progression of the weights in function of the iteration and progression on the test set\n",
    "    plt.figure(0)\n",
    "    plt.plot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And then, we can run the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. MSE gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(499/500): Final loss=0.24858977469312366\n"
     ]
    }
   ],
   "source": [
    "#Compute gradient descent with MSE as loss function (see functions.py for the function)\n",
    "\n",
    "w_mse_gd, loss_mse_gd = f.mean_squared_error_gd(y_train_train, tX_train_train, initial_w, 500, 0.01)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_test_set = []\n",
    "\n",
    "for w in w_mse_gd:\n",
    "    loss_test_set.append(f.compute_mse(y_train_test, tX_train_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(0)\n",
    "plt.semilogy(loss_mse_gd)\n",
    "plt.semilogy(loss_test_set)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135, 321)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109379,)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_rounded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8680631037880557\n",
      "F1 score:  0.11981566820276497\n"
     ]
    }
   ],
   "source": [
    "y_pred = tX_train_test.dot(w_mse_gd[-1])\n",
    "y_pred = np.where(y_pred > 0, 1, -1)\n",
    "\n",
    "_,_,_,_,f1 = f.confusion_matrix(y_train_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", np.sum(y_pred == y_train_test)/len(y_train_test))\n",
    "print(\"F1 score: \", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_csv_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_rounded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubmission_gd.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projet_ML/ML-project/helpers.py:71\u001b[0m, in \u001b[0;36mcreate_csv_submission\u001b[0;34m(ids, y_pred, name)\u001b[0m\n\u001b[1;32m     69\u001b[0m writer\u001b[38;5;241m.\u001b[39mwriteheader()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r1, r2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids, y_pred):\n\u001b[0;32m---> 71\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr1\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(r2)})\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "h.create_csv_submission(test_ids, y_test_rounded, 'submission_gd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      "\n",
      " [ 0.22422365 -0.19710027  0.01173442 -0.00087104  0.56386246  0.27254812\n",
      " -0.01120641  0.07727796 -0.2958502   0.04947018 -0.22956108 -0.04138959\n",
      "  0.09573553  0.07824979 -0.03784105  0.42959655 -0.00787782  0.76234186\n",
      "  0.15607555  0.10708325  0.25915261  0.25050162] \n",
      "\n",
      " Loss =  0.24858977469312366 \n",
      "\n",
      "*****************************************************************************  \n",
      "\n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.06141032556523647\n"
     ]
    }
   ],
   "source": [
    "#Test the model on the test sample. Do we need to standardize ?\n",
    "\n",
    "y_test = tX_test.dot(w_mse_gd[-1])\n",
    "y_test_rounded = np.where(y_test > 0, 1, -1) #not sure about this line\n",
    "\n",
    "print('weights = \\n\\n', w_mse_gd[-1],'\\n\\n Loss = ', loss_mse_gd[-1],'\\n\\n*****************************************************************************',\n",
    "      ' \\n\\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train == 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_rounded == 1)/len(y_test_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run some cross validation to see the best initial weights (as a function of the proportion of 1, -1 and 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. MSE SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 999/999: loss=0.013715028404194465, w0=0.7092641273973717, w1=0.2556935379117967\n"
     ]
    }
   ],
   "source": [
    "w_mse_sgd, loss_mse_sgd = f.mean_squared_error_sgd(y_train, tX_train, initial_w, 1000, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6104973826572594\n",
      "F1 score:  0.2515139281388777\n"
     ]
    }
   ],
   "source": [
    "y_pred = tX_train_test.dot(w_mse_sgd)\n",
    "y_pred = np.where(y_pred > 0, 1, -1)\n",
    "\n",
    "_,_,_,_,f1 = f.confusion_matrix(y_train_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", np.sum(y_pred == y_train_test)/len(y_train_test))\n",
    "\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [ 0.70926413  0.25569354  0.03510328 -0.04251818  0.95954882  0.742897\n",
      "  0.77839916  0.70395303  0.41095238  0.81726349 -0.41729001 -0.50848843\n",
      "  0.74752144  0.79445452  0.76336522  0.7818104  -0.45786616  0.91833267\n",
      "  0.71939817  0.89081692  0.91402388  0.91574478] \n",
      " Loss =  0.013715028404194465 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.4565867305424259\n"
     ]
    }
   ],
   "source": [
    "y_test_sgd = tX_test.dot(w_mse_sgd)\n",
    "y_test_rounded_sgd = np.where(y_test_sgd > 0, 1, -1)\n",
    "\n",
    "print('weights = \\n', w_mse_sgd,'\\n Loss = ', loss_mse_sgd,'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train == 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_rounded_sgd == 1)/len(y_test_rounded_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls, loss_ls = f.least_squares(y_train, tX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9067035487315954\n",
      "F1 score:  0.04651994990159242\n"
     ]
    }
   ],
   "source": [
    "y_pred = tX_train_test.dot(w_ls)\n",
    "y_pred = np.where(y_pred > 0, 1, -1)\n",
    "\n",
    "_,_,_,_,f1 = f.confusion_matrix(y_train_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", np.sum(y_pred == y_train_test)/len(y_train_test))\n",
    "\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [-3.56101104e-01  5.74396708e-02  2.26651131e-03 -2.17063114e-04\n",
      " -3.02761533e-02 -7.33762684e-03 -9.04431916e-02 -3.74132962e-01\n",
      " -4.82598963e-02 -9.90395178e-02 -1.69028700e-03 -5.60240584e-03\n",
      " -9.93472006e-02 -4.22232783e-02 -7.38677250e-02  3.63755048e-02\n",
      "  1.71959745e-02 -2.22558058e-01  3.23527987e-02 -6.39228664e-03\n",
      " -4.69220611e-03 -7.48412076e-03] \n",
      " Loss =  0.13731317795673628 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.002687901699594986\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_test_ls = tX_test.dot(w_ls)\n",
    "y_test_ls = np.where(y_test_ls > 0, 1, -1)\n",
    "\n",
    "print('weights = \\n', w_ls,'\\n Loss = ', loss_ls,'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train == 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_ls == 1)/len(y_test_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ridge, loss_ridge = f.ridge_regression(y_train, tX_train, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [-0.16601266  0.0391845   0.00262514 -0.00060561 -0.04657832 -0.03076275\n",
      " -0.09248568 -0.30124959 -0.06110316 -0.09897353 -0.01768843 -0.00966822\n",
      " -0.11207717 -0.04410244 -0.080523    0.01857876  0.0155675  -0.07562154\n",
      " -0.02572875 -0.01541715 -0.01049847 -0.01446916] \n",
      " Loss =  0.13790402252250836 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.0013165232814342789\n"
     ]
    }
   ],
   "source": [
    "y_test_ridge = tX_test.dot(w_ridge)\n",
    "y_test_ridge = np.where(y_test_ridge > 0, 1, -1)\n",
    "\n",
    "print('weights = \\n', w_ridge,'\\n Loss = ', loss_ridge,'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train == 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_ridge == 1)/len(y_test_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_processed_logreg = np.where(y_train == 1, 1, 0)\n",
    "y_train_train_lg = np.where(y_train == 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=22.228996654053788, w0=0.9088302070838765, w1=0.774412574698857\n",
      "Gradient Descent(1/499): loss=9.61084201026568, w0=0.8176604155043729, w1=0.5488251519014856\n",
      "Gradient Descent(2/499): loss=1.2867255420662551, w0=0.7266421871872321, w1=0.32357539293222454\n",
      "Gradient Descent(3/499): loss=1.0905883795901203, w0=0.7117199546655477, w1=0.2920885638054991\n",
      "Gradient Descent(4/499): loss=1.0210100359812195, w0=0.7050808937172566, w1=0.28412765397195483\n",
      "Gradient Descent(5/499): loss=0.9651652376027849, w0=0.6994507601948652, w1=0.278115964176346\n",
      "Gradient Descent(6/499): loss=0.9197144055840988, w0=0.6942364853647579, w1=0.2724105456807827\n",
      "Gradient Descent(7/499): loss=0.8819968488611999, w0=0.6892588720382995, w1=0.26657257268688056\n",
      "Gradient Descent(8/499): loss=0.8496388889807243, w0=0.6844978206127393, w1=0.26067571485396496\n",
      "Gradient Descent(9/499): loss=0.8210060145414425, w0=0.6799487327086798, w1=0.25484484750932557\n",
      "Gradient Descent(10/499): loss=0.7951040217852328, w0=0.6755849725208484, w1=0.249113935940798\n",
      "Gradient Descent(11/499): loss=0.7713515359665011, w0=0.6713697498934519, w1=0.24345873032886653\n",
      "Gradient Descent(12/499): loss=0.7494096795901417, w0=0.6672673860063033, w1=0.2378364426793095\n",
      "Gradient Descent(13/499): loss=0.7290755831625568, w0=0.6632475025900957, w1=0.2322040413503875\n",
      "Gradient Descent(14/499): loss=0.7102190838766337, w0=0.6592860070030585, w1=0.2265254969927776\n",
      "Gradient Descent(15/499): loss=0.692745805596571, w0=0.6553649419997979, w1=0.22077442268544475\n",
      "Gradient Descent(16/499): loss=0.676576178201523, w0=0.6514719117024028, w1=0.21493458526323436\n",
      "Gradient Descent(17/499): loss=0.6616342797072854, w0=0.6475993288182219, w1=0.2089992932488281\n",
      "Gradient Descent(18/499): loss=0.647842809464743, w0=0.6437435780033998, w1=0.20297013234649522\n",
      "Gradient Descent(19/499): loss=0.635121795846666, w0=0.6399041496115419, w1=0.19685533099109942\n",
      "Gradient Descent(20/499): loss=0.623389384207351, w0=0.6360827929449089, w1=0.1906679767495537\n",
      "Gradient Descent(21/499): loss=0.6125635564866648, w0=0.6322827372493871, w1=0.18442427204588502\n",
      "Gradient Descent(22/499): loss=0.6025640324595581, w0=0.6285080207423088, w1=0.1781419768057557\n",
      "Gradient Descent(23/499): loss=0.5933139242956089, w0=0.6247629526502221, w1=0.17183913179676116\n",
      "Gradient Descent(24/499): loss=0.5847409568670526, w0=0.6210517151890292, w1=0.1655331001862011\n",
      "Gradient Descent(25/499): loss=0.5767782244571997, w0=0.6173780966253809, w1=0.15923991796877862\n",
      "Gradient Descent(26/499): loss=0.5693645410870319, w0=0.6137453359617621, w1=0.15297391286305892\n",
      "Gradient Descent(27/499): loss=0.5624444760001757, w0=0.6101560549399754, w1=0.14674753637950025\n",
      "Gradient Descent(28/499): loss=0.555968168282745, w0=0.6066122528961309, w1=0.14057135155582678\n",
      "Gradient Descent(29/499): loss=0.5498910013794829, w0=0.6031153428421574, w1=0.1344541246579954\n",
      "Gradient Descent(30/499): loss=0.5441731999821561, w0=0.5996662114025926, w1=0.1284029787730974\n",
      "Gradient Descent(31/499): loss=0.5387793941059661, w0=0.5962652897309951, w1=0.12242357768678022\n",
      "Gradient Descent(32/499): loss=0.5336781805121261, w0=0.5929126265713481, w1=0.11652031798431452\n",
      "Gradient Descent(33/499): loss=0.5288417004868587, w0=0.5896079579042603, w1=0.11069651514263773\n",
      "Gradient Descent(34/499): loss=0.5242452449930134, w0=0.5863507700691968, w1=0.10495457531736498\n",
      "Gradient Descent(35/499): loss=0.5198668927310021, w0=0.583140354966551, w1=0.09929614874355833\n",
      "Gradient Descent(36/499): loss=0.515687183038433, w0=0.5799758570594333, w1=0.09372226347684744\n",
      "Gradient Descent(37/499): loss=0.5116888232839957, w0=0.576856312564932, w1=0.08823343994306376\n",
      "Gradient Descent(38/499): loss=0.5078564290573236, w0=0.5737806815811889, w1=0.082829787745558\n",
      "Gradient Descent(39/499): loss=0.5041762947208729, w0=0.5707478740459333, w1=0.07751108664685245\n",
      "Gradient Descent(40/499): loss=0.5006361915588474, w0=0.567756770442852, w1=0.07227685378116619\n",
      "Gradient Descent(41/499): loss=0.49722519068417714, w0=0.5648062381195136, w1=0.06712639909902564\n",
      "Gradient Descent(42/499): loss=0.493933507947066, w0=0.5618951439912112, w1=0.06205887088441573\n",
      "Gradient Descent(43/499): loss=0.4907523682604691, w0=0.5590223643021898, w1=0.05707329297679117\n",
      "Gradient Descent(44/499): loss=0.48767388697493824, w0=0.556186792012964, w1=0.05216859511033524\n",
      "Gradient Descent(45/499): loss=0.48469096616926993, w0=0.5533873422872144, w1=0.04734363757147744\n",
      "Gradient Descent(46/499): loss=0.48179720395672493, w0=0.550622956467468, w1=0.04259723118330369\n",
      "Gradient Descent(47/499): loss=0.4789868151289123, w0=0.5478926048564071, w1=0.037928153456386865\n",
      "Gradient Descent(48/499): loss=0.47625456166508795, w0=0.5451952885598427, w1=0.03333516160037008\n",
      "Gradient Descent(49/499): loss=0.47359569182108274, w0=0.5425300405970531, w1=0.028817002967992344\n",
      "Gradient Descent(50/499): loss=0.471005886678723, w0=0.5398959264430092, w1=0.024372423400850007\n",
      "Gradient Descent(51/499): loss=0.4684812131839921, w0=0.5372920441335753, w1=0.020000173861391666\n",
      "Gradient Descent(52/499): loss=0.4660180828315545, w0=0.5347175240378191, w1=0.015699015665826023\n",
      "Gradient Descent(53/499): loss=0.46361321526614924, w0=0.5321715283799103, w1=0.011467724575359058\n",
      "Gradient Descent(54/499): loss=0.4612636061694697, w0=0.5296532505757665, w1=0.007305093956327766\n",
      "Gradient Descent(55/499): loss=0.45896649888613217, w0=0.5271619144357677, w1=0.003209937181524153\n",
      "Gradient Descent(56/499): loss=0.45671935931580665, w0=0.5246967732738415, w1=-0.0008189105862442224\n",
      "Gradient Descent(57/499): loss=0.4545198536619885, w0=0.5222571089544439, w1=-0.004782591112821596\n",
      "Gradient Descent(58/499): loss=0.45236582868256475, w0=0.5198422309019854, w1=-0.008682222218898531\n",
      "Gradient Descent(59/499): loss=0.4502552941344505, w0=0.5174514750917116, w1=-0.012518897236061581\n",
      "Gradient Descent(60/499): loss=0.44818640714517616, w0=0.5150842030366486, w1=-0.01629368472895682\n",
      "Gradient Descent(61/499): loss=0.4461574582792965, w0=0.5127398007817374, w1=-0.020007628431234485\n",
      "Gradient Descent(62/499): loss=0.44416685909766584, w0=0.5104176779135213, w1=-0.023661747352296\n",
      "Gradient Descent(63/499): loss=0.4422131310336495, w0=0.5081172665915672, w1=-0.02725703601954296\n",
      "Gradient Descent(64/499): loss=0.4402948954328104, w0=0.5058380206060815, w1=-0.030794464827137647\n",
      "Gradient Descent(65/499): loss=0.4384108646220318, w0=0.5035794144648271, w1=-0.03427498046747667\n",
      "Gradient Descent(66/499): loss=0.43655983389082464, w0=0.5013409425113865, w1=-0.03769950642585458\n",
      "Gradient Descent(67/499): loss=0.434740674282115, w0=0.4991221180759894, w1=-0.041068943522319594\n",
      "Gradient Descent(68/499): loss=0.43295232610242024, w0=0.49692247265948036, w1=-0.0443841704876317\n",
      "Gradient Descent(69/499): loss=0.4311937930722552, w0=0.4947415551505099, w1=-0.04764604456263431\n",
      "Gradient Descent(70/499): loss=0.42946413704713526, w0=0.49257893107565504, w1=-0.05085540211233484\n",
      "Gradient Descent(71/499): loss=0.4277624732478118, w0=0.49043418188189547, w1=-0.05401305924762817\n",
      "Gradient Descent(72/499): loss=0.4260879659455984, w0=0.48830690425066153, w1=-0.057119812448952915\n",
      "Gradient Descent(73/499): loss=0.42443982455493423, w0=0.48619670944252336, w1=-0.06017643918728998\n",
      "Gradient Descent(74/499): loss=0.4228173000908439, w0=0.4841032226714864, w1=-0.06318369853883919\n",
      "Gradient Descent(75/499): loss=0.4212196819537594, w0=0.4820260825077938, w1=-0.0661423317904746\n",
      "Gradient Descent(76/499): loss=0.41964629500839734, w0=0.4799649403080964, w1=-0.06905306303371066\n",
      "Gradient Descent(77/499): loss=0.4180964969270872, w0=0.477919459671839, w1=-0.07191659974543227\n",
      "Gradient Descent(78/499): loss=0.41656967577120063, w0=0.47588931592270894, w1=-0.07473363335407117\n",
      "Gradient Descent(79/499): loss=0.41506524778720016, w0=0.4738741956140094, w1=-0.0775048397902637\n",
      "Gradient Descent(80/499): loss=0.413582655396353, w0=0.47187379605684465, w1=-0.08023088002131483\n",
      "Gradient Descent(81/499): loss=0.41212136535937843, w0=0.4698878248700325, w1=-0.0829124005690304\n",
      "Gradient Descent(82/499): loss=0.4106808670992829, w0=0.467915999550698, w1=-0.08555003401067279\n",
      "Gradient Descent(83/499): loss=0.40926067116736187, w0=0.46595804706453964, w1=-0.0881443994629525\n",
      "Gradient Descent(84/499): loss=0.4078603078389125, w0=0.46401370345480175, w1=-0.09069610304909519\n",
      "Gradient Descent(85/499): loss=0.4064793258265659, w0=0.4620827134690292, w1=-0.09320573834912568\n",
      "Gradient Descent(86/499): loss=0.40511729110036243, w0=0.46016483020272203, w1=-0.0956738868335918\n",
      "Gradient Descent(87/499): loss=0.40377378580479245, w0=0.45825981475905214, w1=-0.09810111828101518\n",
      "Gradient Descent(88/499): loss=0.40244840726398123, w0=0.4563674359238439, w1=-0.10048799117940609\n",
      "Gradient Descent(89/499): loss=0.40114076706706825, w0=0.4544874698550621, w1=-0.1028350531122176\n",
      "Gradient Descent(90/499): loss=0.3998504902265934, w0=0.45261969978609085, w1=-0.10514284112914316\n",
      "Gradient Descent(91/499): loss=0.3985772144034089, w0=0.4507639157421226, w1=-0.10741188210218171\n",
      "Gradient Descent(92/499): loss=0.39732058919223145, w0=0.44891991426901573, w1=-0.10964269306740934\n",
      "Gradient Descent(93/499): loss=0.3960802754625279, w0=0.44708749817401117, w1=-0.11183578155290456\n",
      "Gradient Descent(94/499): loss=0.3948559447499072, w0=0.4452664762777331, w1=-0.11399164589327893\n",
      "Gradient Descent(95/499): loss=0.3936472786936484, w0=0.4434566631769299, w1=-0.11611077553126532\n",
      "Gradient Descent(96/499): loss=0.392453968516394, w0=0.4416578790174404, w1=-0.11819365130681454\n",
      "Gradient Descent(97/499): loss=0.391275714542403, w0=0.4398699492768995, w1=-0.12024074573414595\n",
      "Gradient Descent(98/499): loss=0.39011222575108356, w0=0.4380927045567229, w1=-0.1222525232671922\n",
      "Gradient Descent(99/499): loss=0.3889632193628203, w0=0.4363259803829365, w1=-0.12422944055387017\n",
      "Gradient Descent(100/499): loss=0.38782842045438237, w0=0.434569617015439, w1=-0.12617194667960163\n",
      "Gradient Descent(101/499): loss=0.3867075616014352, w0=0.4328234592653077, w1=-0.12808048340049752\n",
      "Gradient Descent(102/499): loss=0.38560038254589996, w0=0.43108735631978046, w1=-0.1299554853666093\n",
      "Gradient Descent(103/499): loss=0.384506629886105, w0=0.429361161574564, w1=-0.1317973803356407\n",
      "Gradient Descent(104/499): loss=0.38342605678784936, w0=0.4276447324731382, w1=-0.1336065893775013\n",
      "Gradient Descent(105/499): loss=0.3823584227146675, w0=0.42593793035274374, w1=-0.13538352707007323\n",
      "Gradient Descent(106/499): loss=0.3813034931757262, w0=0.42424062029675536, w1=-0.13712860168655006\n",
      "Gradient Descent(107/499): loss=0.3802610394899258, w0=0.42255267099316024, w1=-0.13884221537469618\n",
      "Gradient Descent(108/499): loss=0.3792308385648985, w0=0.42087395459887295, w1=-0.14052476432836386\n",
      "Gradient Descent(109/499): loss=0.37821267268970693, w0=0.4192043466096339, w1=-0.14217663895159333\n",
      "Gradient Descent(110/499): loss=0.3772063293401498, w0=0.4175437257352496, w1=-0.14379822401561101\n",
      "Gradient Descent(111/499): loss=0.37621160099567735, w0=0.4158919737799459, w1=-0.14538989880902972\n",
      "Gradient Descent(112/499): loss=0.37522828496699673, w0=0.41424897552761525, w1=-0.14695203728154427\n",
      "Gradient Descent(113/499): loss=0.37425618323353294, w0=0.41261461863175125, w1=-0.1484850081814051\n",
      "Gradient Descent(114/499): loss=0.3732951022899779, w0=0.41098879350987205, w1=-0.1499891751869431\n",
      "Gradient Descent(115/499): loss=0.3723448530012243, w0=0.4093713932422443, w1=-0.15146489703240804\n",
      "Gradient Descent(116/499): loss=0.3714052504650421, w0=0.40776231347472836, w1=-0.1529125276283736\n",
      "Gradient Descent(117/499): loss=0.3704761138819085, w0=0.40616145232557294, w1=-0.15433241617695306\n",
      "Gradient Descent(118/499): loss=0.3695572664314519, w0=0.4045687102959958, w1=-0.15572490728205954\n",
      "Gradient Descent(119/499): loss=0.36864853515501467, w0=0.4029839901843942, w1=-0.15709034105493663\n",
      "Gradient Descent(120/499): loss=0.36774975084388456, w0=0.40140719700403615, w1=-0.1584290532151759\n",
      "Gradient Descent(121/499): loss=0.36686074793277607, w0=0.39983823790408946, w1=-0.15974137518742984\n",
      "Gradient Descent(122/499): loss=0.3659813643981847, w0=0.39827702209385213, w1=-0.1610276341940203\n",
      "Gradient Descent(123/499): loss=0.3651114416612643, w0=0.39672346077005405, w1=-0.16228815334363444\n",
      "Gradient Descent(124/499): loss=0.3642508244949079, w0=0.39517746704710444, w1=-0.16352325171629303\n",
      "Gradient Descent(125/499): loss=0.3633993609347415, w0=0.39363895589016595, w1=-0.1647332444447677\n",
      "Gradient Descent(126/499): loss=0.36255690219375836, w0=0.3921078440509401, w1=-0.1659184427926173\n",
      "Gradient Descent(127/499): loss=0.36172330258035323, w0=0.3905840500060553, w1=-0.16707915422900602\n",
      "Gradient Descent(128/499): loss=0.3608984194195261, w0=0.3890674938979507, w1=-0.1682156825004591\n",
      "Gradient Descent(129/499): loss=0.3600821129770522, w0=0.38755809747815656, w1=-0.16932832769970596\n",
      "Gradient Descent(130/499): loss=0.35927424638642674, w0=0.3860557840528729, w1=-0.1704173863317535\n",
      "Gradient Descent(131/499): loss=0.35847468557841256, w0=0.38456047843075425, w1=-0.17148315137732686\n",
      "Gradient Descent(132/499): loss=0.3576832992130272, w0=0.383072106872811, w1=-0.1725259123538086\n",
      "Gradient Descent(133/499): loss=0.35689995861382834, w0=0.3815905970443422, w1=-0.17354595537380144\n",
      "Gradient Descent(134/499): loss=0.3561245377043581, w0=0.3801158779688166, w1=-0.17454356320143505\n",
      "Gradient Descent(135/499): loss=0.3553569129466278, w0=0.3786478799836245, w1=-0.17551901530653044\n",
      "Gradient Descent(136/499): loss=0.3545969632815246, w0=0.377186534697623, w1=-0.17647258791673234\n",
      "Gradient Descent(137/499): loss=0.3538445700710408, w0=0.375731774950403, w1=-0.1774045540677132\n",
      "Gradient Descent(138/499): loss=0.3530996170422252, w0=0.37428353477320736, w1=-0.1783151836515488\n",
      "Gradient Descent(139/499): loss=0.3523619902327747, w0=0.37284174935143316, w1=-0.17920474346336074\n",
      "Gradient Descent(140/499): loss=0.3516315779381775, w0=0.3714063549886533, w1=-0.18007349724631588\n",
      "Gradient Descent(141/499): loss=0.35090827066033986, w0=0.369977289072096, w1=-0.18092170573506944\n",
      "Gradient Descent(142/499): loss=0.3501919610576232, w0=0.368554490039521, w1=-0.1817496266977341\n",
      "Gradient Descent(143/499): loss=0.3494825438962308, w0=0.3671378973474375, w1=-0.18255751497645306\n",
      "Gradient Descent(144/499): loss=0.3487799160028824, w0=0.3657274514406061, w1=-0.1833456225266519\n",
      "Gradient Descent(145/499): loss=0.34808397621872667, w0=0.3643230937227735, w1=-0.18411419845503985\n",
      "Gradient Descent(146/499): loss=0.3473946253544363, w0=0.36292476652858874, w1=-0.184863489056428\n",
      "Gradient Descent(147/499): loss=0.34671176614644317, w0=0.3615324130966516, w1=-0.1855937378494285\n",
      "Gradient Descent(148/499): loss=0.3460353032142684, w0=0.3601459775436464, w1=-0.1863051856110953\n",
      "Gradient Descent(149/499): loss=0.34536514301890753, w0=0.35876540483951624, w1=-0.1869980704105643\n",
      "Gradient Descent(150/499): loss=0.34470119382223324, w0=0.35739064078363375, w1=-0.1876726276417477\n",
      "Gradient Descent(151/499): loss=0.34404336564738086, w0=0.35602163198192704, w1=-0.1883290900551342\n",
      "Gradient Descent(152/499): loss=0.3433915702400843, w0=0.35465832582492085, w1=-0.18896768778874462\n",
      "Gradient Descent(153/499): loss=0.34274572103093126, w0=0.3533006704666537, w1=-0.1895886483982889\n",
      "Gradient Descent(154/499): loss=0.3421057330985095, w0=0.3519486148044351, w1=-0.19019219688656921\n",
      "Gradient Descent(155/499): loss=0.34147152313341683, w0=0.3506021084594064, w1=-0.19077855573217053\n",
      "Gradient Descent(156/499): loss=0.3408430094031108, w0=0.34926110175787156, w1=-0.19134794491747817\n",
      "Gradient Descent(157/499): loss=0.34022011171757144, w0=0.3479255457133651, w1=-0.1919005819560594\n",
      "Gradient Descent(158/499): loss=0.3396027513957585, w0=0.34659539200942563, w1=-0.19243668191944477\n",
      "Gradient Descent(159/499): loss=0.33899085123283657, w0=0.34527059298304513, w1=-0.1929564574633416\n",
      "Gradient Descent(160/499): loss=0.33838433546815344, w0=0.34395110160876424, w1=-0.19346011885331202\n",
      "Gradient Descent(161/499): loss=0.3377831297539471, w0=0.34263687148338684, w1=-0.19394787398994368\n",
      "Gradient Descent(162/499): loss=0.3371871611247662, w0=0.3413278568112861, w1=-0.19441992843354247\n",
      "Gradient Descent(163/499): loss=0.3365963579675856, w0=0.34002401239027735, w1=-0.19487648542837213\n",
      "Gradient Descent(164/499): loss=0.33601064999259855, w0=0.33872529359803283, w1=-0.19531774592646625\n",
      "Gradient Descent(165/499): loss=0.33542996820467286, w0=0.33743165637901446, w1=-0.19574390861103522\n",
      "Gradient Descent(166/499): loss=0.33485424487544946, w0=0.33614305723190263, w1=-0.1961551699194904\n",
      "Gradient Descent(167/499): loss=0.3342834135160754, w0=0.33485945319749927, w1=-0.19655172406610533\n",
      "Gradient Descent(168/499): loss=0.33371740885055107, w0=0.33358080184708394, w1=-0.19693376306433383\n",
      "Gradient Descent(169/499): loss=0.33315616678968, w0=0.33230706127120385, w1=-0.1973014767488021\n",
      "Gradient Descent(170/499): loss=0.33259962440560875, w0=0.3310381900688779, w1=-0.19765505279699247\n",
      "Gradient Descent(171/499): loss=0.3320477199069402, w0=0.32977414733719745, w1=-0.19799467675063384\n",
      "Gradient Descent(172/499): loss=0.33150039261441167, w0=0.3285148926613055, w1=-0.19832053203681402\n",
      "Gradient Descent(173/499): loss=0.3309575829371216, w0=0.3272603861047381, w1=-0.1986327999888272\n",
      "Gradient Descent(174/499): loss=0.33041923234929527, w0=0.3260105882001117, w1=-0.19893165986676997\n",
      "Gradient Descent(175/499): loss=0.3298852833675769, w0=0.3247654599401411, w1=-0.19921728887789714\n",
      "Gradient Descent(176/499): loss=0.32935567952883626, w0=0.3235249627689737, w1=-0.1994898621967492\n",
      "Gradient Descent(177/499): loss=0.32883036536847876, w0=0.32228905857382534, w1=-0.1997495529850612\n",
      "Gradient Descent(178/499): loss=0.3283092863992481, w0=0.32105770967690517, w1=-0.199996532411463\n",
      "Gradient Descent(179/499): loss=0.3277923890905113, w0=0.31983087882761574, w1=-0.2002309696709797\n",
      "Gradient Descent(180/499): loss=0.32727962084801426, w0=0.31860852919501675, w1=-0.20045303200434056\n",
      "Gradient Descent(181/499): loss=0.3267709299940991, w0=0.31739062436054066, w1=-0.2006628847171042\n",
      "Gradient Descent(182/499): loss=0.32626626574837164, w0=0.31617712831094835, w1=-0.20086069119860703\n",
      "Gradient Descent(183/499): loss=0.32576557820881075, w0=0.31496800543151476, w1=-0.20104661294074141\n",
      "Gradient Descent(184/499): loss=0.32526881833330795, w0=0.31376322049943406, w1=-0.20122080955656999\n",
      "Gradient Descent(185/499): loss=0.3247759379216309, w0=0.3125627386774343, w1=-0.20138343879878118\n",
      "Gradient Descent(186/499): loss=0.32428688959779717, w0=0.31136652550759253, w1=-0.2015346565779917\n",
      "Gradient Descent(187/499): loss=0.3238016267928529, w0=0.3101745469053414, w1=-0.20167461698089995\n",
      "Gradient Descent(188/499): loss=0.32332010372804615, w0=0.30898676915365847, w1=-0.20180347228829548\n",
      "Gradient Descent(189/499): loss=0.3228422753983844, w0=0.30780315889743026, w1=-0.2019213729929278\n",
      "Gradient Descent(190/499): loss=0.32236809755657164, w0=0.30662368313798344, w1=-0.20202846781723854\n",
      "Gradient Descent(191/499): loss=0.3218975266973123, w0=0.3054483092277755, w1=-0.20212490373096006\n",
      "Gradient Descent(192/499): loss=0.3214305200419756, w0=0.30427700486523773, w1=-0.2022108259685836\n",
      "Gradient Descent(193/499): loss=0.32096703552361344, w0=0.3031097380897643, w1=-0.20228637804669977\n",
      "Gradient Descent(194/499): loss=0.32050703177232137, w0=0.3019464772768404, w1=-0.2023517017812134\n",
      "Gradient Descent(195/499): loss=0.320050468100937, w0=0.3007871911333034, w1=-0.2024069373044358\n",
      "Gradient Descent(196/499): loss=0.3195973044910661, w0=0.29963184869273185, w1=-0.20245222308205535\n",
      "Gradient Descent(197/499): loss=0.319147501579431, w0=0.2984804193109558, w1=-0.20248769592998936\n",
      "Gradient Descent(198/499): loss=0.31870102064453204, w0=0.2973328726616838, w1=-0.2025134910311176\n",
      "Gradient Descent(199/499): loss=0.3182578235936165, w0=0.2961891787322409, w1=-0.20252974195190018\n",
      "Gradient Descent(200/499): loss=0.31781787294994646, w0=0.2950493078194139, w1=-0.2025365806588798\n",
      "Gradient Descent(201/499): loss=0.31738113184035915, w0=0.2939132305253981, w1=-0.20253413753507057\n",
      "Gradient Descent(202/499): loss=0.3169475639831139, w0=0.29278091775384146, w1=-0.20252254139623355\n",
      "Gradient Descent(203/499): loss=0.316517133676017, w0=0.291652340705983, w1=-0.20250191950704025\n",
      "Gradient Descent(204/499): loss=0.31608980578482027, w0=0.2905274708768797, w1=-0.202472397597125\n",
      "Gradient Descent(205/499): loss=0.31566554573188477, w0=0.2894062800517198, w1=-0.2024340998770261\n",
      "Gradient Descent(206/499): loss=0.3152443194851045, w0=0.2882887403022181, w1=-0.20238714905401733\n",
      "Gradient Descent(207/499): loss=0.3148260935470853, w0=0.28717482398308963, w1=-0.20233166634782915\n",
      "Gradient Descent(208/499): loss=0.31441083494456884, w0=0.28606450372859904, w1=-0.20226777150626082\n",
      "Gradient Descent(209/499): loss=0.31399851121810046, w0=0.2849577524491825, w1=-0.2021955828206834\n",
      "Gradient Descent(210/499): loss=0.31358909041193195, w0=0.2838545433281386, w1=-0.20211521714143366\n",
      "Gradient Descent(211/499): loss=0.31318254106415544, w0=0.2827548498183861, w1=-0.20202678989309952\n",
      "Gradient Descent(212/499): loss=0.3127788321970612, w0=0.2816586456392859, w1=-0.2019304150896969\n",
      "Gradient Descent(213/499): loss=0.31237793330771574, w0=0.2805659047735238, w1=-0.20182620534973778\n",
      "Gradient Descent(214/499): loss=0.31197981435875344, w0=0.2794766014640528, w1=-0.20171427191119037\n",
      "Gradient Descent(215/499): loss=0.31158444576937727, w0=0.27839071021109213, w1=-0.20159472464633035\n",
      "Gradient Descent(216/499): loss=0.3111917984065644, w0=0.2773082057691805, w1=-0.20146767207648403\n",
      "Gradient Descent(217/499): loss=0.3108018435764686, w0=0.2762290631442823, w1=-0.20133322138666268\n",
      "Gradient Descent(218/499): loss=0.3104145530160195, w0=0.2751532575909438, w1=-0.2011914784400888\n",
      "Gradient Descent(219/499): loss=0.31002989888470806, w0=0.2740807646094985, w1=-0.20104254779261338\n",
      "Gradient Descent(220/499): loss=0.30964785375656084, w0=0.2730115599433194, w1=-0.2008865327070247\n",
      "Gradient Descent(221/499): loss=0.30926839061229094, w0=0.2719456195761159, w1=-0.20072353516724822\n",
      "Gradient Descent(222/499): loss=0.30889148283162693, w0=0.27088291972927514, w1=-0.2005536558924378\n",
      "Gradient Descent(223/499): loss=0.3085171041858125, w0=0.26982343685924476, w1=-0.20037699435095774\n",
      "Gradient Descent(224/499): loss=0.3081452288302739, w0=0.26876714765495674, w1=-0.20019364877425588\n",
      "Gradient Descent(225/499): loss=0.3077758312974506, w0=0.2677140290352903, w1=-0.20000371617062757\n",
      "Gradient Descent(226/499): loss=0.3074088864897859, w0=0.2666640581465729, w1=-0.19980729233887012\n",
      "Gradient Descent(227/499): loss=0.3070443696728709, w0=0.26561721236011804, w1=-0.19960447188182837\n",
      "Gradient Descent(228/499): loss=0.30668225646874314, w0=0.26457346926979847, w1=-0.1993953482198303\n",
      "Gradient Descent(229/499): loss=0.30632252284933, w0=0.26353280668965395, w1=-0.19918001360401366\n",
      "Gradient Descent(230/499): loss=0.30596514513003953, w0=0.2624952026515325, w1=-0.19895855912954263\n",
      "Gradient Descent(231/499): loss=0.30561009996348804, w0=0.2614606354027638, w1=-0.1987310747487152\n",
      "Gradient Descent(232/499): loss=0.30525736433336975, w0=0.26042908340386445, w1=-0.19849764928396066\n",
      "Gradient Descent(233/499): loss=0.3049069155484557, w0=0.25940052532627306, w1=-0.19825837044072767\n",
      "Gradient Descent(234/499): loss=0.3045587312367269, w0=0.25837494005011585, w1=-0.19801332482026257\n",
      "Gradient Descent(235/499): loss=0.3042127893396314, w0=0.2573523066620004, w1=-0.1977625979322781\n",
      "Gradient Descent(236/499): loss=0.30386906810647013, w0=0.2563326044528377, w1=-0.19750627420751238\n",
      "Gradient Descent(237/499): loss=0.3035275460888993, w0=0.2553158129156917, w1=-0.1972444370101786\n",
      "Gradient Descent(238/499): loss=0.30318820213555414, w0=0.2543019117436554, w1=-0.19697716865030498\n",
      "Gradient Descent(239/499): loss=0.30285101538678655, w0=0.25329088082775236, w1=-0.1967045503959655\n",
      "Gradient Descent(240/499): loss=0.3025159652695162, w0=0.2522827002548643, w1=-0.19642666248540105\n",
      "Gradient Descent(241/499): loss=0.30218303149219106, w0=0.251277350305683, w1=-0.19614358413903196\n",
      "Gradient Descent(242/499): loss=0.30185219403985397, w0=0.2502748114526862, w1=-0.19585539357136078\n",
      "Gradient Descent(243/499): loss=0.30152343316931585, w0=0.24927506435813748, w1=-0.19556216800276674\n",
      "Gradient Descent(244/499): loss=0.30119672940442915, w0=0.2482780898721086, w1=-0.19526398367119122\n",
      "Gradient Descent(245/499): loss=0.3008720635314612, w0=0.24728386903052493, w1=-0.19496091584371486\n",
      "Gradient Descent(246/499): loss=0.3005494165945651, w0=0.24629238305323273, w1=-0.19465303882802631\n",
      "Gradient Descent(247/499): loss=0.3002287698913444, w0=0.24530361334208794, w1=-0.19434042598378298\n",
      "Gradient Descent(248/499): loss=0.2999101049685112, w0=0.24431754147906634, w1=-0.19402314973386398\n",
      "Gradient Descent(249/499): loss=0.2995934036176333, w0=0.24333414922439442, w1=-0.19370128157551547\n",
      "Gradient Descent(250/499): loss=0.2992786478709709, w0=0.24235341851470055, w1=-0.19337489209138892\n",
      "Gradient Descent(251/499): loss=0.29896581999739663, w0=0.24137533146118625, w1=-0.19304405096047245\n",
      "Gradient Descent(252/499): loss=0.2986549024984024, w0=0.24039987034781712, w1=-0.19270882696891536\n",
      "Gradient Descent(253/499): loss=0.2983458781041857, w0=0.23942701762953297, w1=-0.19236928802074682\n",
      "Gradient Descent(254/499): loss=0.2980387297698167, w0=0.23845675593047702, w1=-0.19202550114848818\n",
      "Gradient Descent(255/499): loss=0.297733440671483, w0=0.23748906804224368, w1=-0.19167753252366035\n",
      "Gradient Descent(256/499): loss=0.2974299942028113, w0=0.23652393692214477, w1=-0.19132544746718547\n",
      "Gradient Descent(257/499): loss=0.29712837397126113, w0=0.23556134569149376, w1=-0.1909693104596846\n",
      "Gradient Descent(258/499): loss=0.29682856379459366, w0=0.23460127763390784, w1=-0.19060918515167039\n",
      "Gradient Descent(259/499): loss=0.2965305476974092, w0=0.23364371619362745, w1=-0.1902451343736367\n",
      "Gradient Descent(260/499): loss=0.2962343099077553, w0=0.23268864497385316, w1=-0.1898772201460441\n",
      "Gradient Descent(261/499): loss=0.2959398348538007, w0=0.2317360477350996, w1=-0.18950550368920338\n",
      "Gradient Descent(262/499): loss=0.29564710716057685, w0=0.23078590839356602, w1=-0.18913004543305573\n",
      "Gradient Descent(263/499): loss=0.29535611164678266, w0=0.22983821101952367, w1=-0.18875090502685182\n",
      "Gradient Descent(264/499): loss=0.295066833321652, w0=0.22889293983571934, w1=-0.1883681413487291\n",
      "Gradient Descent(265/499): loss=0.2947792573818831, w0=0.22795007921579502, w1=-0.18798181251518833\n",
      "Gradient Descent(266/499): loss=0.2944933692086274, w0=0.2270096136827237, w1=-0.18759197589046997\n",
      "Gradient Descent(267/499): loss=0.2942091543645373, w0=0.22607152790726073, w1=-0.18719868809583065\n",
      "Gradient Descent(268/499): loss=0.2939265985908706, w0=0.2251358067064107, w1=-0.18680200501872052\n",
      "Gradient Descent(269/499): loss=0.29364568780465145, w0=0.2242024350419099, w1=-0.18640198182186207\n",
      "Gradient Descent(270/499): loss=0.2933664080958847, w0=0.22327139801872373, w1=-0.1859986729522309\n",
      "Gradient Descent(271/499): loss=0.2930887457248252, w0=0.2223426808835593, w1=-0.18559213214993908\n",
      "Gradient Descent(272/499): loss=0.29281268711929725, w0=0.22141626902339273, w1=-0.18518241245702163\n",
      "Gradient Descent(273/499): loss=0.29253821887206716, w0=0.22049214796401131, w1=-0.18476956622612697\n",
      "Gradient Descent(274/499): loss=0.29226532773826314, w0=0.21957030336857003, w1=-0.18435364512911176\n",
      "Gradient Descent(275/499): loss=0.291994000632846, w0=0.21865072103616248, w1=-0.18393470016554067\n",
      "Gradient Descent(276/499): loss=0.29172422462812564, w0=0.21773338690040617, w1=-0.18351278167109206\n",
      "Gradient Descent(277/499): loss=0.2914559869513252, w0=0.21681828702804168, w1=-0.18308793932586995\n",
      "Gradient Descent(278/499): loss=0.29118927498218977, w0=0.21590540761754592, w1=-0.18266022216262298\n",
      "Gradient Descent(279/499): loss=0.2909240762506406, w0=0.21499473499775912, w1=-0.18222967857487113\n",
      "Gradient Descent(280/499): loss=0.2906603784344708, w0=0.21408625562652542, w1=-0.18179635632494073\n",
      "Gradient Descent(281/499): loss=0.290398169357086, w0=0.21317995608934714, w1=-0.18136030255190858\n",
      "Gradient Descent(282/499): loss=0.2901374369852836, w0=0.21227582309805226, w1=-0.18092156377945576\n",
      "Gradient Descent(283/499): loss=0.2898781694270758, w0=0.2113738434894753, w1=-0.18048018592363177\n",
      "Gradient Descent(284/499): loss=0.2896203549295498, w0=0.21047400422415133, w1=-0.18003621430052996\n",
      "Gradient Descent(285/499): loss=0.2893639818767676, w0=0.2095762923850229, w1=-0.17958969363387456\n",
      "Gradient Descent(286/499): loss=0.2891090387877047, w0=0.20868069517616, w1=-0.1791406680625203\n",
      "Gradient Descent(287/499): loss=0.28885551431422435, w0=0.2077871999214927, w1=-0.17868918114786528\n",
      "Gradient Descent(288/499): loss=0.28860339723908957, w0=0.2068957940635565, w1=-0.17823527588117763\n",
      "Gradient Descent(289/499): loss=0.2883526764740103, w0=0.20600646516225019, w1=-0.17777899469083697\n",
      "Gradient Descent(290/499): loss=0.28810334105772417, w0=0.20511920089360622, w1=-0.17732037944949106\n",
      "Gradient Descent(291/499): loss=0.28785538015411405, w0=0.20423398904857326, w1=-0.17685947148112854\n",
      "Gradient Descent(292/499): loss=0.2876087830503561, w0=0.20335081753181114, w1=-0.1763963115680686\n",
      "Gradient Descent(293/499): loss=0.28736353915510204, w0=0.20246967436049773, w1=-0.1759309399578679\n",
      "Gradient Descent(294/499): loss=0.28711963799669327, w0=0.2015905476631479, w1=-0.17546339637014596\n",
      "Gradient Descent(295/499): loss=0.2868770692214058, w0=0.20071342567844438, w1=-0.1749937200033293\n",
      "Gradient Descent(296/499): loss=0.286635822591726, w0=0.19983829675408032, w1=-0.17452194954131545\n",
      "Gradient Descent(297/499): loss=0.2863958879846564, w0=0.19896514934561368, w1=-0.17404812316005716\n",
      "Gradient Descent(298/499): loss=0.28615725539005105, w0=0.19809397201533302, w1=-0.17357227853406798\n",
      "Gradient Descent(299/499): loss=0.2859199149089792, w0=0.19722475343113488, w1=-0.1730944528428495\n",
      "Gradient Descent(300/499): loss=0.28568385675211744, w0=0.19635748236541253, w1=-0.17261468277724115\n",
      "Gradient Descent(301/499): loss=0.2854490712381696, w0=0.19549214769395587, w1=-0.17213300454569372\n",
      "Gradient Descent(302/499): loss=0.285215548792313, w0=0.19462873839486278, w1=-0.1716494538804662\n",
      "Gradient Descent(303/499): loss=0.28498327994467165, w0=0.19376724354746117, w1=-0.17116406604374826\n",
      "Gradient Descent(304/499): loss=0.2847522553288166, w0=0.19290765233124238, w1=-0.17067687583370736\n",
      "Gradient Descent(305/499): loss=0.28452246568028855, w0=0.1920499540248052, w1=-0.1701879175904628\n",
      "Gradient Descent(306/499): loss=0.2842939018351487, w0=0.1911941380048109, w1=-0.1696972252019863\n",
      "Gradient Descent(307/499): loss=0.28406655472855225, w0=0.1903401937449486, w1=-0.1692048321099303\n",
      "Gradient Descent(308/499): loss=0.2838404153933462, w0=0.1894881108149118, w1=-0.16871077131538478\n",
      "Gradient Descent(309/499): loss=0.28361547495869077, w0=0.18863787887938482, w1=-0.16821507538456312\n",
      "Gradient Descent(310/499): loss=0.28339172464870294, w0=0.18778948769704013, w1=-0.1677177764544177\n",
      "Gradient Descent(311/499): loss=0.2831691557811245, w0=0.1869429271195456, w1=-0.1672189062381862\n",
      "Gradient Descent(312/499): loss=0.28294775976601005, w0=0.18609818709058243, w1=-0.16671849603086886\n",
      "Gradient Descent(313/499): loss=0.2827275281044374, w0=0.18525525764487277, w1=-0.16621657671463816\n",
      "Gradient Descent(314/499): loss=0.28250845238724065, w0=0.18441412890721773, w1=-0.1657131787641806\n",
      "Gradient Descent(315/499): loss=0.2822905242937607, w0=0.18357479109154515, w1=-0.16520833225197237\n",
      "Gradient Descent(316/499): loss=0.2820737355906209, w0=0.18273723449996737, w1=-0.1647020668534887\n",
      "Gradient Descent(317/499): loss=0.2818580781305178, w0=0.1819014495218487, w1=-0.1641944118523482\n",
      "Gradient Descent(318/499): loss=0.28164354385103557, w0=0.18106742663288267, w1=-0.1636853961453925\n",
      "Gradient Descent(319/499): loss=0.2814301247734774, w0=0.18023515639417875, w1=-0.16317504824770224\n",
      "Gradient Descent(320/499): loss=0.2812178130017165, w0=0.1794046294513588, w1=-0.16266339629754967\n",
      "Gradient Descent(321/499): loss=0.28100660072106576, w0=0.17857583653366274, w1=-0.162150468061289\n",
      "Gradient Descent(322/499): loss=0.28079648019716547, w0=0.17774876845306378, w1=-0.16163629093818466\n",
      "Gradient Descent(323/499): loss=0.28058744377488926, w0=0.17692341610339274, w1=-0.16112089196517881\n",
      "Gradient Descent(324/499): loss=0.2803794838772666, w0=0.17609977045947162, w1=-0.16060429782159807\n",
      "Gradient Descent(325/499): loss=0.28017259300442393, w0=0.1752778225762563, w1=-0.16008653483380067\n",
      "Gradient Descent(326/499): loss=0.2799667637325417, w0=0.17445756358798825, w1=-0.15956762897976437\n",
      "Gradient Descent(327/499): loss=0.27976198871282787, w0=0.17363898470735517, w1=-0.15904760589361602\n",
      "Gradient Descent(328/499): loss=0.27955826067050876, w0=0.17282207722466042, w1=-0.15852649087010323\n",
      "Gradient Descent(329/499): loss=0.2793555724038347, w0=0.17200683250700124, w1=-0.158004308869009\n",
      "Gradient Descent(330/499): loss=0.279153916783102, w0=0.17119324199745575, w1=-0.15748108451950976\n",
      "Gradient Descent(331/499): loss=0.27895328674969017, w0=0.17038129721427847, w1=-0.15695684212447752\n",
      "Gradient Descent(332/499): loss=0.2787536753151145, w0=0.16957098975010437, w1=-0.156431605664727\n",
      "Gradient Descent(333/499): loss=0.27855507556009323, w0=0.16876231127116134, w1=-0.15590539880320792\n",
      "Gradient Descent(334/499): loss=0.27835748063362875, w0=0.16795525351649113, w1=-0.1553782448891435\n",
      "Gradient Descent(335/499): loss=0.2781608837521047, w0=0.1671498082971785, w1=-0.15485016696211565\n",
      "Gradient Descent(336/499): loss=0.27796527819839584, w0=0.16634596749558855, w1=-0.1543211877560973\n",
      "Gradient Descent(337/499): loss=0.27777065732099154, w0=0.16554372306461232, w1=-0.15379132970343276\n",
      "Gradient Descent(338/499): loss=0.27757701453313416, w0=0.16474306702692018, w1=-0.1532606149387666\n",
      "Gradient Descent(339/499): loss=0.27738434331197, w0=0.16394399147422353, w1=-0.1527290653029216\n",
      "Gradient Descent(340/499): loss=0.27719263719771253, w0=0.1631464885665442, w1=-0.15219670234672636\n",
      "Gradient Descent(341/499): loss=0.27700188979282026, w0=0.1623505505314916, w1=-0.15166354733479345\n",
      "Gradient Descent(342/499): loss=0.2768120947611858, w0=0.1615561696635479, w1=-0.15112962124924817\n",
      "Gradient Descent(343/499): loss=0.2766232458273383, w0=0.16076333832336057, w1=-0.15059494479340912\n",
      "Gradient Descent(344/499): loss=0.27643533677565735, w0=0.15997204893704273, w1=-0.15005953839542055\n",
      "Gradient Descent(345/499): loss=0.27624836144959886, w0=0.15918229399548095, w1=-0.1495234222118376\n",
      "Gradient Descent(346/499): loss=0.2760623137509342, w0=0.15839406605365058, w1=-0.14898661613116454\n",
      "Gradient Descent(347/499): loss=0.2758771876389991, w0=0.15760735772993828, w1=-0.1484491397773471\n",
      "Gradient Descent(348/499): loss=0.27569297712995416, w0=0.15682216170547209, w1=-0.14791101251321878\n",
      "Gradient Descent(349/499): loss=0.2755096762960583, w0=0.15603847072345844, w1=-0.14737225344390245\n",
      "Gradient Descent(350/499): loss=0.2753272792649515, w0=0.15525627758852678, w1=-0.14683288142016693\n",
      "Gradient Descent(351/499): loss=0.27514578021894803, w0=0.15447557516608074, w1=-0.1462929150417402\n",
      "Gradient Descent(352/499): loss=0.27496517339434257, w0=0.15369635638165682, w1=-0.14575237266057844\n",
      "Gradient Descent(353/499): loss=0.27478545308072455, w0=0.1529186142202897, w1=-0.1452112723840929\n",
      "Gradient Descent(354/499): loss=0.27460661362030375, w0=0.15214234172588462, w1=-0.1446696320783335\n",
      "Gradient Descent(355/499): loss=0.2744286494072455, w0=0.15136753200059638, w1=-0.14412746937113158\n",
      "Gradient Descent(356/499): loss=0.274251554887017, w0=0.15059417820421542, w1=-0.14358480165520027\n",
      "Gradient Descent(357/499): loss=0.2740753245557425, w0=0.14982227355356018, w1=-0.1430416460911949\n",
      "Gradient Descent(358/499): loss=0.2738999529595672, w0=0.1490518113218764, w1=-0.1424980196107324\n",
      "Gradient Descent(359/499): loss=0.2737254346940328, w0=0.14828278483824273, w1=-0.14195393891937147\n",
      "Gradient Descent(360/499): loss=0.2735517644034603, w0=0.14751518748698303, w1=-0.14140942049955285\n",
      "Gradient Descent(361/499): loss=0.27337893678034375, w0=0.1467490127070849, w1=-0.14086448061350135\n",
      "Gradient Descent(362/499): loss=0.2732069465647509, w0=0.14598425399162468, w1=-0.14031913530608897\n",
      "Gradient Descent(363/499): loss=0.27303578854373506, w0=0.14522090488719863, w1=-0.13977340040766073\n",
      "Gradient Descent(364/499): loss=0.2728654575507534, w0=0.1444589589933605, w1=-0.1392272915368225\n",
      "Gradient Descent(365/499): loss=0.2726959484650958, w0=0.143698409962065, w1=-0.13868082410319274\n",
      "Gradient Descent(366/499): loss=0.27252725621132035, w0=0.1429392514971177, w1=-0.13813401331011677\n",
      "Gradient Descent(367/499): loss=0.27235937575869856, w0=0.14218147735363054, w1=-0.13758687415734627\n",
      "Gradient Descent(368/499): loss=0.27219230212066814, w0=0.14142508133748383, w1=-0.137039421443682\n",
      "Gradient Descent(369/499): loss=0.2720260303542931, w0=0.14067005730479354, w1=-0.13649166976958285\n",
      "Gradient Descent(370/499): loss=0.27186055555973376, w0=0.13991639916138507, w1=-0.1359436335397388\n",
      "Gradient Descent(371/499): loss=0.27169587287972163, w0=0.13916410086227224, w1=-0.13539532696561124\n",
      "Gradient Descent(372/499): loss=0.2715319774990449, w0=0.13841315641114255, w1=-0.13484676406793802\n",
      "Gradient Descent(373/499): loss=0.2713688646440389, w0=0.13766355985984743, w1=-0.13429795867920694\n",
      "Gradient Descent(374/499): loss=0.27120652958208624, w0=0.13691530530789897, w1=-0.13374892444609437\n",
      "Gradient Descent(375/499): loss=0.27104496762112196, w0=0.13616838690197125, w1=-0.13319967483187375\n",
      "Gradient Descent(376/499): loss=0.27088417410914767, w0=0.13542279883540798, w1=-0.13265022311878968\n",
      "Gradient Descent(377/499): loss=0.2707241444337517, w0=0.13467853534773497, w1=-0.13210058241040273\n",
      "Gradient Descent(378/499): loss=0.270564874021637, w0=0.13393559072417846, w1=-0.1315507656339008\n",
      "Gradient Descent(379/499): loss=0.2704063583381548, w0=0.13319395929518826, w1=-0.13100078554238212\n",
      "Gradient Descent(380/499): loss=0.2702485928868458, w0=0.13245363543596675, w1=-0.13045065471710532\n",
      "Gradient Descent(381/499): loss=0.2700915732089879, w0=0.13171461356600236, w1=-0.1299003855697121\n",
      "Gradient Descent(382/499): loss=0.2699352948831501, w0=0.13097688814860908, w1=-0.12934999034441774\n",
      "Gradient Descent(383/499): loss=0.2697797535247528, w0=0.1302404536904701, w1=-0.12879948112017517\n",
      "Gradient Descent(384/499): loss=0.2696249447856349, w0=0.1295053047411875, w1=-0.12824886981280723\n",
      "Gradient Descent(385/499): loss=0.2694708643536269, w0=0.12877143589283582, w1=-0.1276981681771136\n",
      "Gradient Descent(386/499): loss=0.2693175079521293, w0=0.12803884177952188, w1=-0.12714738780894663\n",
      "Gradient Descent(387/499): loss=0.2691648713396986, w0=0.12730751707694798, w1=-0.12659654014726254\n",
      "Gradient Descent(388/499): loss=0.2690129503096376, w0=0.1265774565019815, w1=-0.1260456364761421\n",
      "Gradient Descent(389/499): loss=0.2688617406895927, w0=0.1258486548122278, w1=-0.12549468792678767\n",
      "Gradient Descent(390/499): loss=0.26871123834115607, w0=0.12512110680560912, w1=-0.12494370547949031\n",
      "Gradient Descent(391/499): loss=0.26856143915947434, w0=0.1243948073199471, w1=-0.12439269996557405\n",
      "Gradient Descent(392/499): loss=0.268412339072862, w0=0.12366975123255096, w1=-0.12384168206931064\n",
      "Gradient Descent(393/499): loss=0.26826393404242066, w0=0.12294593345980905, w1=-0.12329066232981262\n",
      "Gradient Descent(394/499): loss=0.26811622006166413, w0=0.1222233489567863, w1=-0.12273965114289707\n",
      "Gradient Descent(395/499): loss=0.26796919315614803, w0=0.12150199271682477, w1=-0.12218865876292884\n",
      "Gradient Descent(396/499): loss=0.2678228493831046, w0=0.12078185977114995, w1=-0.12163769530463457\n",
      "Gradient Descent(397/499): loss=0.26767718483108366, w0=0.12006294518848015, w1=-0.12108677074489749\n",
      "Gradient Descent(398/499): loss=0.2675321956195976, w0=0.11934524407464156, w1=-0.12053589492452305\n",
      "Gradient Descent(399/499): loss=0.2673878778987718, w0=0.11862875157218614, w1=-0.11998507754998647\n",
      "Gradient Descent(400/499): loss=0.2672442278489998, w0=0.1179134628600154, w1=-0.11943432819515132\n",
      "Gradient Descent(401/499): loss=0.2671012416806037, w0=0.11719937315300659, w1=-0.11888365630297117\n",
      "Gradient Descent(402/499): loss=0.2669589156334986, w0=0.11648647770164487, w1=-0.11833307118716216\n",
      "Gradient Descent(403/499): loss=0.2668172459768622, w0=0.11577477179165771, w1=-0.11778258203386023\n",
      "Gradient Descent(404/499): loss=0.2666762290088091, w0=0.1150642507436554, w1=-0.1172321979032487\n",
      "Gradient Descent(405/499): loss=0.2665358610560696, w0=0.11435490991277321, w1=-0.11668192773117236\n",
      "Gradient Descent(406/499): loss=0.2663961384736724, w0=0.11364674468832023, w1=-0.11613178033072129\n",
      "Gradient Descent(407/499): loss=0.26625705764463253, w0=0.11293975049342919, w1=-0.11558176439380292\n",
      "Gradient Descent(408/499): loss=0.2661186149796432, w0=0.11223392278471328, w1=-0.11503188849268321\n",
      "Gradient Descent(409/499): loss=0.2659808069167715, w0=0.11152925705192354, w1=-0.11448216108151797\n",
      "Gradient Descent(410/499): loss=0.2658436299211595, w0=0.11082574881761349, w1=-0.11393259049785236\n",
      "Gradient Descent(411/499): loss=0.26570708048472874, w0=0.11012339363680389, w1=-0.11338318496411269\n",
      "Gradient Descent(412/499): loss=0.26557115512588786, w0=0.10942218709665506, w1=-0.11283395258906531\n",
      "Gradient Descent(413/499): loss=0.265435850389246, w0=0.10872212481613885, w1=-0.11228490136926998\n",
      "Gradient Descent(414/499): loss=0.26530116284532945, w0=0.10802320244571846, w1=-0.11173603919049918\n",
      "Gradient Descent(415/499): loss=0.2651670890903019, w0=0.10732541566702736, w1=-0.11118737382915418\n",
      "Gradient Descent(416/499): loss=0.2650336257456884, w0=0.10662876019255663, w1=-0.11063891295364572\n",
      "Gradient Descent(417/499): loss=0.2649007694581041, w0=0.10593323176534059, w1=-0.110090664125774\n",
      "Gradient Descent(418/499): loss=0.26476851689898545, w0=0.10523882615865147, w1=-0.10954263480207144\n",
      "Gradient Descent(419/499): loss=0.2646368647643261, w0=0.10454553917569165, w1=-0.10899483233514785\n",
      "Gradient Descent(420/499): loss=0.2645058097744157, w0=0.10385336664929545, w1=-0.10844726397499622\n",
      "Gradient Descent(421/499): loss=0.26437534867358264, w0=0.10316230444162783, w1=-0.10789993687030401\n",
      "Gradient Descent(422/499): loss=0.26424547822994027, w0=0.10247234844389327, w1=-0.10735285806972275\n",
      "Gradient Descent(423/499): loss=0.2641161952351364, w0=0.1017834945760406, w1=-0.10680603452314681\n",
      "Gradient Descent(424/499): loss=0.26398749650410636, w0=0.10109573878647897, w1=-0.10625947308294742\n",
      "Gradient Descent(425/499): loss=0.263859378874829, w0=0.10040907705178852, w1=-0.10571318050522\n",
      "Gradient Descent(426/499): loss=0.2637318392080868, w0=0.09972350537644344, w1=-0.10516716345098334\n",
      "Gradient Descent(427/499): loss=0.26360487438722846, w0=0.09903901979252829, w1=-0.10462142848739671\n",
      "Gradient Descent(428/499): loss=0.26347848131793544, w0=0.09835561635946796, w1=-0.10407598208892473\n",
      "Gradient Descent(429/499): loss=0.2633526569279901, w0=0.0976732911637495, w1=-0.10353083063852546\n",
      "Gradient Descent(430/499): loss=0.2632273981670492, w0=0.09699204031865895, w1=-0.10298598042878149\n",
      "Gradient Descent(431/499): loss=0.2631027020064187, w0=0.09631185996400837, w1=-0.10244143766306028\n",
      "Gradient Descent(432/499): loss=0.26297856543883247, w0=0.09563274626587961, w1=-0.10189720845661188\n",
      "Gradient Descent(433/499): loss=0.2628549854782329, w0=0.09495469541635619, w1=-0.10135329883770275\n",
      "Gradient Descent(434/499): loss=0.2627319591595565, w0=0.09427770363327402, w1=-0.1008097147486805\n",
      "Gradient Descent(435/499): loss=0.26260948353851965, w0=0.09360176715995797, w1=-0.1002664620470826\n",
      "Gradient Descent(436/499): loss=0.2624875556914098, w0=0.09292688226497944, w1=-0.09972354650666822\n",
      "Gradient Descent(437/499): loss=0.2623661727148775, w0=0.09225304524189734, w1=-0.09918097381850327\n",
      "Gradient Descent(438/499): loss=0.26224533172573217, w0=0.09158025240902261, w1=-0.09863874959195953\n",
      "Gradient Descent(439/499): loss=0.26212502986074027, w0=0.09090850010916322, w1=-0.09809687935577753\n",
      "Gradient Descent(440/499): loss=0.26200526427642645, w0=0.09023778470939585, w1=-0.0975553685590328\n",
      "Gradient Descent(441/499): loss=0.26188603214887696, w0=0.08956810260081442, w1=-0.09701422257217825\n",
      "Gradient Descent(442/499): loss=0.26176733067354563, w0=0.08889945019830911, w1=-0.09647344668797748\n",
      "Gradient Descent(443/499): loss=0.26164915706506314, w0=0.08823182394031807, w1=-0.09593304612252826\n",
      "Gradient Descent(444/499): loss=0.26153150855704754, w0=0.08756522028861392, w1=-0.09539302601616245\n",
      "Gradient Descent(445/499): loss=0.2614143824019183, w0=0.08689963572805816, w1=-0.09485339143445265\n",
      "Gradient Descent(446/499): loss=0.2612977758707122, w0=0.08623506676639539, w1=-0.09431414736907813\n",
      "Gradient Descent(447/499): loss=0.2611816862529026, w0=0.08557150993401001, w1=-0.09377529873881661\n",
      "Gradient Descent(448/499): loss=0.26106611085621917, w0=0.08490896178372846, w1=-0.0932368503903754\n",
      "Gradient Descent(449/499): loss=0.2609510470064721, w0=0.08424741889057748, w1=-0.0926988070993707\n",
      "Gradient Descent(450/499): loss=0.26083649204737724, w0=0.08358687785159488, w1=-0.09216117357112277\n",
      "Gradient Descent(451/499): loss=0.26072244334038386, w0=0.08292733528558874, w1=-0.09162395444162538\n",
      "Gradient Descent(452/499): loss=0.2606088982645048, w0=0.0822687878329571, w1=-0.09108715427830343\n",
      "Gradient Descent(453/499): loss=0.26049585421614885, w0=0.08161123215544736, w1=-0.09055077758097561\n",
      "Gradient Descent(454/499): loss=0.2603833086089548, w0=0.08095466493598558, w1=-0.09001482878257258\n",
      "Gradient Descent(455/499): loss=0.2602712588736281, w0=0.0802990828784351, w1=-0.08947931225009617\n",
      "Gradient Descent(456/499): loss=0.26015970245777975, w0=0.07964448270743624, w1=-0.08894423228529563\n",
      "Gradient Descent(457/499): loss=0.26004863682576673, w0=0.07899086116816306, w1=-0.08840959312562739\n",
      "Gradient Descent(458/499): loss=0.25993805945853465, w0=0.07833821502617443, w1=-0.0878753989448864\n",
      "Gradient Descent(459/499): loss=0.2598279678534628, w0=0.07768654106716769, w1=-0.08734165385417074\n",
      "Gradient Descent(460/499): loss=0.25971835952421085, w0=0.07703583609684218, w1=-0.08680836190246466\n",
      "Gradient Descent(461/499): loss=0.2596092320005673, w0=0.07638609694064852, w1=-0.08627552707761302\n",
      "Gradient Descent(462/499): loss=0.2595005828283004, w0=0.07573732044366571, w1=-0.08574315330685162\n",
      "Gradient Descent(463/499): loss=0.2593924095690104, w0=0.07508950347034445, w1=-0.08521124445779751\n",
      "Gradient Descent(464/499): loss=0.259284709799984, w0=0.07444264290439957, w1=-0.0846798043389212\n",
      "Gradient Descent(465/499): loss=0.2591774811140507, w0=0.07379673564854528, w1=-0.08414883670055978\n",
      "Gradient Descent(466/499): loss=0.2590707211194408, w0=0.07315177862440486, w1=-0.08361834523532463\n",
      "Gradient Descent(467/499): loss=0.2589644274396449, w0=0.07250776877223558, w1=-0.0830883335791455\n",
      "Gradient Descent(468/499): loss=0.25885859771327574, w0=0.07186470305085803, w1=-0.08255880531160574\n",
      "Gradient Descent(469/499): loss=0.25875322959393193, w0=0.07122257843736789, w1=-0.08202976395702707\n",
      "Gradient Descent(470/499): loss=0.25864832075006206, w0=0.07058139192708766, w1=-0.08150121298472303\n",
      "Gradient Descent(471/499): loss=0.25854386886483205, w0=0.06994114053326218, w1=-0.08097315581013548\n",
      "Gradient Descent(472/499): loss=0.25843987163599336, w0=0.06930182128703602, w1=-0.0804455957949951\n",
      "Gradient Descent(473/499): loss=0.2583363267757528, w0=0.06866343123712891, w1=-0.07991853624852306\n",
      "Gradient Descent(474/499): loss=0.25823323201064435, w0=0.0680259674498428, w1=-0.07939198042748487\n",
      "Gradient Descent(475/499): loss=0.2581305850814026, w0=0.06738942700871269, w1=-0.078865931537473\n",
      "Gradient Descent(476/499): loss=0.2580283837428368, w0=0.06675380701454808, w1=-0.07834039273283788\n",
      "Gradient Descent(477/499): loss=0.25792662576370784, w0=0.06611910458505392, w1=-0.07781536711807023\n",
      "Gradient Descent(478/499): loss=0.2578253089266055, w0=0.06548531685491203, w1=-0.07729085774758956\n",
      "Gradient Descent(479/499): loss=0.25772443102782844, w0=0.06485244097536577, w1=-0.07676686762724858\n",
      "Gradient Descent(480/499): loss=0.2576239898772645, w0=0.06422047411434839, w1=-0.07624339971395573\n",
      "Gradient Descent(481/499): loss=0.25752398329827286, w0=0.06358941345602367, w1=-0.07572045691732822\n",
      "Gradient Descent(482/499): loss=0.2574244091275681, w0=0.06295925620096941, w1=-0.07519804209912048\n",
      "Gradient Descent(483/499): loss=0.25732526521510485, w0=0.062329999565664984, w1=-0.07467615807505741\n",
      "Gradient Descent(484/499): loss=0.2572265494239653, w0=0.06170164078273964, w1=-0.07415480761403503\n",
      "Gradient Descent(485/499): loss=0.25712825963024527, w0=0.0610741771003962, w1=-0.0736339934401718\n",
      "Gradient Descent(486/499): loss=0.257030393722945, w0=0.06044760578273594, w1=-0.07311371823174079\n",
      "Gradient Descent(487/499): loss=0.2569329496038592, w0=0.05982192410910532, w1=-0.07259398462348438\n",
      "Gradient Descent(488/499): loss=0.25683592518746834, w0=0.059197129374511576, w1=-0.0720747952052293\n",
      "Gradient Descent(489/499): loss=0.2567393184008327, w0=0.0585732188888769, w1=-0.0715561525245188\n",
      "Gradient Descent(490/499): loss=0.2566431271834861, w0=0.057950189977561704, w1=-0.07103805908485199\n",
      "Gradient Descent(491/499): loss=0.25654734948733193, w0=0.057328039980507506, w1=-0.07052051734869845\n",
      "Gradient Descent(492/499): loss=0.25645198327653973, w0=0.056706766252888216, w1=-0.07000352973529145\n",
      "Gradient Descent(493/499): loss=0.2563570265274439, w0=0.05608636616411929, w1=-0.06948709862410309\n",
      "Gradient Descent(494/499): loss=0.2562624772284424, w0=0.05546683709866151, w1=-0.06897122635210642\n",
      "Gradient Descent(495/499): loss=0.25616833337989753, w0=0.0548481764548694, w1=-0.06845591521780493\n",
      "Gradient Descent(496/499): loss=0.2560745929940379, w0=0.05423038164597698, w1=-0.06794116747786164\n",
      "Gradient Descent(497/499): loss=0.25598125409486044, w0=0.05361345009875304, w1=-0.06742698535179534\n",
      "Gradient Descent(498/499): loss=0.25588831471803564, w0=0.05299737925470423, w1=-0.06691337101785394\n",
      "Gradient Descent(499/499): loss=0.25579577291081107, w0=0.05238216656849813, w1=-0.06640032661851313\n"
     ]
    }
   ],
   "source": [
    "w_logreg, loss_logreg = f.logistic_regression(y_train_train_lg, tX_train,np.ones(22),500, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9054255151525762\n",
      "F1 score:  0.008443465491923641\n"
     ]
    }
   ],
   "source": [
    "y_pred = tX_train_test.dot(w_logreg)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "y_pred = np.where(y_pred == 1, 1, -1)\n",
    "\n",
    "_,_,_,_,f1 = f.confusion_matrix(y_train_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", np.sum(y_pred == y_train_test)/len(y_train_test))\n",
    "\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [ 0.05238217 -0.06640033  0.02116758 -0.00274931  0.49022348  0.18084201\n",
      " -0.32492353 -0.32597621 -0.52899741 -0.28392638 -0.42508117 -0.12185999\n",
      " -0.17506193 -0.1238589  -0.38367223  0.526824    0.11760777  0.71570066\n",
      "  0.07131136  0.13223593  0.25722637  0.26169432] \n",
      " Loss =  0.25579577291081107 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.00571407674233628\n"
     ]
    }
   ],
   "source": [
    "y_test_logreg = tX_test.dot(w_logreg)\n",
    "y_test_logreg = np.where(y_test_logreg > 0.5, 1, 0)\n",
    "\n",
    "print('weights = \\n', w_logreg,'\\n Loss = ', loss_logreg,'\\n*****************************************************************************',\n",
    "        ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train== 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_logreg == 1)/len(y_test_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=22.194147963526632, w0=0.9078302070838765, w1=0.7734125746988572\n",
      "Gradient Descent(1/99): loss=9.553879248884057, w0=0.815752585328088, w1=0.5470517393869585\n",
      "Gradient Descent(2/99): loss=1.275628811054163, w0=0.7239263887868533, w1=0.3212723314856586\n",
      "Gradient Descent(3/99): loss=1.0866978531591471, w0=0.7086874934055847, w1=0.2906129836396613\n",
      "Gradient Descent(4/99): loss=1.0164421060079472, w0=0.70138209336078, w1=0.28243419986045\n",
      "Gradient Descent(5/99): loss=0.9599804311227546, w0=0.6950858445218292, w1=0.2762006208844834\n",
      "Gradient Descent(6/99): loss=0.9139561901309793, w0=0.6892038677378615, w1=0.27026134063383106\n",
      "Gradient Descent(7/99): loss=0.8756876908497888, w0=0.6835594191296854, w1=0.26418521230851383\n",
      "Gradient Descent(8/99): loss=0.8427952517628434, w0=0.6781341712395589, w1=0.25805240609881636\n",
      "Gradient Descent(9/99): loss=0.8136485893783099, w0=0.6729235495850487, w1=0.25198839440245785\n",
      "Gradient Descent(10/99): loss=0.7872596314963121, w0=0.6679004489666673, w1=0.24602570022783482\n",
      "Gradient Descent(11/99): loss=0.7630521631260537, w0=0.6630278695695623, w1=0.24013927232355703\n",
      "Gradient Descent(12/99): loss=0.7406906590852991, w0=0.65827018378082, w1=0.2342862987032837\n",
      "Gradient Descent(13/99): loss=0.7199736422405703, w0=0.6535972346543272, w1=0.22842424100008393\n",
      "Gradient Descent(14/99): loss=0.7007705100177021, w0=0.6489852834921294, w1=0.22251796446624386\n",
      "Gradient Descent(15/99): loss=0.682984883119144, w0=0.6444168382701644, w1=0.2165423119380165\n",
      "Gradient Descent(16/99): loss=0.6665339877727033, w0=0.6398800532775173, w1=0.2104825381767696\n",
      "Gradient Descent(17/99): loss=0.6513379189249903, w0=0.6353679406023933, w1=0.20433359741054846\n",
      "Gradient Descent(18/99): loss=0.6373150430886527, w0=0.6308774908160648, w1=0.19809876130928145\n",
      "Gradient Descent(19/99): loss=0.6243810890759683, w0=0.6264087630647388, w1=0.19178786560168012\n",
      "Gradient Descent(20/99): loss=0.6124502291961634, w0=0.6219640005456845, w1=0.18541542404926467\n",
      "Gradient Descent(21/99): loss=0.6014369831010037, w0=0.6175468249440217, w1=0.17899881184111607\n",
      "Gradient Descent(22/99): loss=0.5912581999398969, w0=0.6131615525001921, w1=0.17255667164151317\n",
      "Gradient Descent(23/99): loss=0.5818347129945973, w0=0.6088126560094888, w1=0.16610763379895813\n",
      "Gradient Descent(24/99): loss=0.5730925077835037, w0=0.6045043768518495, w1=0.15966938066083958\n",
      "Gradient Descent(25/99): loss=0.564963400344037, w0=0.6002404744449946, w1=0.15325803603942045\n",
      "Gradient Descent(26/99): loss=0.5573853021708367, w0=0.5960240900617446, w1=0.14688783039884082\n",
      "Gradient Descent(27/99): loss=0.5503021751681562, w0=0.5918576978744249, w1=0.14057097945871888\n",
      "Gradient Descent(28/99): loss=0.5436637763715318, w0=0.5877431169056977, w1=0.13431771413833124\n",
      "Gradient Descent(29/99): loss=0.5374252749429663, w0=0.5836815613090558, w1=0.12813640776022386\n",
      "Gradient Descent(30/99): loss=0.5315468033914791, w0=0.5796737113445247, w1=0.12203375771542892\n",
      "Gradient Descent(31/99): loss=0.5259929862867602, w0=0.5757197923576791, w1=0.11601499033284875\n",
      "Gradient Descent(32/99): loss=0.5207324747920306, w0=0.5718196533495268, w1=0.11008406782334494\n",
      "Gradient Descent(33/99): loss=0.5157375042852071, w0=0.5679728400853506, w1=0.10424388422056054\n",
      "Gradient Descent(34/99): loss=0.5109834845858735, w0=0.564178660131862, w1=0.09849644317283533\n",
      "Gradient Descent(35/99): loss=0.506448627101607, w0=0.5604362388631988, w1=0.09284301452753174\n",
      "Gradient Descent(36/99): loss=0.5021136098548425, w0=0.5567445665110554, w1=0.08728426927812066\n",
      "Gradient Descent(37/99): loss=0.4979612792932531, w0=0.5531025369219948, w1=0.08182039400107965\n",
      "Gradient Descent(38/99): loss=0.49397638660691645, w0=0.5495089789686349, w1=0.07645118672368845\n",
      "Gradient Descent(39/99): loss=0.4901453556769832, w0=0.5459626816508566, w1=0.07117613649225749\n",
      "Gradient Descent(40/99): loss=0.48645607955601244, w0=0.542462413897542, w1=0.06599448894085615\n",
      "Gradient Descent(41/99): loss=0.48289774238807587, w0=0.5390069399923476, w1=0.060905300022969025\n",
      "Gradient Descent(42/99): loss=0.4794606638219823, w0=0.535595031433224, w1=0.05590747984804375\n",
      "Gradient Descent(43/99): loss=0.47613616319068375, w0=0.5322254759157485, w1=0.05099982831487743\n",
      "Gradient Descent(44/99): loss=0.472916440983363, w0=0.5288970840165491, w1=0.046181063985439134\n",
      "Gradient Descent(45/99): loss=0.4697944753982461, w0=0.5256086940509564, w1=0.04144984741274871\n",
      "Gradient Descent(46/99): loss=0.466763932018341, w0=0.522359175490661, w1=0.03680479993239445\n",
      "Gradient Descent(47/99): loss=0.4638190848902644, w0=0.5191474312526185, w1=0.03224451875127755\n",
      "Gradient Descent(48/99): loss=0.46095474750373316, w0=0.5159723991086984, w1=0.027767589018269062\n",
      "Gradient Descent(49/99): loss=0.4581662123645181, w0=0.5128330524150679, w1=0.02337259343717488\n",
      "Gradient Descent(50/99): loss=0.45544919802680656, w0=0.5097284003193888, w1=0.01905811987963782\n",
      "Gradient Descent(51/99): loss=0.4527998026031313, w0=0.5066574875709856, w1=0.014822767371194355\n",
      "Gradient Descent(52/99): loss=0.45021446290289685, w0=0.5036193940327931, w1=0.010665150754677125\n",
      "Gradient Descent(53/99): loss=0.44768991846599443, w0=0.5006132339728735, w1=0.006583904278871279\n",
      "Gradient Descent(54/99): loss=0.4452231798579363, w0=0.4976381551965722, w1=0.0025776843145166144\n",
      "Gradient Descent(55/99): loss=0.44281150067922703, w0=0.4946933380671083, w1=-0.0013548286375189835\n",
      "Gradient Descent(56/99): loss=0.4404523528160645, w0=0.491777994451867, w1=-0.005214928511378793\n",
      "Gradient Descent(57/99): loss=0.4381434045234659, w0=0.4888913666233244, w1=-0.009003882704813532\n",
      "Gradient Descent(58/99): loss=0.43588250098699294, w0=0.4860327261369367, w1=-0.012722931481458213\n",
      "Gradient Descent(59/99): loss=0.4336676470566002, w0=0.48320137270310676, w1=-0.016373287673378194\n",
      "Gradient Descent(60/99): loss=0.43149699188686036, w0=0.48039663306622116, w1=-0.01995613662382504\n",
      "Gradient Descent(61/99): loss=0.4293688152528574, w0=0.4776178599004986, w1=-0.02347263632105126\n",
      "Gradient Descent(62/99): loss=0.4272815153412042, w0=0.47486443072982926, w1=-0.026923917682954025\n",
      "Gradient Descent(63/99): loss=0.4252335978416184, w0=0.47213574687677273, w1=-0.030311084959623896\n",
      "Gradient Descent(64/99): loss=0.42322366618689583, w0=0.4694312324443036, w1=-0.03363521622686455\n",
      "Gradient Descent(65/99): loss=0.4212504128084548, w0=0.4667503333326631, w1=-0.03689736394866467\n",
      "Gradient Descent(66/99): loss=0.41931261129132963, w0=0.4640925162927164, w1=-0.040098555590642404\n",
      "Gradient Descent(67/99): loss=0.4174091093269474, w0=0.4614572680164787, w1=-0.04323979426980295\n",
      "Gradient Descent(68/99): loss=0.41553882237453665, w0=0.4588440942649029, w1=-0.04632205942868239\n",
      "Gradient Descent(69/99): loss=0.41370072795288254, w0=0.4562525190325975, w1=-0.04934630752420102\n",
      "Gradient Descent(70/99): loss=0.411893860493566, w0=0.4536820837488183, w1=-0.05231347272340181\n",
      "Gradient Descent(71/99): loss=0.4101173066950321, w0=0.4511323465138466, w1=-0.05522446759977733\n",
      "Gradient Descent(72/99): loss=0.408370201323981, w0=0.4486028813696967, w1=-0.05808018382514618\n",
      "Gradient Descent(73/99): loss=0.4066517234168004, w0=0.4460932776039839, w1=-0.06088149285307676\n",
      "Gradient Descent(74/99): loss=0.40496109283920984, w0=0.4436031390857092, w1=-0.06362924659071015\n",
      "Gradient Descent(75/99): loss=0.40329756716705006, w0=0.44113208363168, w1=-0.06632427805653666\n",
      "Gradient Descent(76/99): loss=0.40166043885532116, w0=0.43867974240226737, w1=-0.06896740202225896\n",
      "Gradient Descent(77/99): loss=0.40004903266624, w0=0.43624575932520654, w1=-0.07155941563734916\n",
      "Gradient Descent(78/99): loss=0.3984627033303077, w0=0.43382979054616466, w1=-0.07410109903529773\n",
      "Gradient Descent(79/99): loss=0.3969008334172018, w0=0.4314315039048277, w1=-0.07659321592086968\n",
      "Gradient Descent(80/99): loss=0.3953628313958196, w0=0.42905057843529587, w1=-0.07903651413794541\n",
      "Gradient Descent(81/99): loss=0.3938481298649892, w0=0.4266867038896176, w1=-0.08143172621773424\n",
      "Gradient Descent(82/99): loss=0.3923561839383216, w0=0.424339580283338, w1=-0.08377956990732155\n",
      "Gradient Descent(83/99): loss=0.3908864697684022, w0=0.42200891746198416, w1=-0.08608074867864826\n",
      "Gradient Descent(84/99): loss=0.38943848319704183, w0=0.4196944346874585, w1=-0.08833595221813327\n",
      "Gradient Descent(85/99): loss=0.3880117385196771, w0=0.4173958602433593, w1=-0.09054585689723742\n",
      "Gradient Descent(86/99): loss=0.386605767353205, w0=0.41511293105829594, w1=-0.09271112622433711\n",
      "Gradient Descent(87/99): loss=0.38522011759762, w0=0.4128453923463121, w1=-0.09483241127833028\n",
      "Gradient Descent(88/99): loss=0.3838543524827734, w0=0.4105929972635772, w1=-0.09691035112443788\n",
      "Gradient Descent(89/99): loss=0.38250804969243635, w0=0.40835550658054826, w1=-0.0989455732126953\n",
      "Gradient Descent(90/99): loss=0.3811808005586051, w0=0.40613268836884925, w1=-0.10093869375964962\n",
      "Gradient Descent(91/99): loss=0.3798722093196763, w0=0.4039243177021517, w1=-0.10289031811379364\n",
      "Gradient Descent(92/99): loss=0.3785818924367302, w0=0.40173017637038205, w1=-0.10480104110527617\n",
      "Gradient Descent(93/99): loss=0.37730947796271175, w0=0.39955005260661536, w1=-0.10667144738043258\n",
      "Gradient Descent(94/99): loss=0.37605460495979, w0=0.3973837408260507, w1=-0.10850211172167912\n",
      "Gradient Descent(95/99): loss=0.37481692296062286, w0=0.3952310413764953, w1=-0.11029359935331212\n",
      "Gradient Descent(96/99): loss=0.37359609146965395, w0=0.39309176029981563, w1=-0.11204646623374726\n",
      "Gradient Descent(97/99): loss=0.37239177950092506, w0=0.3909657091038436, w1=-0.1137612593347262\n",
      "Gradient Descent(98/99): loss=0.3712036651492199, w0=0.38885270454425036, w1=-0.11543851690800935\n",
      "Gradient Descent(99/99): loss=0.3700314351916368, w0=0.3867525684159301, w1=-0.11707876874006173\n",
      "Gradient Descent(100/99): loss=0.3688747847169664, w0=0.38466512735345637, w1=-0.1186825363952288\n"
     ]
    }
   ],
   "source": [
    "w_reg_logreg, loss_reg_logreg = f.reg_logistic_regression(y_train_processed_logreg, tX_train, 0.01, initial_w, 100, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [ 0.38466513 -0.11868254  0.03348657 -0.02180457  0.70951366  0.47367152\n",
      "  0.32920329  0.29529946 -0.24753073  0.41312117 -0.88487795 -0.42905013\n",
      "  0.33098771  0.4264872   0.25716685  0.62096664  0.05302878  0.75844239\n",
      "  0.40304051  0.55241231  0.64394116  0.65871171] \n",
      " Loss =  0.3688747847169664 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.027372713226487717\n"
     ]
    }
   ],
   "source": [
    "y_test_reg_logreg = tX_test.dot(w_reg_logreg)\n",
    "y_test_reg_logreg = np.where(y_test_reg_logreg > 0.5, 1, 0)\n",
    "\n",
    "print('weights = \\n', w_reg_logreg,'\\n Loss = ', loss_reg_logreg,'\\n*****************************************************************************',\n",
    "        ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train == 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_reg_logreg == 1)/len(y_test_reg_logreg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sub = np.where(y_test_reg_logreg == 1, 1, -1)\n",
    "h.create_csv_submission(test_ids, y_sub, 'submission_reg_logreg2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
