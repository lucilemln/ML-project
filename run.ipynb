{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "try:\n",
    "    import importlib\n",
    "    importlib.reload(h)\n",
    "    importlib.reload(f)\n",
    "    importlib.reload(d)\n",
    "except NameError: # It hasn't been imported yet\n",
    "    import helpers as h\n",
    "    import implementations as f\n",
    "    import data_processing as d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing and feature selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#For this to work, the data folder needs to be one level above the project folder and the folder name needs\n",
    "#to be 'data'\n",
    "data_folder = '../data/'\n",
    "x_train, x_test, y_train, train_ids, test_ids = h.load_csv_data(data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = h.load_csv_data(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/y_train.npy', y_train)\n",
    "np.save('../data/x_train.npy', x_train)\n",
    "np.save('../data/x_test.npy', x_test)\n",
    "np.save('../data/train_ids.npy', train_ids)\n",
    "np.save('../data/test_ids.npy', test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = np.load(\"../data/x_train.npy\")\n",
    "x_test = np.load(\"../data/x_test.npy\")\n",
    "y_train = np.load(\"../data/y_train.npy\")\n",
    "train_ids = np.load(\"../data/train_ids.npy\")\n",
    "test_ids = np.load(\"../data/test_ids.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['_STATE', 'FMONTH', 'IDATE', 'IMONTH', 'IDAY', 'IYEAR', 'DISPCODE',\n",
       "       'SEQNO', '_PSU', 'CTELENUM', 'PVTRESD1', 'COLGHOUS', 'STATERES',\n",
       "       'CELLFON3', 'LADULT', 'NUMADULT', 'NUMMEN', 'NUMWOMEN', 'CTELNUM1',\n",
       "       'CELLFON2', 'CADULT', 'PVTRESD2', 'CCLGHOUS', 'CSTATE', 'LANDLINE',\n",
       "       'HHADULT', 'GENHLTH', 'PHYSHLTH', 'MENTHLTH', 'POORHLTH',\n",
       "       'HLTHPLN1', 'PERSDOC2', 'MEDCOST', 'CHECKUP1', 'BPHIGH4', 'BPMEDS',\n",
       "       'BLOODCHO', 'CHOLCHK', 'TOLDHI2', 'CVDSTRK3', 'ASTHMA3', 'ASTHNOW',\n",
       "       'CHCSCNCR', 'CHCOCNCR', 'CHCCOPD1', 'HAVARTH3', 'ADDEPEV2',\n",
       "       'CHCKIDNY', 'DIABETE3', 'DIABAGE2', 'SEX', 'MARITAL', 'EDUCA',\n",
       "       'RENTHOM1', 'NUMHHOL2', 'NUMPHON2', 'CPDEMO1', 'VETERAN3',\n",
       "       'EMPLOY1', 'CHILDREN', 'INCOME2', 'INTERNET', 'WEIGHT2', 'HEIGHT3',\n",
       "       'PREGNANT', 'QLACTLM2', 'USEEQUIP', 'BLIND', 'DECIDE', 'DIFFWALK',\n",
       "       'DIFFDRES', 'DIFFALON', 'SMOKE100', 'SMOKDAY2', 'STOPSMK2',\n",
       "       'LASTSMK2', 'USENOW3', 'ALCDAY5', 'AVEDRNK2', 'DRNK3GE5',\n",
       "       'MAXDRNKS', 'FRUITJU1', 'FRUIT1', 'FVBEANS', 'FVGREEN', 'FVORANG',\n",
       "       'VEGETAB1', 'EXERANY2', 'EXRACT11', 'EXEROFT1', 'EXERHMM1',\n",
       "       'EXRACT21', 'EXEROFT2', 'EXERHMM2', 'STRENGTH', 'LMTJOIN3',\n",
       "       'ARTHDIS2', 'ARTHSOCL', 'JOINPAIN', 'SEATBELT', 'FLUSHOT6',\n",
       "       'FLSHTMY2', 'IMFVPLAC', 'PNEUVAC3', 'HIVTST6', 'HIVTSTD3',\n",
       "       'WHRTST10', 'PDIABTST', 'PREDIAB1', 'INSULIN', 'BLDSUGAR',\n",
       "       'FEETCHK2', 'DOCTDIAB', 'CHKHEMO3', 'FEETCHK', 'EYEEXAM',\n",
       "       'DIABEYE', 'DIABEDU', 'CAREGIV1', 'CRGVREL1', 'CRGVLNG1',\n",
       "       'CRGVHRS1', 'CRGVPRB1', 'CRGVPERS', 'CRGVHOUS', 'CRGVMST2',\n",
       "       'CRGVEXPT', 'VIDFCLT2', 'VIREDIF3', 'VIPRFVS2', 'VINOCRE2',\n",
       "       'VIEYEXM2', 'VIINSUR2', 'VICTRCT4', 'VIGLUMA2', 'VIMACDG2',\n",
       "       'CIMEMLOS', 'CDHOUSE', 'CDASSIST', 'CDHELP', 'CDSOCIAL',\n",
       "       'CDDISCUS', 'WTCHSALT', 'LONGWTCH', 'DRADVISE', 'ASTHMAGE',\n",
       "       'ASATTACK', 'ASERVIST', 'ASDRVIST', 'ASRCHKUP', 'ASACTLIM',\n",
       "       'ASYMPTOM', 'ASNOSLEP', 'ASTHMED3', 'ASINHALR', 'HAREHAB1',\n",
       "       'STREHAB1', 'CVDASPRN', 'ASPUNSAF', 'RLIVPAIN', 'RDUCHART',\n",
       "       'RDUCSTRK', 'ARTTODAY', 'ARTHWGT', 'ARTHEXER', 'ARTHEDU',\n",
       "       'TETANUS', 'HPVADVC2', 'HPVADSHT', 'SHINGLE2', 'HADMAM', 'HOWLONG',\n",
       "       'HADPAP2', 'LASTPAP2', 'HPVTEST', 'HPLSTTST', 'HADHYST2',\n",
       "       'PROFEXAM', 'LENGEXAM', 'BLDSTOOL', 'LSTBLDS3', 'HADSIGM3',\n",
       "       'HADSGCO1', 'LASTSIG3', 'PCPSAAD2', 'PCPSADI1', 'PCPSARE1',\n",
       "       'PSATEST1', 'PSATIME', 'PCPSARS1', 'PCPSADE1', 'PCDMDECN',\n",
       "       'SCNTMNY1', 'SCNTMEL1', 'SCNTPAID', 'SCNTWRK1', 'SCNTLPAD',\n",
       "       'SCNTLWK1', 'SXORIENT', 'TRNSGNDR', 'RCSGENDR', 'RCSRLTN2',\n",
       "       'CASTHDX2', 'CASTHNO2', 'EMTSUPRT', 'LSATISFY', 'ADPLEASR',\n",
       "       'ADDOWN', 'ADSLEEP', 'ADENERGY', 'ADEAT1', 'ADFAIL', 'ADTHINK',\n",
       "       'ADMOVE', 'MISTMNT', 'ADANXEV', 'QSTVER', 'QSTLANG', 'MSCODE',\n",
       "       '_STSTR', '_STRWT', '_RAWRAKE', '_WT2RAKE', '_CHISPNC', '_CRACE1',\n",
       "       '_CPRACE', '_CLLCPWT', '_DUALUSE', '_DUALCOR', '_LLCPWT',\n",
       "       '_RFHLTH', '_HCVU651', '_RFHYPE5', '_CHOLCHK', '_RFCHOL',\n",
       "       '_LTASTH1', '_CASTHM1', '_ASTHMS1', '_DRDXAR1', '_PRACE1',\n",
       "       '_MRACE1', '_HISPANC', '_RACE', '_RACEG21', '_RACEGR3', '_RACE_G1',\n",
       "       '_AGEG5YR', '_AGE65YR', '_AGE80', '_AGE_G', 'HTIN4', 'HTM4',\n",
       "       'WTKG3', '_BMI5', '_BMI5CAT', '_RFBMI5', '_CHLDCNT', '_EDUCAG',\n",
       "       '_INCOMG', '_SMOKER3', '_RFSMOK3', 'DRNKANY5', 'DROCDY3_',\n",
       "       '_RFBING5', '_DRNKWEK', '_RFDRHV5', 'FTJUDA1_', 'FRUTDA1_',\n",
       "       'BEANDAY_', 'GRENDAY_', 'ORNGDAY_', 'VEGEDA1_', '_MISFRTN',\n",
       "       '_MISVEGN', '_FRTRESP', '_VEGRESP', '_FRUTSUM', '_VEGESUM',\n",
       "       '_FRTLT1', '_VEGLT1', '_FRT16', '_VEG23', '_FRUITEX', '_VEGETEX',\n",
       "       '_TOTINDA', 'METVL11_', 'METVL21_', 'MAXVO2_', 'FC60_', 'ACTIN11_',\n",
       "       'ACTIN21_', 'PADUR1_', 'PADUR2_', 'PAFREQ1_', 'PAFREQ2_',\n",
       "       '_MINAC11', '_MINAC21', 'STRFREQ_', 'PAMISS1_', 'PAMIN11_',\n",
       "       'PAMIN21_', 'PA1MIN_', 'PAVIG11_', 'PAVIG21_', 'PA1VIGM_',\n",
       "       '_PACAT1', '_PAINDX1', '_PA150R2', '_PA300R2', '_PA30021',\n",
       "       '_PASTRNG', '_PAREC1', '_PASTAE1', '_LMTACT1', '_LMTWRK1',\n",
       "       '_LMTSCL1', '_RFSEAT2', '_RFSEAT3', '_FLSHOT6', '_PNEUMO2',\n",
       "       '_AIDTST3'], dtype='<U8')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features_named all the features names and remove the ID column\n",
    "features_name = np.genfromtxt('../data/x_train.csv', delimiter=',', dtype=str, max_rows=1)[1:] \n",
    "features_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GENHLTH'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_name[26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "one paper on internet suggests to use these features : \n",
    "\n",
    " _RFHYPE5, TOLDHI2, _CHOLCHK, _BMI5, SMOKE100, CVDSTRK3, DIABETE3, _TOTINDA, _FRTLT1, _VEGLT1, _RFDRHV5, HLTHPLN1, MEDCOST, GENHLTH, MENTHLTH, PHYSHLTH, DIFFWALK, SEX, _AGEG5YR, EDUCA, and INCOME2\n",
    "\n",
    "We apply a mask to get only these important features.\n",
    "\n",
    "Then using we use our preprocessing function. For feature where the answer is yes or no we make the data binary, ordinal (categorical) variables ares changed to 0,1,2,...,Missing values are replace by the mean of the column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the important features\n",
    "features_list = ['_RFHYPE5', 'TOLDHI2', '_CHOLCHK', '_BMI5', 'SMOKE100', 'CVDSTRK3', 'DIABETE3', '_TOTINDA', '_FRTLT1', '_VEGLT1', '_RFDRHV5', \n",
    "                 'HLTHPLN1', 'MEDCOST', 'GENHLTH', 'MENTHLTH', 'PHYSHLTH', 'DIFFWALK', 'SEX', '_AGEG5YR', 'EDUCA', 'INCOME2', ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n",
      "328135\n"
     ]
    }
   ],
   "source": [
    "trainMask, testMask = f.masking((x_train, x_test), features_name, features_list)\n",
    "trainProcessed = d.feature_processing_test(trainMask)\n",
    "\n",
    "#Test data Processing \n",
    "testProcessed  = d.feature_processing_test(testMask)\n",
    "\n",
    "\n",
    "\n",
    "x_train_algo = f.replaceMissingValuesMean(trainMask)\n",
    "x_test_algo = f.replaceMissingValuesMean(testMask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57950, 21)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_dis = x_train_algo[y_train == 1]\n",
    "non_hd = x_train_algo[y_train == -1]\n",
    "\n",
    "random_indices_nonhd = np.random.choice(range(len(non_hd)), len(heart_dis), replace=False)\n",
    "\n",
    "train_sample = np.concatenate((heart_dis, non_hd[random_indices_nonhd]), axis=0)\n",
    "train_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57950, 21) (57950,)\n"
     ]
    }
   ],
   "source": [
    "y_train_sample = np.concatenate((np.ones(len(heart_dis)), -np.ones(len(heart_dis))), axis=0)\n",
    "\n",
    "#shuffle both train_sample and y_train_sample the same way\n",
    "\n",
    "shuffle_indices = np.random.permutation(len(train_sample))\n",
    "train_sample = train_sample[shuffle_indices]\n",
    "y_train_sample = y_train_sample[shuffle_indices]\n",
    "\n",
    "print(train_sample.shape, y_train_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##test chelou\n",
    "x1=x_train_algo[:,1]\n",
    "x2=x_train_algo[:,2]\n",
    "x1_stand=f.standardize(x1)\n",
    "x2_stand=f.standardize(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the preprocessing has been done, we can format the data to be used by the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train = np.c_[np.ones((len(train_sample), 1)), train_sample]\n",
    "tX_test = np.c_[np.ones((len(x_test_algo), 1)), x_test_algo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation of set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_w = [random.choice([1, -1]) for i in range(len(tX_train[0]))]\n",
    "max_iter = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation of the dataset in a test/train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tX_train_train = tX_train[:int(len(tX_train)*0.7)]\n",
    "y_train_train = y_train_sample[:int(len(tX_train)*0.7)]\n",
    "tX_train_test = tX_train[int(len(tX_train)*0.7):]\n",
    "y_train_test = y_train_sample[int(len(tX_train)*0.7):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_progression(w):\n",
    "    # Plot progression of the weights in function of the iteration and progression on the test set\n",
    "    plt.figure(0)\n",
    "    plt.plot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And then, we can run the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLossMSE(weights, loss, y, x ):\n",
    "    loss_test_set = []\n",
    "\n",
    "    for w in weights:\n",
    "        loss_test_set.append(f.compute_mse(y, x, w))\n",
    "\n",
    "    plt.figure(0)\n",
    "    plt.semilogy(loss)\n",
    "    plt.semilogy(loss_test_set)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. MSE gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3999/4000): Final loss=0.5468254528464432\n"
     ]
    }
   ],
   "source": [
    "#Compute gradient descent with MSE as loss function (see functions.py for the function)\n",
    "\n",
    "w_mse_gd, loss_mse_gd = f.mean_squared_error_gd(y_train_train, tX_train_train, initial_w, 4000, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_test_set = []\n",
    "\n",
    "for w in w_mse_gd:\n",
    "    loss_test_set.append(f.compute_mse(y_train_test, tX_train_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.semilogy(loss_mse_gd)\n",
    "plt.semilogy(loss_test_set)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6754673569168824\n",
      "F1 score:  0.6756725684065302\n"
     ]
    }
   ],
   "source": [
    "y_pred = tX_train_test.dot(w_mse_gd[-1, :])\n",
    "y_pred = np.where(y_pred > 0, 1, -1)\n",
    "\n",
    "_,_,_,_,f1 = f.confusion_matrix(y_train_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", np.sum(y_pred == y_train_test)/len(y_train_test))\n",
    "print(\"F1 score: \", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#h.create_csv_submission(test_ids, y_test_rounded, 'submission_gd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      "\n",
      " [ 1.12409325 -0.05409309 -0.00129316  0.00199183  0.91530097  0.78064954\n",
      " -0.48872879 -0.729357   -0.06688515 -0.5401672  -0.09102535 -0.05643531\n",
      " -0.59655664  0.4490607  -0.47310667  0.85272966  0.02360115  0.98391684\n",
      " -0.75633775  0.48521196 -0.67598482  0.49035065] \n",
      "\n",
      " Loss =  0.5468254528464432 \n",
      "\n",
      "*****************************************************************************  \n",
      "\n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.35697894477001985\n"
     ]
    }
   ],
   "source": [
    "#Test the model on the test sample. Do we need to standardize ?\n",
    "\n",
    "y_test = tX_test.dot(w_mse_gd[-1, :])\n",
    "y_test_rounded = np.where(y_test > 0, 1, -1) \n",
    "\n",
    "print('weights = \\n\\n', w_mse_gd[-1, :],'\\n\\n Loss = ', loss_mse_gd[-1],'\\n\\n*****************************************************************************',\n",
    "      ' \\n\\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train == 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_rounded == 1)/len(y_test_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run some cross validation to see the best initial weights (as a function of the proportion of 1, -1 and 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. MSE SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 3999/3999: loss=0.0036711284385088864, w0=1.115735752160372, w1=-0.022624010650256536\n"
     ]
    }
   ],
   "source": [
    "w_mse_sgd, loss_mse_sgd = f.mean_squared_error_sgd(y_train_train, tX_train_train, initial_w, 4000, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLossMSE(weights, loss, y, x ):\n",
    "    loss_test_set = []\n",
    "\n",
    "    for w in weights:\n",
    "        loss_test_set.append(f.compute_mse(y, x, w))\n",
    "\n",
    "    plt.figure(0)\n",
    "    plt.semilogy(loss)\n",
    "    plt.semilogy(loss_test_set)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotLossMSE(w_mse_sgd, loss_mse_sgd, y_train_test, tX_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5894161633592178\n",
      "F1 score:  0.4940459313864474\n"
     ]
    }
   ],
   "source": [
    "y_pred = tX_train_test.dot(w_mse_sgd[-1])\n",
    "y_pred = np.where(y_pred > 0, 1, -1)\n",
    "\n",
    "_,_,_,_,f1 = f.confusion_matrix(y_train_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", np.sum(y_pred == y_train_test)/len(y_train_test))\n",
    "\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [ 1.11573575 -0.02262401 -0.06662782  0.04785737  0.8593002   0.78102103\n",
      " -0.48071372 -0.70276297 -0.06850756 -0.56944096 -0.13414413 -0.0250058\n",
      " -0.56852379  0.46900134 -0.44687142  0.8487253  -0.00709451  0.98343007\n",
      " -0.7986301   0.46472511 -0.71405065  0.53565178] \n",
      " Loss =  0.0036711284385088864 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.24253284451311494\n"
     ]
    }
   ],
   "source": [
    "y_test_sgd = tX_test.dot(w_mse_sgd[-1])\n",
    "y_test_rounded_sgd = np.where(y_test_sgd > 0, 1, -1)\n",
    "\n",
    "print('weights = \\n', w_mse_sgd[-1],'\\n Loss = ', loss_mse_sgd[-1],'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train == 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_rounded_sgd == 1)/len(y_test_rounded_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls, loss_ls = f.least_squares(y_train_train, tX_train_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7706068449813057\n",
      "F1 score:  0.7779015370906661\n"
     ]
    }
   ],
   "source": [
    "y_pred = tX_train_test.dot(w_ls)\n",
    "y_pred = np.where(y_pred > 0, 1, -1)\n",
    "\n",
    "_,_,_,_,f1 = f.confusion_matrix(y_train_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", np.sum(y_pred == y_train_test)/len(y_train_test))\n",
    "\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [-0.26212366  0.16214817  0.001177    0.00139231 -0.02964512 -0.03944538\n",
      " -0.23185505 -0.33937519 -0.07037331 -0.2468298  -0.00567709 -0.01720115\n",
      " -0.14126258 -0.12447874 -0.2483691   0.18457867  0.07125126 -0.09836726\n",
      "  0.09744234 -0.01292019 -0.0063776  -0.0063678 ] \n",
      " Loss =  0.31898642983936953 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.30945611131935746\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_test_ls = tX_test.dot(w_ls)\n",
    "y_test_ls = np.where(y_test_ls > 0, 1, -1)\n",
    "\n",
    "print('weights = \\n', w_ls,'\\n Loss = ', loss_ls,'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train == 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_ls == 1)/len(y_test_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ridge, loss_ridge = f.ridge_regression(y_train_train, tX_train_train, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [-0.03369243  0.10297796  0.00662819  0.00111429 -0.01808219 -0.02680741\n",
      " -0.119152   -0.09644003 -0.09925366 -0.11427922 -0.05412173 -0.03591432\n",
      " -0.07561305 -0.07301119 -0.13274258  0.02311154  0.07718318 -0.00649128\n",
      " -0.02014133 -0.00837351 -0.00492967 -0.00402232] \n",
      " Loss =  0.33614110099127775 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.3278234395999232\n"
     ]
    }
   ],
   "source": [
    "y_test_ridge = tX_test.dot(w_ridge)\n",
    "y_test_ridge = np.where(y_test_ridge > 0, 1, -1)\n",
    "\n",
    "print('weights = \\n', w_ridge,'\\n Loss = ', loss_ridge,'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train == 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_ridge == 1)/len(y_test_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_processed_logreg = np.where(y_train_train == 1, 1, 0)\n",
    "y_train_train_lg = np.where(y_train_train == 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(999/999): loss=0.7198830905554637, w0=0.6879136248913457, w1=0.5949118600479766\n"
     ]
    }
   ],
   "source": [
    "w_logreg, loss_logreg = f.logistic_regression(y_train_train_lg, tX_train_train,initial_w,1000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6438884095484613\n",
      "F1 score:  0.490578457993911\n"
     ]
    }
   ],
   "source": [
    "y_pred = tX_train_test.dot(w_logreg[-1, :])\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "y_pred = np.where(y_pred == 1, 1, -1)\n",
    "\n",
    "_,_,_,_,f1 = f.confusion_matrix(y_train_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", np.sum(y_pred == y_train_test)/len(y_train_test))\n",
    "\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [ 0.68791362  0.59491186 -0.0148592  -0.02405817  0.59078638  0.29775118\n",
      " -1.21618327 -1.76308994 -0.42782865 -1.45203191 -0.22204886 -0.1608316\n",
      " -1.03001076 -0.43733057 -1.23270404  0.96724441  0.25350789  0.83411045\n",
      " -0.79822042  0.05115232 -0.47555601  0.10186887] \n",
      " Loss =  0.7198830905554637 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.08138673785644411\n"
     ]
    }
   ],
   "source": [
    "y_test_logreg = tX_test.dot(w_logreg[-1, :])\n",
    "y_test_logreg = np.where(y_test_logreg > 0.5, 1, 0)\n",
    "\n",
    "print('weights = \\n', w_logreg[-1,:],'\\n Loss = ', loss_logreg[-1],'\\n*****************************************************************************',\n",
    "        ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train== 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_logreg == 1)/len(y_test_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_logreg = np.where(y_test_logreg == 1, 1, -1)\n",
    "h.create_csv_submission(test_ids, y_test_logreg, 'submission_logreg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=3.357436338341679, w0=0.9865523856979913, w1=-0.8319848942484627\n",
      "Gradient Descent(1/499): loss=2.334750381999612, w0=0.9612951320951524, w1=-0.7144603907266327\n",
      "Gradient Descent(2/499): loss=1.988003654426145, w0=0.9143751189160024, w1=-0.6636726673866472\n",
      "Gradient Descent(3/499): loss=1.6868183381489736, w0=0.8661897283272298, w1=-0.6241860251151005\n",
      "Gradient Descent(4/499): loss=1.424590805173383, w0=0.8224320292103068, w1=-0.5803510857935563\n",
      "Gradient Descent(5/499): loss=1.1947182388044084, w0=0.7796000903567106, w1=-0.541570344914193\n",
      "Gradient Descent(6/499): loss=0.9980621310526969, w0=0.7400496034157836, w1=-0.5014281344743763\n",
      "Gradient Descent(7/499): loss=0.8441140010741505, w0=0.7013885475134826, w1=-0.46644210954798276\n",
      "Gradient Descent(8/499): loss=0.7472642002023265, w0=0.6667287064039132, w1=-0.4268273821712598\n",
      "Gradient Descent(9/499): loss=0.7085773353776703, w0=0.6303868301946092, w1=-0.39893748106749516\n",
      "Gradient Descent(10/499): loss=0.7622211918337118, w0=0.6041215004738497, w1=-0.34615290078101174\n",
      "Gradient Descent(11/499): loss=1.1757282403587883, w0=0.5579545181252171, w1=-0.3626125445490325\n",
      "Gradient Descent(12/499): loss=2.479118935361457, w0=0.5654242827511096, w1=-0.21532349135905723\n",
      "Gradient Descent(13/499): loss=0.9922493326641978, w0=0.4923028318443279, w1=-0.3159510914468107\n",
      "Gradient Descent(14/499): loss=1.0266475401256896, w0=0.4925625063008503, w1=-0.22291182735367202\n",
      "Gradient Descent(15/499): loss=1.68935893807371, w0=0.437685705883086, w1=-0.2870819209502829\n",
      "Gradient Descent(16/499): loss=2.7275489064454925, w0=0.4609410943826046, w1=-0.11560498788565998\n",
      "Gradient Descent(17/499): loss=0.9049888879024566, w0=0.3922924436731473, w1=-0.22343233463711046\n",
      "Gradient Descent(18/499): loss=0.7815241362749504, w0=0.391912138426258, w1=-0.1557391835424525\n",
      "Gradient Descent(19/499): loss=1.4531864966413557, w0=0.3496529896255107, w1=-0.20486841209765672\n",
      "Gradient Descent(20/499): loss=2.7204313335363524, w0=0.37489362056989234, w1=-0.04493937761515751\n",
      "Gradient Descent(21/499): loss=0.8588450069413455, w0=0.31050244920744946, w1=-0.15664115857280675\n",
      "Gradient Descent(22/499): loss=0.7641260447293602, w0=0.313919315128713, w1=-0.09351624784863018\n",
      "Gradient Descent(23/499): loss=1.4743741424895587, w0=0.27543569497238285, w1=-0.14694481196823955\n",
      "Gradient Descent(24/499): loss=2.726038065591373, w0=0.30462273355486896, w1=0.010675586280160965\n",
      "Gradient Descent(25/499): loss=0.8412871157791301, w0=0.2437413708208494, w1=-0.10398666115606964\n",
      "Gradient Descent(26/499): loss=0.7461287125520505, w0=0.25001213420555685, w1=-0.04543167124889928\n",
      "Gradient Descent(27/499): loss=1.4536741785402194, w0=0.21529164432753156, w1=-0.10042044251286947\n",
      "Gradient Descent(28/499): loss=2.7205649679995187, w0=0.24717897228176838, w1=0.05377372371861158\n",
      "Gradient Descent(29/499): loss=0.8356989463275099, w0=0.18918758508843278, w1=-0.06312549519244978\n",
      "Gradient Descent(30/499): loss=0.7411497877387634, w0=0.19805795291267017, w1=-0.007296409258758962\n",
      "Gradient Descent(31/499): loss=1.449559145595718, w0=0.16608359724367644, w1=-0.0642519493405729\n",
      "Gradient Descent(32/499): loss=2.7182450743601585, w0=0.20029859653176987, w1=0.08760299084862977\n",
      "Gradient Descent(33/499): loss=0.8344133769614261, w0=0.1446453531278055, w1=-0.03108676341135777\n",
      "Gradient Descent(34/499): loss=0.739468691206726, w0=0.15566384020219884, w1=0.022693746240914157\n",
      "Gradient Descent(35/499): loss=1.4475153952176685, w0=0.12589120610438836, w1=-0.03586283572594087\n",
      "Gradient Descent(36/499): loss=2.716869613209379, w0=0.16200524259639884, w1=0.11413782931255262\n",
      "Gradient Descent(37/499): loss=0.8352504066275309, w0=0.1082503562676852, w1=-0.005976212806162173\n",
      "Gradient Descent(38/499): loss=0.7400314060835269, w0=0.12106062099058496, w1=0.046322684631338366\n",
      "Gradient Descent(39/499): loss=1.448365754754212, w0=0.09302773028968322, w1=-0.013616238963463125\n",
      "Gradient Descent(40/499): loss=2.71658379590646, w0=0.1307174104451953, w1=0.13499315149627417\n",
      "Gradient Descent(41/499): loss=0.8368910142493017, w0=0.0785050455508268, w1=0.013743786061307911\n",
      "Gradient Descent(42/499): loss=0.7413965661146542, w0=0.09279292675752808, w1=0.06492827168664694\n",
      "Gradient Descent(43/499): loss=1.4501661804898587, w0=0.06615689294539263, w1=0.0038521503047434374\n",
      "Gradient Descent(44/499): loss=2.716830950715828, w0=0.10514499339102623, w1=0.15139573152842478\n",
      "Gradient Descent(45/499): loss=0.8387609025197359, w0=0.054188729185601345, w1=0.029244312106051176\n",
      "Gradient Descent(46/499): loss=0.7430486114057538, w0=0.06969305407818929, w1=0.07958545436754705\n",
      "Gradient Descent(47/499): loss=1.4523615080465975, w0=0.04418305499579344, w1=0.017580731619170682\n",
      "Gradient Descent(48/499): loss=2.7173566481970233, w0=0.08424069554396776, w1=0.1643079197747192\n",
      "Gradient Descent(49/499): loss=0.8405689129596433, w0=0.03430877905537188, w1=0.041441128477086364\n",
      "Gradient Descent(50/499): loss=0.7446803804184717, w0=0.050811790588513796, w1=0.09113615958927998\n",
      "Gradient Descent(51/499): loss=1.454528963936968, w0=0.02621444246103898, w1=0.02838284447633277\n",
      "Gradient Descent(52/499): loss=2.7179755739473364, w0=0.06715116682649641, w1=0.1744800151905602\n",
      "Gradient Descent(53/499): loss=0.8421929047705825, w0=0.01805558060982901, w1=0.05104692420591256\n",
      "Gradient Descent(54/499): loss=0.7461655528740326, w0=0.03537739611191963, w1=0.10024357552628228\n",
      "Gradient Descent(55/499): loss=1.4565011221057762, w0=0.01152205085966733, w1=0.036890709067202224\n",
      "Gradient Descent(56/499): loss=2.7185934896106936, w0=0.05318043081587629, w1=0.18249959239519292\n",
      "Gradient Descent(57/499): loss=0.8435913937637616, w0=0.004768190319046316, w1=0.0586187211873732\n",
      "Gradient Descent(58/499): loss=0.7474547324599458, w0=0.022760560366439867, w1=0.10742852950799567\n",
      "Gradient Descent(59/499): loss=1.4582086257911109, w0=-0.0004900569040653079, w1=0.04359805016274869\n",
      "Gradient Descent(60/499): loss=2.719162506975648, w0=0.041760074134493605, w1=0.18882683895636743\n",
      "Gradient Descent(61/499): loss=0.8447646442069269, w0=-0.0060935491059298935, w1=0.06459215587699768\n",
      "Gradient Descent(62/499): loss=0.7485423496300905, w0=0.012447527296540337, w1=0.11310027451519344\n",
      "Gradient Descent(63/499): loss=1.4596443674636286, w0=-0.010309456884776632, w1=0.04889065627020127\n",
      "Gradient Descent(64/499): loss=2.7196638846332197, w0=0.03242550350596182, w1=0.19382259005330588\n",
      "Gradient Descent(65/499): loss=0.8457318670559798, w0=-0.014971343609698776, w1=0.06930843513230332\n",
      "Gradient Descent(66/499): loss=0.7494426373283191, w0=0.0040184454851804885, w1=0.11758033954426295\n",
      "Gradient Descent(67/499): loss=1.4608284417575674, w0=-0.01833515548571172, w1=0.05307051126584231\n",
      "Gradient Descent(68/499): loss=2.720093969880161, w0=0.024796768659675, w1=0.19776996392354027\n",
      "Gradient Descent(69/499): loss=0.8465196079298626, w0=-0.022226535584181487, w1=0.07303506695997247\n",
      "Gradient Descent(70/499): loss=0.7501782041182082, w0=-0.0028699892519022663, w1=0.12112142210949611\n",
      "Gradient Descent(71/499): loss=1.4617924612890207, w0=-0.024893732412086412, w1=0.05637428166921561\n",
      "Gradient Descent(72/499): loss=2.7204567365772827, w0=0.018563044852672402, w1=0.20089127073984459\n",
      "Gradient Descent(73/499): loss=0.8471556044912427, w0=-0.028154782543994453, w1=0.07598198940723311\n",
      "Gradient Descent(74/499): loss=0.7507736322525901, w0=-0.00849858822512457, w1=0.12392219549874656\n",
      "Gradient Descent(75/499): loss=1.4625703999461925, w0=-0.03025248847530233, w1=0.058987669840025334\n",
      "Gradient Descent(76/499): loss=2.720759395180705, w0=0.013470018571922104, w1=0.20336119769875283\n",
      "Gradient Descent(77/499): loss=0.847665829020044, w0=-0.03299801297576714, w1=0.07831410848645287\n",
      "Gradient Descent(78/499): loss=0.7512523895980665, w0=-0.013097082803068302, w1=0.12613893371404591\n",
      "Gradient Descent(79/499): loss=1.4631943486990149, w0=-0.03463019074057678, w1=0.061056543412701086\n",
      "Gradient Descent(80/499): loss=2.7210100959374457, w0=0.009309607694069869, w1=0.20531711108382947\n",
      "Gradient Descent(81/499): loss=0.8480732293322095, w0=-0.036954184011893466, w1=0.08016107007264223\n",
      "Gradient Descent(82/499): loss=0.7516354385237433, w0=-0.016853423707753494, w1=0.1278946249064964\n",
      "Gradient Descent(83/499): loss=1.4636926699547128, w0=-0.038205861936553, w1=0.06269559394807307\n",
      "Gradient Descent(84/499): loss=2.721216765509521, w0=0.005911566799595737, w1=0.20686710848281298\n",
      "Gradient Descent(85/499): loss=0.8483973859542626, w0=-0.04018525770775163, w1=0.08162488396147388\n",
      "Gradient Descent(86/499): loss=0.7519407917624367, w0=-0.01992137262588782, w1=0.12928611684514732\n",
      "Gradient Descent(87/499): loss=1.464089494712806, w0=-0.04112597715116501, w1=0.06399508325921842\n",
      "Gradient Descent(88/499): loss=2.7213865879934054, w0=0.003136622962078131, w1=0.20809631757617147\n",
      "Gradient Descent(89/499): loss=0.8486546301825232, w0=-0.04282372755307763, w1=0.08278587986405744\n",
      "Gradient Descent(90/499): loss=0.7521835465795785, w0=-0.022426708466687317, w1=0.13038971919874998\n",
      "Gradient Descent(91/499): loss=1.4644048661506446, w0=-0.04351035904266121, w1=0.06502610903419065\n",
      "Gradient Descent(92/499): loss=2.7215258253759744, w0=0.0008708581176919764, w1=0.20907182645417943\n",
      "Gradient Descent(93/499): loss=0.8488583724737057, w0=-0.04497796205126138, w1=0.08370736507267937\n",
      "Gradient Descent(94/499): loss=0.7523761454625028, w0=-0.024472305461438845, w1=0.1312655970226273\n",
      "Gradient Descent(95/499): loss=1.4646551718293517, w0=-0.0454570081508682, w1=0.06584472040590766\n",
      "Gradient Descent(96/499): loss=2.721639810659423, w0=-0.0009788906942129402, w1=0.20984654546676498\n",
      "Gradient Descent(97/499): loss=0.8490195037047972, w0=-0.04673658123963047, w1=0.08443927040133786\n",
      "Gradient Descent(98/499): loss=0.7525287224526027, w0=-0.026142290380531105, w1=0.13196121738161537\n",
      "Gradient Descent(99/499): loss=1.4648536682898534, w0=-0.04704606091028138, w1=0.06649513871570581\n",
      "Gradient Descent(100/499): loss=2.7217330206077417, w0=-0.0024887986227940806, w1=0.21046223424228777\n",
      "Gradient Descent(101/499): loss=0.849146797851162, w0=-0.04817204166388764, w1=0.08502100674660368\n",
      "Gradient Descent(102/499): loss=0.7526494615560303, w0=-0.02750544679951171, w1=0.13251405430365126\n",
      "Gradient Descent(103/499): loss=1.4650110010965631, w0=-0.048343032720698634, w1=0.0670122808170245\n",
      "Gradient Descent(104/499): loss=2.721809178898517, w0=-0.003721138741151521, w1=0.21095187601447785\n",
      "Gradient Descent(105/499): loss=0.8492472817383971, w0=-0.049343572967361096, w1=0.08548370510629494\n",
      "Gradient Descent(106/499): loss=0.7527449309733969, w0=-0.028618003069709942, w1=0.1329537126722086\n",
      "Gradient Descent(107/499): loss=1.4651356759784346, w0=-0.04940147485874594, w1=0.0674237379564988\n",
      "Gradient Descent(108/499): loss=2.7218713645394375, w0=-0.004726809428964772, w1=0.21134154131451194\n",
      "Gradient Descent(109/499): loss=0.8493265581718755, w0=-0.050299583136847736, w1=0.0858519745773629\n",
      "Gradient Descent(110/499): loss=0.7528203778843259, w0=-0.029525915517861108, w1=0.1333035967547059\n",
      "Gradient Descent(111/499): loss=1.4652344652981932, w0=-0.05026515076795103, w1=0.06775132914267766\n",
      "Gradient Descent(112/499): loss=2.7219221136938856, w0=-0.005547404183749154, w1=0.21165185189686903\n",
      "Gradient Descent(113/499): loss=0.8493890797542788, w0=-0.05107962823671298, w1=0.0861452831627671\n",
      "Gradient Descent(114/499): loss=0.7528799797623946, w0=-0.030266738072588773, w1=0.13358222173680723\n",
      "Gradient Descent(115/499): loss=1.4653127484652912, w0=-0.05096981827835847, w1=0.06801232156571442\n",
      "Gradient Descent(116/499): loss=2.7219635101002426, w0=-0.006216906406958468, w1=0.21189913147997747\n",
      "Gradient Descent(117/499): loss=0.8494383762072669, w0=-0.05171602517071998, w1=0.08637904316559322\n",
      "Gradient Descent(118/499): loss=0.7529270539186105, w0=-0.03087115299015726, w1=0.1338042452582232\n",
      "Gradient Descent(119/499): loss=1.4653747916350952, w0=-0.05154468876445864, w1=0.06822039023029032\n",
      "Gradient Descent(120/499): loss=2.721997262877054, w0=-0.006763076827788227, w1=0.21209631095994075\n",
      "Gradient Descent(121/499): loss=0.8494772404204686, w0=-0.05223517187368687, w1=0.08656546502868798\n",
      "Gradient Descent(122/499): loss=0.7529642295653323, w0=-0.03136422387002742, w1=0.13398127923055625\n",
      "Gradient Descent(123/499): loss=1.465423974612171, w0=-0.05201362150757921, w1=0.06838637313320173\n",
      "Gradient Descent(124/499): loss=2.72202477223275, w0=-0.00720858905078748, w1=0.2122536410035176\n",
      "Gradient Descent(125/499): loss=0.84950787917341, w0=-0.05265862775578957, w1=0.08671422952991292\n",
      "Gradient Descent(126/499): loss=0.7529935875792803, w0=-0.03176642112529668, w1=0.1341225291474109\n",
      "Gradient Descent(127/499): loss=1.4654629733329052, w0=-0.05239610107201678, w1=0.0685188660093827\n",
      "Gradient Descent(128/499): loss=2.722047184328018, w0=-0.007571958735460668, w1=0.21237925341887512\n",
      "Gradient Descent(129/499): loss=0.8495320343289499, w0=-0.053003997727246274, w1=0.08683301737481713\n",
      "Gradient Descent(130/499): loss=0.75301677313646, w0=-0.03209446104233438, w1=0.13423529787567137\n",
      "Gradient Descent(131/499): loss=1.4654939057500733, w0=-0.05270803691867549, w1=0.06862469108672758\n",
      "Gradient Descent(132/499): loss=2.722065436757304, w0=-0.00786830373209485, w1=0.21247960372014055\n",
      "Gradient Descent(133/499): loss=0.8495510797438722, w0=-0.05328565533985133, w1=0.08692792675170122\n",
      "Gradient Descent(134/499): loss=0.7530350859588141, w0=-0.03236199215964043, w1=0.13432538292186824\n",
      "Gradient Descent(135/499): loss=1.46551844798086, w0=-0.05296241742942237, w1=0.06870926681797893\n",
      "Gradient Descent(136/499): loss=2.7220802960871953, w0=-0.008109965782028813, w1=0.21255982028582598\n",
      "Gradient Descent(137/499): loss=0.8495660984401728, w0=-0.053515334186539934, w1=0.08700380279654163\n",
      "Gradient Descent(138/499): loss=0.7530495523102866, w0=-0.03258015662183883, w1=0.13439738991217415\n",
      "Gradient Descent(139/499): loss=1.4655379265194957, w0=-0.05316984472531794, w1=0.06877689972527601\n",
      "Gradient Descent(140/499): loss=2.722092388753418, w0=-0.008307018877986037, w1=0.21262398002760202\n",
      "Gradient Descent(141/499): loss=0.8495779438652443, w0=-0.05370261145028001, w1=0.0870644977447747\n",
      "Gradient Descent(142/499): loss=0.7530609822473475, w0=-0.032758049180466015, w1=0.13445498012848983\n",
      "Gradient Descent(143/499): loss=1.4655533913092058, w0=-0.05333897190761748, w1=0.06883101493530729\n",
      "Gradient Descent(144/499): loss=2.722102226447623, w0=-0.008467684855198825, w1=0.21267532619716373\n",
      "Gradient Descent(145/499): loss=0.8495872884061102, w0=-0.05385530318423588, w1=0.08711307650397557\n",
      "Gradient Descent(146/499): loss=0.7530700150329211, w0=-0.032903092423082154, w1=0.13450106610922086\n",
      "Gradient Descent(147/499): loss=1.4655656735744478, w0=-0.053476860447506255, w1=0.06887433841842247\n",
      "Gradient Descent(148/499): loss=2.7221102269533057, w0=-0.008598673070377809, w1=0.2127164406023861\n",
      "Gradient Descent(149/499): loss=0.8495946617410666, w0=-0.053979787368522346, w1=0.08715197921754561\n",
      "Gradient Descent(150/499): loss=0.753077155095362, w0=-0.03302134345608131, w1=0.13453796631998718\n",
      "Gradient Descent(151/499): loss=1.465575431557403, w0=-0.05358927224744367, w1=0.06890904115470826\n",
      "Gradient Descent(152/499): loss=2.7221167312337977, w0=-0.008705457978331915, w1=0.2127493798752541\n",
      "Gradient Descent(153/499): loss=0.8496004811167058, w0=-0.054081267887092714, w1=0.08718314991175782\n",
      "Gradient Descent(154/499): loss=0.7530828004620088, w0=-0.03311774451322673, w1=0.1345675275453486\n",
      "Gradient Descent(155/499): loss=1.4655831866729667, w0=-0.05368090826904431, w1=0.06893685326398605\n",
      "Gradient Descent(156/499): loss=2.722122017438733, w0=-0.00879250591632616, w1=0.21277578337385422\n",
      "Gradient Descent(157/499): loss=0.849605075226549, w0=-0.054163990188148686, w1=0.087208138376586\n",
      "Gradient Descent(158/499): loss=0.7530872652196606, w0=-0.03319632770363899, w1=0.13459122180649685\n",
      "Gradient Descent(159/499): loss=1.4655893520844276, w0=-0.053755603467096305, w1=0.06895915442280369\n",
      "Gradient Descent(160/499): loss=2.722126312379236, w0=-0.008863460356315342, w1=0.21279695868505\n",
      "Gradient Descent(161/499): loss=0.8496087030296822, w0=-0.05423141743976577, w1=0.08722818090771955\n",
      "Gradient Descent(162/499): loss=0.7530907972438481, w0=-0.03326038226124617, w1=0.13461022316118734\n",
      "Gradient Descent(163/499): loss=1.4655942552887005, w0=-0.05381648600221574, w1=0.068977045546688\n",
      "Gradient Descent(164/499): loss=2.7221298009254653, w0=-0.008921293204633376, w1=0.21281394942643544\n",
      "Gradient Descent(165/499): loss=0.8496115685739432, w0=-0.05428637439259329, w1=0.08724426434178276\n",
      "Gradient Descent(166/499): loss=0.7530935921862847, w0=-0.0333125911398162, w1=0.13462546860475133\n",
      "Gradient Descent(167/499): loss=1.465598155968753, w0=-0.05386610725630942, w1=0.06899140566004715\n",
      "Gradient Descent(168/499): loss=2.7221326336992355, w0=-0.008968428351444395, w1=0.2128275890503365\n",
      "Gradient Descent(169/499): loss=0.849613832667878, w0=-0.0543311648505096, w1=0.08725717687785953\n",
      "Gradient Descent(170/499): loss=0.7530958045065818, w0=-0.03335514255382845, w1=0.1346377063974166\n",
      "Gradient Descent(171/499): loss=1.4656012601082107, w0=-0.05390654798838408, w1=0.0690029370463744\n",
      "Gradient Descent(172/499): loss=2.7221349333684546, w0=-0.00900684254389695, w1=0.21283854356997756\n",
      "Gradient Descent(173/499): loss=0.8496156220693264, w0=-0.054367667576190486, w1=0.08726754844047879\n",
      "Gradient Descent(174/499): loss=0.7530975561708967, w0=-0.03338982104647294, w1=0.13464753444063995\n",
      "Gradient Descent(175/499): loss=1.4656037311534449, w0=-0.0539395049963057, w1=0.06901220111902893\n",
      "Gradient Descent(176/499): loss=2.7221367997951518, w0=-0.00903814773279387, w1=0.2128473455123962\n",
      "Gradient Descent(177/499): loss=0.8496170367182039, w0=-0.05439741457908543, w1=0.08727588275870679\n",
      "Gradient Descent(178/499): loss=0.7530989435103085, w0=-0.03341808183155537, w1=0.13465543077249567\n",
      "Gradient Descent(179/499): loss=1.4656056988435284, w0=-0.05396636185430234, w1=0.06901964693943351\n",
      "Gradient Descent(180/499): loss=2.7221383142438316, w0=-0.009063658285955914, w1=0.21285442091823087\n",
      "Gradient Descent(181/499): loss=0.8496181554296548, w0=-0.054421655013311206, w1=0.08728258287909177\n",
      "Gradient Descent(182/499): loss=0.7531000426281869, w0=-0.03344111147287268, w1=0.13466177781713265\n",
      "Gradient Descent(183/499): loss=1.4656072661977482, w0=-0.05398824664458483, w1=0.06902563390527906\n",
      "Gradient Descent(184/499): loss=2.7221395428201034, w0=-0.009084445842133512, w1=0.21286011082694384\n",
      "Gradient Descent(185/499): loss=0.849619040375416, w0=-0.054441407323705296, w1=0.0872879714702863\n",
      "Gradient Descent(186/499): loss=0.7531009136637211, w0=-0.03345987740542407, w1=0.13466688168060925\n",
      "Gradient Descent(187/499): loss=1.4656085150469278, w0=-0.05400607906838474, w1=0.06903044981274299\n",
      "Gradient Descent(188/499): loss=2.722140539279038, w0=-0.009101384072563376, w1=0.21286468838530415\n",
      "Gradient Descent(189/499): loss=0.849619740611712, w0=-0.05445750179615182, w1=0.0872923069934386\n",
      "Gradient Descent(190/499): loss=0.7531016041539418, w0=-0.03347516834526303, w1=0.13467098751526083\n",
      "Gradient Descent(191/499): loss=1.465609510413614, w0=-0.05402060888561604, w1=0.06903432524547819\n",
      "Gradient Descent(192/499): loss=2.722141347317436, w0=-0.009115185202689682, w1=0.21286837247971685\n",
      "Gradient Descent(193/499): loss=0.8496202948569108, w0=-0.0544706152739029, w1=0.08729579658862754\n",
      "Gradient Descent(194/499): loss=0.7531021516852895, w0=-0.033487627260480005, w1=0.13467429176168064\n",
      "Gradient Descent(195/499): loss=1.4656103039820796, w0=-0.054032447275671525, w1=0.06903744504487673\n",
      "Gradient Descent(196/499): loss=2.7221420024442335, w0=-0.009126429807519897, w1=0.21287133860577973\n",
      "Gradient Descent(197/499): loss=0.8496207336788324, w0=-0.05448129947910565, w1=0.08729860635093975\n",
      "Gradient Descent(198/499): loss=0.753102585985194, w0=-0.03349777826970366, w1=0.13467695190916124\n",
      "Gradient Descent(199/499): loss=1.4656109368481371, w0=-0.05404209242022103, w1=0.06903995745952703\n",
      "Gradient Descent(200/499): loss=2.7221425335056115, w0=-0.009135591116864065, w1=0.21287372754049627\n",
      "Gradient Descent(201/499): loss=0.849621081217597, w0=-0.054490004115056015, w1=0.08730086953016677\n",
      "Gradient Descent(202/499): loss=0.7531029305723467, w0=-0.03350604858420331, w1=0.1346790942825758\n",
      "Gradient Descent(203/499): loss=1.4656114416991832, w0=-0.05404995037044247, w1=0.06904198144803995\n",
      "Gradient Descent(204/499): loss=2.722142963928292, w0=-0.009143054840077179, w1=0.21287565226569827\n",
      "Gradient Descent(205/499): loss=0.8496213565428501, w0=-0.0544970957091597, w1=0.0873026930777697\n",
      "Gradient Descent(206/499): loss=0.7531032040590095, w0=-0.03351278640508559, w1=0.13468082025862124\n",
      "Gradient Descent(207/499): loss=1.4656118445429949, w0=-0.054056352066250715, w1=0.0690436125115467\n",
      "Gradient Descent(208/499): loss=2.722143312732273, w0=-0.0091491353347211, w1=0.21287720349870679\n",
      "Gradient Descent(209/499): loss=0.8496215747233286, w0=-0.054502872980441194, w1=0.08730416287742929\n",
      "Gradient Descent(210/499): loss=0.7531034211790735, w0=-0.03351827551985484, w1=0.13468221123127086\n",
      "Gradient Descent(211/499): loss=1.4656121660793566, w0=-0.054061567215848275, w1=0.06904492735471761\n",
      "Gradient Descent(212/499): loss=2.722143595355762, w0=-0.009154088792213105, w1=0.2128784541130646\n",
      "Gradient Descent(213/499): loss=0.8496217476698217, w0=-0.054507579371513154, w1=0.08730534792637282\n",
      "Gradient Descent(214/499): loss=0.7531035935997433, w0=-0.033522747205983, w1=0.134683332580469\n",
      "Gradient Descent(215/499): loss=1.4656124227885217, w0=-0.054065815613824154, w1=0.06904598761279442\n",
      "Gradient Descent(216/499): loss=2.722143824326497, w0=-0.009158123989893616, w1=0.21287946267415314\n",
      "Gradient Descent(217/499): loss=0.8496218847998593, w0=-0.05451141326735302, w1=0.08730630367990302\n",
      "Gradient Descent(218/499): loss=0.7531037305626871, w0=-0.03352638993750252, w1=0.13468423684602876\n",
      "Gradient Descent(219/499): loss=1.4656126277954797, w0=-0.05406927636976208, w1=0.06904684283351398\n",
      "Gradient Descent(220/499): loss=2.7221440098080993, w0=-0.009161411057956377, w1=0.21288027626852624\n",
      "Gradient Descent(221/499): loss=0.8496219935618551, w0=-0.05451453632720452, w1=0.08730707472810875\n",
      "Gradient Descent(222/499): loss=0.7531038393904791, w0=-0.03352935729947595, w1=0.13468496626732956\n",
      "Gradient Descent(223/499): loss=1.4656127915561687, w0=-0.054072095432525946, w1=0.06904753286422088\n",
      "Gradient Descent(224/499): loss=2.7221441600443725, w0=-0.009164088627209042, w1=0.21288093276930983\n",
      "Gradient Descent(225/499): loss=0.8496220798486297, w0=-0.05451708027749989, w1=0.08730769693930096\n",
      "Gradient Descent(226/499): loss=0.753103925886921, w0=-0.033531774440722674, w1=0.13468555481669126\n",
      "Gradient Descent(227/499): loss=1.4656129224028587, w0=-0.05407439172452799, w1=0.06904808976386813\n",
      "Gradient Descent(228/499): loss=2.7221442817205963, w0=-0.009166269656287446, w1=0.21288146265105556\n",
      "Gradient Descent(229/499): loss=0.849622148323688, w0=-0.054519152449674396, w1=0.08730819917734131\n",
      "Gradient Descent(230/499): loss=0.7531039946533983, w0=-0.03353374333440354, w1=0.13468602982822855\n",
      "Gradient Descent(231/499): loss=1.4656130269773218, w0=-0.05407626214241724, w1=0.06904853933528439\n",
      "Gradient Descent(232/499): loss=2.7221443802570926, w0=-0.009168046181970771, w1=0.21288189044440792\n",
      "Gradient Descent(233/499): loss=0.849622202678777, w0=-0.054520840294471865, w1=0.08730860467828608\n",
      "Gradient Descent(234/499): loss=0.7531040493391156, w0=-0.033535347066411224, w1=0.13468641330336156\n",
      "Gradient Descent(235/499): loss=1.4656131105754464, w0=-0.05407778563340438, w1=0.06904890235375251\n",
      "Gradient Descent(236/499): loss=2.7221444600478404, w0=-0.009169493191361892, w1=0.21288223590267968\n",
      "Gradient Descent(237/499): loss=0.8496222458372645, w0=-0.05452221506167296, w1=0.087308932154502\n",
      "Gradient Descent(238/499): loss=0.7531040928390021, w0=-0.03353665333101488, w1=0.13468672295769732\n",
      "Gradient Descent(239/499): loss=1.4656131774211443, w0=-0.0540790265178802, w1=0.06904919555259129\n",
      "Gradient Descent(240/499): loss=2.722144524654161, w0=-0.009170671778071654, w1=0.21288251493784352\n",
      "Gradient Descent(241/499): loss=0.8496222801147446, w0=-0.05452333479935656, w1=0.08730919668063883\n",
      "Gradient Descent(242/499): loss=0.7531041274502889, w0=-0.0335377172801257, w1=0.1346869730609397\n",
      "Gradient Descent(243/499): loss=1.465613230884276, w0=-0.054080037197526104, w1=0.06904943241418696\n",
      "Gradient Descent(244/499): loss=2.722144576962125, w0=-0.009171631714645495, w1=0.21288274037188787\n",
      "Gradient Descent(245/499): loss=0.8496223073459281, w0=-0.0545242467983871, w1=0.08730941040489573\n",
      "Gradient Descent(246/499): loss=0.7531041549964559, w0=-0.03353858384557181, w1=0.13468717511106285\n",
      "Gradient Descent(247/499): loss=1.4656132736541212, w0=-0.05408086036244684, w1=0.06904962380516864\n",
      "Gradient Descent(248/499): loss=2.7221446193102006, w0=-0.009172413549094274, w1=0.2128829225402274\n",
      "Gradient Descent(249/499): loss=0.8496223289849534, w0=-0.05452498958464026, w1=0.08730958312029838\n",
      "Gradient Descent(250/499): loss=0.7531041769253461, w0=-0.03353928963174957, w1=0.13468733837571423\n",
      "Gradient Descent(251/499): loss=1.465613307877353, w0=-0.054081530789905574, w1=0.06904977848668008\n",
      "Gradient Descent(252/499): loss=2.722144653592938, w0=-0.009173050313472875, w1=0.2128830697765271\n",
      "Gradient Descent(253/499): loss=0.8496223461846345, w0=-0.054525594542551785, w1=0.08730972272374332\n",
      "Gradient Descent(254/499): loss=0.7531041943868821, w0=-0.03353986445804918, w1=0.1346874703271789\n",
      "Gradient Descent(255/499): loss=1.465613335268029, w0=-0.05408207681015621, w1=0.06904990352447742\n",
      "Gradient Descent(256/499): loss=2.722144681345027, w0=-0.009173568916231362, w1=0.21288318880240786\n",
      "Gradient Descent(257/499): loss=0.8496223598591882, w0=-0.05452608723815754, w1=0.0873098355850189\n",
      "Gradient Descent(258/499): loss=0.7531042082946282, w0=-0.03354033261580217, w1=0.13468757699199485\n",
      "Gradient Descent(259/499): loss=1.4656133571951087, w0=-0.054082521500928564, w1=0.06905000461866961\n",
      "Gradient Descent(260/499): loss=2.7221447038095223, w0=-0.009173991276809706, w1=0.21288328504083362\n",
      "Gradient Descent(261/499): loss=0.849622370733818, w0=-0.054526488497191966, w1=0.08730992684358252\n",
      "Gradient Descent(262/499): loss=0.7531042193745966, w0=-0.03354071389253721, w1=0.134687663232078\n",
      "Gradient Descent(263/499): loss=1.4656133747522655, w0=-0.054082883660751487, w1=0.06905008636894938\n",
      "Gradient Descent(264/499): loss=2.7221447219931805, w0=-0.0091743352501533, w1=0.21288336286821916\n",
      "Gradient Descent(265/499): loss=0.8496223793839357, w0=-0.05452681528355652, w1=0.08731000064732855\n",
      "Gradient Descent(266/499): loss=0.7531042282038539, w0=-0.03354102440658821, w1=0.1346877329708587\n",
      "Gradient Descent(267/499): loss=1.4656133888133522, w0=-0.054083178602028825, w1=0.06905015248802399\n",
      "Gradient Descent(268/499): loss=2.72214473671124, w0=-0.009174615380015794, w1=0.21288342581732625\n",
      "Gradient Descent(269/499): loss=0.849622386266252, w0=-0.05452708141509344, w1=0.08731006034476535\n",
      "Gradient Descent(270/499): loss=0.753104235241295, w0=-0.03354127728713365, w1=0.13468778937527925\n",
      "Gradient Descent(271/499): loss=1.4656134000769327, w0=-0.054083418797225113, w1=0.0690502059734192\n",
      "Gradient Descent(272/499): loss=2.7221447486238675, w0=-0.009174843512729354, w1=0.21288347674061575\n",
      "Gradient Descent(273/499): loss=0.849622391743345, w0=-0.05452729814677227, w1=0.08731010863974956\n",
      "Gradient Descent(274/499): loss=0.7531042408518465, w0=-0.033541483228280614, w1=0.13468783500229345\n",
      "Gradient Descent(275/499): loss=1.4656134091014041, w0=-0.054083614405340326, w1=0.06905024924584317\n",
      "Gradient Descent(276/499): loss=2.722144758265584, w0=-0.009175029297272048, w1=0.21288351794180205\n",
      "Gradient Descent(277/499): loss=0.8496223961031448, w0=-0.05452747464583143, w1=0.08731014771611009\n",
      "Gradient Descent(278/499): loss=0.7531042453258506, w0=-0.03354165094049444, w1=0.13468787191685627\n",
      "Gradient Descent(279/499): loss=1.465613416333344, w0=-0.0540837737008419, w1=0.06905028426063359\n",
      "Gradient Descent(280/499): loss=2.7221447660691256, w0=-0.009175180592677482, w1=0.21288355128184974\n",
      "Gradient Descent(281/499): loss=0.8496223995743676, w0=-0.054527618378879567, w1=0.0873101793380695\n",
      "Gradient Descent(282/499): loss=0.753104248894346, w0=-0.03354178751838892, w1=0.1346879017869786\n",
      "Gradient Descent(283/499): loss=1.4656134221299402, w0=-0.054083903423110355, w1=0.06905031259758003\n",
      "Gradient Descent(284/499): loss=2.7221447723848327, w0=-0.00917530379994766, w1=0.21288357826439203\n",
      "Gradient Descent(285/499): loss=0.8496224023387354, w0=-0.054527735427256474, w1=0.08731020493117495\n",
      "Gradient Descent(286/499): loss=0.7531042517412335, w0=-0.03354189874035625, w1=0.13468792596032622\n",
      "Gradient Descent(287/499): loss=1.4656134267769754, w0=-0.054084009061119695, w1=0.06905033553332161\n",
      "Gradient Descent(288/499): loss=2.7221447774963172, w0=-0.00917540413244762, w1=0.21288360010459922\n",
      "Gradient Descent(289/499): loss=0.8496224045406785, w0=-0.054527830743943496, w1=0.0873102256475527\n",
      "Gradient Descent(290/499): loss=0.7531042540129282, w0=-0.033541989312665214, w1=0.13468794552597138\n",
      "Gradient Descent(291/499): loss=1.46561343050313, w0=-0.05408409508534928, w1=0.06905035409973036\n",
      "Gradient Descent(292/499): loss=2.7221447816331503, w0=-0.009175485836178388, w1=0.2128836177847152\n",
      "Gradient Descent(293/499): loss=0.8496224062950015, w0=-0.054527908362849445, w1=0.08731024241853738\n",
      "Gradient Descent(294/499): loss=0.7531042558260305, w0=-0.0335420630683107, w1=0.13468796136419164\n",
      "Gradient Descent(295/499): loss=1.4656134334914463, w0=-0.05408416513670258, w1=0.06905036913100171\n",
      "Gradient Descent(296/499): loss=2.7221447849811558, w0=-0.009175552369214413, w1=0.21288363209885577\n",
      "Gradient Descent(297/499): loss=0.8496224076929936, w0=-0.0545279715692949, w1=0.08731025599712383\n",
      "Gradient Descent(298/499): loss=0.7531042572734171, w0=-0.03354212312899843, w1=0.13468797418663353\n",
      "Gradient Descent(299/499): loss=1.4656134358884596, w0=-0.054084222180398246, w1=0.0690503813016481\n",
      "Gradient Descent(300/499): loss=2.7221447876907465, w0=-0.009175606547870023, w1=0.2128836436891563\n",
      "Gradient Descent(301/499): loss=0.8496224088072628, w0=-0.05452802303888603, w1=0.0873102669922253\n",
      "Gradient Descent(302/499): loss=0.7531042584290952, w0=-0.033542172037095946, w1=0.13468798456871964\n",
      "Gradient Descent(303/499): loss=1.4656134378115164, w0=-0.05408426863132565, w1=0.06905039115715876\n",
      "Gradient Descent(304/499): loss=2.7221447898836555, w0=-0.009175650665762258, w1=0.2128836530749475\n",
      "Gradient Descent(305/499): loss=0.8496224096955699, w0=-0.05452806495063489, w1=0.0873102758963297\n",
      "Gradient Descent(306/499): loss=0.7531042593520324, w0=-0.033542211863114485, w1=0.13468799297580894\n",
      "Gradient Descent(307/499): loss=1.4656134393545812, w0=-0.05408430645615174, w1=0.06905039913875825\n",
      "Gradient Descent(308/499): loss=2.7221447916584083, w0=-0.009175686590792556, w1=0.21288366067631326\n",
      "Gradient Descent(309/499): loss=0.8496224104038853, w0=-0.05452809907910088, w1=0.08731028310782335\n",
      "Gradient Descent(310/499): loss=0.7531042600892582, w0=-0.03354224429325784, w1=0.13468799978431126\n",
      "Gradient Descent(311/499): loss=1.4656134405929753, w0=-0.054084337256491755, w1=0.06905040560338882\n",
      "Gradient Descent(312/499): loss=2.7221447930947553, w0=-0.009175715844140613, w1=0.2128836668331083\n",
      "Gradient Descent(313/499): loss=0.8496224109687845, w0=-0.0545281268694411, w1=0.08731028894902348\n",
      "Gradient Descent(314/499): loss=0.753104260678249, w0=-0.033542270700734636, w1=0.13468800529873454\n",
      "Gradient Descent(315/499): loss=1.4656134415870112, w0=-0.054084362336651017, w1=0.06905041083985611\n",
      "Gradient Descent(316/499): loss=2.722144794257225, w0=-0.009175739664606948, w1=0.21288367182032197\n",
      "Gradient Descent(317/499): loss=0.8496224114193925, w0=-0.05452814949854019, w1=0.08731029368073931\n",
      "Gradient Descent(318/499): loss=0.7531042611488966, w0=-0.03354229220384054, w1=0.13468800976545828\n",
      "Gradient Descent(319/499): loss=1.4656134423850273, w0=-0.05408438275880083, w1=0.06905041508186868\n",
      "Gradient Descent(320/499): loss=2.7221447951980546, w0=-0.009175759061016885, w1=0.2128836758604899\n",
      "Gradient Descent(321/499): loss=0.8496224117789025, w0=-0.05452816792479723, w1=0.08731029751404101\n",
      "Gradient Descent(322/499): loss=0.7531042615250565, w0=-0.03354230971326572, w1=0.13468801338385905\n",
      "Gradient Descent(323/499): loss=1.4656134430258003, w0=-0.05408439938791758, w1=0.06905041851857396\n",
      "Gradient Descent(324/499): loss=2.722144795959505, w0=-0.009175774854903958, w1=0.21288367913372577\n",
      "Gradient Descent(325/499): loss=0.849622412065786, w0=-0.05452818292867899, w1=0.08731030061976791\n",
      "Gradient Descent(326/499): loss=0.7531042618257497, w0=-0.03354232397062791, w1=0.13468801631529814\n",
      "Gradient Descent(327/499): loss=1.4656134435403807, w0=-0.05408441292838404, w1=0.06905042130307837\n",
      "Gradient Descent(328/499): loss=2.7221447965758023, w0=-0.009175787715273838, w1=0.21288368178582392\n",
      "Gradient Descent(329/499): loss=0.8496224122947449, w0=-0.05452819514574542, w1=0.08731030313621357\n",
      "Gradient Descent(330/499): loss=0.7531042620661452, w0=-0.0335423355798534, w1=0.1346880186903848\n",
      "Gradient Descent(331/499): loss=1.4656134439536617, w0=-0.05408442395379905, w1=0.06905042355932559\n",
      "Gradient Descent(332/499): loss=2.7221447970745976, w0=-0.009175798186916086, w1=0.21288368393481463\n",
      "Gradient Descent(333/499): loss=0.8496224124775198, w0=-0.05452820509354882, w1=0.08731030517534027\n",
      "Gradient Descent(334/499): loss=0.7531042622583911, w0=-0.033542345032734136, w1=0.13468802061485757\n",
      "Gradient Descent(335/499): loss=1.4656134442856834, w0=-0.05408443293125525, w1=0.06905042538766626\n",
      "Gradient Descent(336/499): loss=2.722144797478325, w0=-0.009175806713462402, w1=0.21288368567626478\n",
      "Gradient Descent(337/499): loss=0.8496224126234448, w0=-0.05452821319353967, w1=0.08731030682780391\n",
      "Gradient Descent(338/499): loss=0.7531042624121429, w0=-0.03354235272974648, w1=0.13468802217432096\n",
      "Gradient Descent(339/499): loss=1.4656134445524256, w0=-0.054084440241110976, w1=0.06905042686935771\n",
      "Gradient Descent(340/499): loss=2.722144797805096, w0=-0.00917581365616757, w1=0.21288368708755712\n",
      "Gradient Descent(341/499): loss=0.8496224127399693, w0=-0.05452821978890881, w1=0.08731030816701382\n",
      "Gradient Descent(342/499): loss=0.7531042625351313, w0=-0.033542358997001774, w1=0.13468802343809178\n",
      "Gradient Descent(343/499): loss=1.4656134447667641, w0=-0.05408444619309296, w1=0.06905042807020248\n",
      "Gradient Descent(344/499): loss=2.722144798069596, w0=-0.009175819309201368, w1=0.21288368823135906\n",
      "Gradient Descent(345/499): loss=0.8496224128330317, w0=-0.05452822515911563, w1=0.08731030925242192\n",
      "Gradient Descent(346/499): loss=0.7531042626335268, w0=-0.03354236410005235, w1=0.1346880244623041\n",
      "Gradient Descent(347/499): loss=1.465613444939016, w0=-0.054084451039409884, w1=0.0690504290434948\n",
      "Gradient Descent(348/499): loss=2.7221447982836766, w0=-0.009175823912105155, w1=0.21288368915842715\n",
      "Gradient Descent(349/499): loss=0.8496224129073714, w0=-0.05452822953172257, w1=0.0873103101321814\n",
      "Gradient Descent(350/499): loss=0.7531042627122652, w0=-0.033542368255136595, w1=0.13468802529242016\n",
      "Gradient Descent(351/499): loss=1.4656134450774747, w0=-0.05408445498543289, w1=0.06905042983240064\n",
      "Gradient Descent(352/499): loss=2.722144798456976, w0=-0.00917582765993416, w1=0.21288368990987352\n",
      "Gradient Descent(353/499): loss=0.8496224129667592, w0=-0.054528233092029456, w1=0.08731031084529747\n",
      "Gradient Descent(354/499): loss=0.7531042627752745, w0=-0.03354237163833422, w1=0.13468802596526175\n",
      "Gradient Descent(355/499): loss=1.465613445188765, w0=-0.054084458198390695, w1=0.0690504304718881\n",
      "Gradient Descent(356/499): loss=2.7221447985972502, w0=-0.009175830711518125, w1=0.21288369051900113\n",
      "Gradient Descent(357/499): loss=0.8496224130142145, w0=-0.05452823599092225, w1=0.08731031142336693\n",
      "Gradient Descent(358/499): loss=0.7531042628257106, w0=-0.033542374393022975, w1=0.13468802651065792\n",
      "Gradient Descent(359/499): loss=1.4656134452782419, w0=-0.05408446081445367, w1=0.06905043099028474\n",
      "Gradient Descent(360/499): loss=2.7221447987108016, w0=-0.009175833196187588, w1=0.21288369101279064\n",
      "Gradient Descent(361/499): loss=0.8496224130521365, w0=-0.054528238351262706, w1=0.0873103118919892\n",
      "Gradient Descent(362/499): loss=0.7531042628660876, w0=-0.033542376635951866, w1=0.13468802695277157\n",
      "Gradient Descent(363/499): loss=1.465613445350196, w0=-0.05408446294450114, w1=0.06905043141054104\n",
      "Gradient Descent(364/499): loss=2.722144798802726, w0=-0.009175835219252464, w1=0.21288369141310223\n",
      "Gradient Descent(365/499): loss=0.8496224130824461, w0=-0.05452824027309328, w1=0.08731031227190536\n",
      "Gradient Descent(366/499): loss=0.7531042628984117, w0=-0.03354237846218573, w1=0.13468802731118007\n",
      "Gradient Descent(367/499): loss=1.4656134454080467, w0=-0.054084464678817286, w1=0.06905043175125386\n",
      "Gradient Descent(368/499): loss=2.7221447988771423, w0=-0.009175836866462728, w1=0.21288369173764576\n",
      "Gradient Descent(369/499): loss=0.8496224131066772, w0=-0.05452824183787443, w1=0.08731031257991989\n",
      "Gradient Descent(370/499): loss=0.7531042629243022, w0=-0.033542379949131826, w1=0.13468802760174498\n",
      "Gradient Descent(371/499): loss=1.4656134454545888, w0=-0.05408446609091713, w1=0.06905043202748987\n",
      "Gradient Descent(372/499): loss=2.722144798937386, w0=-0.009175838207640279, w1=0.21288369200077528\n",
      "Gradient Descent(373/499): loss=0.849622413126047, w0=-0.05452824311193506, w1=0.08731031282965329\n",
      "Gradient Descent(374/499): loss=0.7531042629450336, w0=-0.03354238115981957, w1=0.1346880278373196\n",
      "Gradient Descent(375/499): loss=1.4656134454920255, w0=-0.05408446724065972, w1=0.06905043225146205\n",
      "Gradient Descent(376/499): loss=2.722144798986156, w0=-0.00917583929963775, w1=0.21288369221412162\n",
      "Gradient Descent(377/499): loss=0.8496224131415346, w0=-0.05452824414928387, w1=0.08731031303214173\n",
      "Gradient Descent(378/499): loss=0.7531042629616379, w0=-0.03354238214557055, w1=0.13468802802831956\n",
      "Gradient Descent(379/499): loss=1.4656134455221335, w0=-0.054084468176785215, w1=0.06905043243306606\n",
      "Gradient Descent(380/499): loss=2.7221447990256413, w0=-0.009175840188747439, w1=0.2128836923871099\n",
      "Gradient Descent(381/499): loss=0.849622413153924, w0=-0.05452824499389708, w1=0.08731031319632895\n",
      "Gradient Descent(382/499): loss=0.7531042629749441, w0=-0.03354238294817301, w1=0.1346880281831853\n",
      "Gradient Descent(383/499): loss=1.4656134455463656, w0=-0.05408446893897974, w1=0.06905043258032174\n",
      "Gradient Descent(384/499): loss=2.722144799057612, w0=-0.009175840912661867, w1=0.21288369252738093\n",
      "Gradient Descent(385/499): loss=0.8496224131638298, w0=-0.0545282456815814, w1=0.0873103133294657\n",
      "Gradient Descent(386/499): loss=0.7531042629856015, w0=-0.033542383601652834, w1=0.1346880283087577\n",
      "Gradient Descent(387/499): loss=1.4656134455658587, w0=-0.054084469559557265, w1=0.06905043269973068\n",
      "Gradient Descent(388/499): loss=2.722144799083496, w0=-0.009175841502072145, w1=0.21288369264112603\n",
      "Gradient Descent(389/499): loss=0.849622413171754, w0=-0.05452824624149246, w1=0.08731031343742762\n",
      "Gradient Descent(390/499): loss=0.7531042629941389, w0=-0.03354238413371498, w1=0.13468802841058164\n",
      "Gradient Descent(391/499): loss=1.4656134455815362, w0=-0.05408447006482858, w1=0.06905043279656255\n",
      "Gradient Descent(392/499): loss=2.722144799104444, w0=-0.009175841981967664, w1=0.21288369273336494\n",
      "Gradient Descent(393/499): loss=0.849622413178096, w0=-0.05452824669736908, w1=0.08731031352497814\n",
      "Gradient Descent(394/499): loss=0.7531042630009882, w0=-0.033542384566917174, w1=0.13468802849315192\n",
      "Gradient Descent(395/499): loss=1.465613445594172, w0=-0.05408447047621688, w1=0.06905043287508791\n",
      "Gradient Descent(396/499): loss=2.7221447991214123, w0=-0.00917584237269499, w1=0.2128836928081662\n",
      "Gradient Descent(397/499): loss=0.8496224131831682, w0=-0.05452824706854009, w1=0.08731031359597871\n",
      "Gradient Descent(398/499): loss=0.7531042630064745, w0=-0.033542384919627216, w1=0.13468802856011103\n",
      "Gradient Descent(399/499): loss=1.4656134456043395, w0=-0.05408447081116505, w1=0.06905043293877076\n",
      "Gradient Descent(400/499): loss=2.722144799135161, w0=-0.009175842690821459, w1=0.2128836928688293\n",
      "Gradient Descent(401/499): loss=0.8496224131872239, w0=-0.054528247370743695, w1=0.0873103136535601\n",
      "Gradient Descent(402/499): loss=0.7531042630108651, w0=-0.033542385206800474, w1=0.13468802861441162\n",
      "Gradient Descent(403/499): loss=1.4656134456125014, w0=-0.054084471083875596, w1=0.06905043299041794\n",
      "Gradient Descent(404/499): loss=2.722144799146283, w0=-0.009175842949836505, w1=0.21288369291802678\n",
      "Gradient Descent(405/499): loss=0.8496224131904716, w0=-0.054528247616794125, w1=0.08731031370025946\n",
      "Gradient Descent(406/499): loss=0.7531042630143935, w0=-0.033542385440613186, w1=0.13468802865844964\n",
      "Gradient Descent(407/499): loss=1.4656134456190977, w0=-0.054084471305912776, w1=0.06905043303230493\n",
      "Gradient Descent(408/499): loss=2.722144799155289, w0=-0.009175843160722792, w1=0.21288369295792825\n",
      "Gradient Descent(409/499): loss=0.8496224131930739, w0=-0.05452824781712463, w1=0.08731031373813511\n",
      "Gradient Descent(410/499): loss=0.7531042630172233, w0=-0.03354238563097992, w1=0.13468802869416574\n",
      "Gradient Descent(411/499): loss=1.4656134456244094, w0=-0.05408447148669167, w1=0.06905043306627807\n",
      "Gradient Descent(412/499): loss=2.7221447991625873, w0=-0.009175843332422703, w1=0.21288369299029042\n",
      "Gradient Descent(413/499): loss=0.8496224131951556, w0=-0.05452824798022994, w1=0.08731031376885454\n",
      "Gradient Descent(414/499): loss=0.7531042630194876, w0=-0.033542385785973114, w1=0.13468802872313204\n",
      "Gradient Descent(415/499): loss=1.465613445628674, w0=-0.05408447163387792, w1=0.06905043309383263\n",
      "Gradient Descent(416/499): loss=2.7221447991685017, w0=-0.009175843472217449, w1=0.21288369301653828\n",
      "Gradient Descent(417/499): loss=0.8496224131968183, w0=-0.054528248113027054, w1=0.08731031379377038\n",
      "Gradient Descent(418/499): loss=0.7531042630213014, w0=-0.0335423859121657, w1=0.134688028746625\n",
      "Gradient Descent(419/499): loss=1.4656134456321068, w0=-0.05408447175371401, w1=0.06905043311618188\n",
      "Gradient Descent(420/499): loss=2.7221447991732837, w0=-0.009175843586035563, w1=0.21288369303782778\n",
      "Gradient Descent(421/499): loss=0.849622413198156, w0=-0.05452824822114782, w1=0.08731031381398\n",
      "Gradient Descent(422/499): loss=0.753104263022763, w0=-0.033542386014909, w1=0.13468802876568037\n",
      "Gradient Descent(423/499): loss=1.4656134456348822, w0=-0.05408447185128196, w1=0.06905043313430964\n",
      "Gradient Descent(424/499): loss=2.72214479917716, w0=-0.009175843678703742, w1=0.21288369305509605\n",
      "Gradient Descent(425/499): loss=0.8496224131992207, w0=-0.05452824830917707, w1=0.08731031383037219\n",
      "Gradient Descent(426/499): loss=0.7531042630239303, w0=-0.033542386098560174, w1=0.13468802878113564\n",
      "Gradient Descent(427/499): loss=1.4656134456371097, w0=-0.054084471930719115, w1=0.06905043314901366\n",
      "Gradient Descent(428/499): loss=2.722144799180296, w0=-0.009175843754151723, w1=0.212883693069103\n",
      "Gradient Descent(429/499): loss=0.8496224132000779, w0=-0.05452824838084829, w1=0.08731031384366869\n",
      "Gradient Descent(430/499): loss=0.7531042630248718, w0=-0.03354238616666671, w1=0.13468802879367217\n",
      "Gradient Descent(431/499): loss=1.4656134456389152, w0=-0.054084471995394734, w1=0.0690504331609408\n",
      "Gradient Descent(432/499): loss=2.722144799182839, w0=-0.009175843815579454, w1=0.21288369308046484\n",
      "Gradient Descent(433/499): loss=0.8496224132007604, w0=-0.0545282484392009, w1=0.08731031385445479\n",
      "Gradient Descent(434/499): loss=0.7531042630256201, w0=-0.033542386222117374, w1=0.13468802880384056\n",
      "Gradient Descent(435/499): loss=1.465613445640356, w0=-0.05408447204805167, w1=0.06905043317061588\n",
      "Gradient Descent(436/499): loss=2.7221447991848966, w0=-0.009175843865592108, w1=0.2128836930896812\n",
      "Gradient Descent(437/499): loss=0.8496224132013102, w0=-0.05452824848670981, w1=0.08731031386320406\n",
      "Gradient Descent(438/499): loss=0.7531042630262258, w0=-0.03354238626726337, w1=0.13468802881208927\n",
      "Gradient Descent(439/499): loss=1.4656134456415242, w0=-0.05408447209092331, w1=0.06905043317846432\n",
      "Gradient Descent(440/499): loss=2.7221447991865646, w0=-0.009175843906310918, w1=0.2128836930971573\n",
      "Gradient Descent(441/499): loss=0.849622413201751, w0=-0.05452824852539026, w1=0.0873103138703013\n",
      "Gradient Descent(442/499): loss=0.753104263026712, w0=-0.033542386304020066, w1=0.13468802881878011\n",
      "Gradient Descent(443/499): loss=1.465613445642469, w0=-0.05408447212582811, w1=0.06905043318483083\n",
      "Gradient Descent(444/499): loss=2.7221447991879204, w0=-0.009175843939462858, w1=0.21288369310322258\n",
      "Gradient Descent(445/499): loss=0.8496224132021006, w0=-0.05452824855688263, w1=0.0873103138760592\n",
      "Gradient Descent(446/499): loss=0.7531042630270972, w0=-0.033542386333946315, w1=0.13468802882420758\n",
      "Gradient Descent(447/499): loss=1.46561344564322, w0=-0.05408447215424642, w1=0.06905043318999596\n",
      "Gradient Descent(448/499): loss=2.7221447991890093, w0=-0.009175843966454274, w1=0.21288369310814279\n",
      "Gradient Descent(449/499): loss=0.8496224132023822, w0=-0.05452824858252262, w1=0.0873103138807303\n",
      "Gradient Descent(450/499): loss=0.75310426302741, w0=-0.033542386358311124, w1=0.13468802882861108\n",
      "Gradient Descent(451/499): loss=1.4656134456438292, w0=-0.054084472177383616, w1=0.06905043319418624\n",
      "Gradient Descent(452/499): loss=2.722144799189899, w0=-0.009175843988429667, w1=0.21288369311213393\n",
      "Gradient Descent(453/499): loss=0.8496224132026102, w0=-0.054528248603397986, w1=0.08731031388451915\n",
      "Gradient Descent(454/499): loss=0.7531042630276599, w0=-0.03354238637814827, w1=0.13468802883218256\n",
      "Gradient Descent(455/499): loss=1.4656134456443182, w0=-0.054084472196221145, w1=0.06905043319758519\n",
      "Gradient Descent(456/499): loss=2.7221447991906156, w0=-0.009175844006321147, w1=0.21288369311537161\n",
      "Gradient Descent(457/499): loss=0.8496224132027901, w0=-0.05452824862039373, w1=0.08731031388759292\n",
      "Gradient Descent(458/499): loss=0.7531042630278603, w0=-0.03354238639429877, w1=0.13468802883508\n",
      "Gradient Descent(459/499): loss=1.4656134456447119, w0=-0.05408447221155788, w1=0.06905043320034258\n",
      "Gradient Descent(460/499): loss=2.7221447991911956, w0=-0.009175844020887891, w1=0.21288369311799793\n",
      "Gradient Descent(461/499): loss=0.849622413202936, w0=-0.054528248634231134, w1=0.08731031389008623\n",
      "Gradient Descent(462/499): loss=0.7531042630280238, w0=-0.03354238640744797, w1=0.1346880288374303\n",
      "Gradient Descent(463/499): loss=1.4656134456450378, w0=-0.054084472224044514, w1=0.0690504332025791\n",
      "Gradient Descent(464/499): loss=2.7221447991916707, w0=-0.00917584403274748, w1=0.21288369312012906\n",
      "Gradient Descent(465/499): loss=0.84962241320305, w0=-0.054528248645496984, w1=0.08731031389210955\n",
      "Gradient Descent(466/499): loss=0.7531042630281504, w0=-0.033542386418153596, w1=0.1346880288393371\n",
      "Gradient Descent(467/499): loss=1.4656134456452887, w0=-0.05408447223421054, w1=0.06905043320439411\n",
      "Gradient Descent(468/499): loss=2.7221447991920544, w0=-0.009175844042403077, w1=0.21288369312185806\n",
      "Gradient Descent(469/499): loss=0.8496224132031437, w0=-0.054528248654669195, w1=0.08731031389375099\n",
      "Gradient Descent(470/499): loss=0.7531042630282545, w0=-0.03354238642686966, w1=0.13468802884088404\n",
      "Gradient Descent(471/499): loss=1.4656134456455014, w0=-0.05408447224248737, w1=0.06905043320586655\n",
      "Gradient Descent(472/499): loss=2.7221447991923635, w0=-0.0091758440502644, w1=0.21288369312326066\n",
      "Gradient Descent(473/499): loss=0.8496224132032181, w0=-0.054528248662136916, w1=0.08731031389508276\n",
      "Gradient Descent(474/499): loss=0.7531042630283377, w0=-0.03354238643396598, w1=0.13468802884213932\n",
      "Gradient Descent(475/499): loss=1.4656134456456684, w0=-0.05408447224922609, w1=0.06905043320706125\n",
      "Gradient Descent(476/499): loss=2.722144799192613, w0=-0.009175844056664738, w1=0.21288369312439853\n",
      "Gradient Descent(477/499): loss=0.849622413203278, w0=-0.05452824866821682, w1=0.08731031389616309\n",
      "Gradient Descent(478/499): loss=0.7531042630284058, w0=-0.03354238643974349, w1=0.13468802884315778\n",
      "Gradient Descent(479/499): loss=1.4656134456458085, w0=-0.0540844722547125, w1=0.06905043320803052\n",
      "Gradient Descent(480/499): loss=2.7221447991928174, w0=-0.009175844061875528, w1=0.2128836931253219\n",
      "Gradient Descent(481/499): loss=0.8496224132033261, w0=-0.05452824867316675, w1=0.08731031389703975\n",
      "Gradient Descent(482/499): loss=0.7531042630284597, w0=-0.033542386444447236, w1=0.1346880288439838\n",
      "Gradient Descent(483/499): loss=1.465613445645914, w0=-0.054084472259179124, w1=0.0690504332088169\n",
      "Gradient Descent(484/499): loss=2.72214479919298, w0=-0.009175844066117891, w1=0.2128836931260708\n",
      "Gradient Descent(485/499): loss=0.8496224132033652, w0=-0.05452824867719666, w1=0.08731031389775079\n",
      "Gradient Descent(486/499): loss=0.7531042630285035, w0=-0.033542386448276715, w1=0.13468802884465408\n",
      "Gradient Descent(487/499): loss=1.4656134456460015, w0=-0.05408447226281567, w1=0.06905043320945478\n",
      "Gradient Descent(488/499): loss=2.722144799193113, w0=-0.009175844069571878, w1=0.2128836931266784\n",
      "Gradient Descent(489/499): loss=0.849622413203396, w0=-0.0545282486804777, w1=0.08731031389832766\n",
      "Gradient Descent(490/499): loss=0.7531042630285393, w0=-0.03354238645139453, w1=0.13468802884519776\n",
      "Gradient Descent(491/499): loss=1.4656134456460745, w0=-0.054084472265776395, w1=0.06905043320997234\n",
      "Gradient Descent(492/499): loss=2.722144799193222, w0=-0.009175844072383949, w1=0.21288369312717179\n",
      "Gradient Descent(493/499): loss=0.8496224132034208, w0=-0.05452824868314898, w1=0.08731031389879612\n",
      "Gradient Descent(494/499): loss=0.7531042630285673, w0=-0.03354238645393297, w1=0.1346880288456393\n",
      "Gradient Descent(495/499): loss=1.4656134456461305, w0=-0.0540844722681869, w1=0.06905043321039248\n",
      "Gradient Descent(496/499): loss=2.7221447991933077, w0=-0.009175844074673367, w1=0.21288369312757202\n",
      "Gradient Descent(497/499): loss=0.8496224132034395, w0=-0.054528248685323744, w1=0.08731031389917615\n",
      "Gradient Descent(498/499): loss=0.7531042630285882, w0=-0.03354238645599961, w1=0.13468802884599737\n",
      "Gradient Descent(499/499): loss=1.4656134456461745, w0=-0.05408447227014933, w1=0.06905043321073366\n",
      "Gradient Descent(500/499): loss=2.7221447991933774, w0=-0.009175844076537314, w1=0.2128836931278968\n"
     ]
    }
   ],
   "source": [
    "w_reg_logreg, loss_reg_logreg = f.reg_logistic_regression(y_train_processed_logreg, tX_train_train, 0.5, np.array(initial_w), 500, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.531090020132298\n",
      "F1 score:  0.6798366192757835\n"
     ]
    }
   ],
   "source": [
    "y_pred = tX_train_test.dot(w_reg_logreg)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "y_pred = np.where(y_pred == 1, 1, -1)\n",
    "\n",
    "_,_,_,_,f1 = f.confusion_matrix(y_train_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", np.sum(y_pred == y_train_test)/len(y_train_test))\n",
    "\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [-0.00917584  0.21288369  0.38506364  0.16010529 -0.01373309 -0.00622924\n",
      " -0.10052114 -0.05696044 -0.13351661 -0.07252695 -0.06596427 -0.0719465\n",
      " -0.06936557 -0.06038476 -0.12316657  0.03060088  0.55660477  0.0008614\n",
      " -0.00178988  0.00175129  0.00513793  0.01785084] \n",
      " Loss =  2.7221447991933774 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.939339361303358\n"
     ]
    }
   ],
   "source": [
    "y_test_reg_logreg = tX_test.dot(w_reg_logreg)\n",
    "y_test_reg_logreg = np.where(y_test_reg_logreg > 0.5, 1, 0)\n",
    "\n",
    "print('weights = \\n', w_reg_logreg,'\\n Loss = ', loss_reg_logreg,'\\n*****************************************************************************',\n",
    "        ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train== 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_reg_logreg == 1)/len(y_test_reg_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sub = np.where(y_test_reg_logreg == 1, 1, -1)\n",
    "h.create_csv_submission(test_ids, y_sub, 'submission_reg_logreg2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
