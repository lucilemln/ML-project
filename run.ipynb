{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "try:\n",
    "    import importlib\n",
    "    importlib.reload(h)\n",
    "    importlib.reload(f)\n",
    "    importlib.reload(d)\n",
    "except NameError: # It hasn't been imported yet\n",
    "    import helpers as h\n",
    "    import implementations as f\n",
    "    import data_processing as d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing and feature selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#For this to work, the data folder needs to be one level above the project folder and the folder name needs\n",
    "#to be 'data'\n",
    "data_folder = '../data/'\n",
    "x_train, x_test, y_train, train_ids, test_ids = h.load_csv_data(data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\EPFL\\MA1\\ML-project\\run.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/EPFL/MA1/ML-project/run.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x_train, x_test, y_train, train_ids, test_ids \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39;49mload_csv_data(\u001b[39m\"\u001b[39;49m\u001b[39m../data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\EPFL\\MA1\\ML-project\\helpers.py:30\u001b[0m, in \u001b[0;36mload_csv_data\u001b[1;34m(data_path, sub_sample)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mThis function loads the data and returns the respectinve numpy arrays.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mRemember to put the 3 files in the same folder and to not change the names of the files.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39m    test_ids (np.array): ids of test data\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mgenfromtxt(\n\u001b[0;32m     24\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, \u001b[39m\"\u001b[39m\u001b[39my_train.csv\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     25\u001b[0m     delimiter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     usecols\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     29\u001b[0m )\n\u001b[1;32m---> 30\u001b[0m x_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mgenfromtxt(\n\u001b[0;32m     31\u001b[0m     os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(data_path, \u001b[39m\"\u001b[39;49m\u001b[39mx_train.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m), delimiter\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m, skip_header\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     33\u001b[0m x_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mgenfromtxt(\n\u001b[0;32m     34\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, \u001b[39m\"\u001b[39m\u001b[39mx_test.csv\u001b[39m\u001b[39m\"\u001b[39m), delimiter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m, skip_header\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     37\u001b[0m train_ids \u001b[39m=\u001b[39m x_train[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mastype(dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\npyio.py:2324\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[0;32m   2320\u001b[0m \u001b[39m# Convert each value according to the converter:\u001b[39;00m\n\u001b[0;32m   2321\u001b[0m \u001b[39m# We want to modify the list in place to avoid creating a new one...\u001b[39;00m\n\u001b[0;32m   2322\u001b[0m \u001b[39mif\u001b[39;00m loose:\n\u001b[0;32m   2323\u001b[0m     rows \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m-> 2324\u001b[0m         \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[[conv\u001b[39m.\u001b[39m_loose_call(_r) \u001b[39mfor\u001b[39;00m _r \u001b[39min\u001b[39;00m \u001b[39mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2325\u001b[0m               \u001b[39mfor\u001b[39;00m (i, conv) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(converters)]))\n\u001b[0;32m   2326\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2327\u001b[0m     rows \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   2328\u001b[0m         \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[[conv\u001b[39m.\u001b[39m_strict_call(_r) \u001b[39mfor\u001b[39;00m _r \u001b[39min\u001b[39;00m \u001b[39mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2329\u001b[0m               \u001b[39mfor\u001b[39;00m (i, conv) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(converters)]))\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\npyio.py:2324\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2320\u001b[0m \u001b[39m# Convert each value according to the converter:\u001b[39;00m\n\u001b[0;32m   2321\u001b[0m \u001b[39m# We want to modify the list in place to avoid creating a new one...\u001b[39;00m\n\u001b[0;32m   2322\u001b[0m \u001b[39mif\u001b[39;00m loose:\n\u001b[0;32m   2323\u001b[0m     rows \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m-> 2324\u001b[0m         \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[[conv\u001b[39m.\u001b[39m_loose_call(_r) \u001b[39mfor\u001b[39;00m _r \u001b[39min\u001b[39;00m \u001b[39mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2325\u001b[0m               \u001b[39mfor\u001b[39;00m (i, conv) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(converters)]))\n\u001b[0;32m   2326\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2327\u001b[0m     rows \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   2328\u001b[0m         \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[[conv\u001b[39m.\u001b[39m_strict_call(_r) \u001b[39mfor\u001b[39;00m _r \u001b[39min\u001b[39;00m \u001b[39mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2329\u001b[0m               \u001b[39mfor\u001b[39;00m (i, conv) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(converters)]))\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\npyio.py:2324\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2320\u001b[0m \u001b[39m# Convert each value according to the converter:\u001b[39;00m\n\u001b[0;32m   2321\u001b[0m \u001b[39m# We want to modify the list in place to avoid creating a new one...\u001b[39;00m\n\u001b[0;32m   2322\u001b[0m \u001b[39mif\u001b[39;00m loose:\n\u001b[0;32m   2323\u001b[0m     rows \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m-> 2324\u001b[0m         \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[[conv\u001b[39m.\u001b[39;49m_loose_call(_r) \u001b[39mfor\u001b[39;00m _r \u001b[39min\u001b[39;00m \u001b[39mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2325\u001b[0m               \u001b[39mfor\u001b[39;00m (i, conv) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(converters)]))\n\u001b[0;32m   2326\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2327\u001b[0m     rows \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   2328\u001b[0m         \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[[conv\u001b[39m.\u001b[39m_strict_call(_r) \u001b[39mfor\u001b[39;00m _r \u001b[39min\u001b[39;00m \u001b[39mmap\u001b[39m(itemgetter(i), rows)]\n\u001b[0;32m   2329\u001b[0m               \u001b[39mfor\u001b[39;00m (i, conv) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(converters)]))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = h.load_csv_data(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/y_train.npy', y_train)\n",
    "np.save('../data/x_train.npy', x_train)\n",
    "np.save('../data/x_test.npy', x_test)\n",
    "np.save('../data/train_ids.npy', train_ids)\n",
    "np.save('../data/test_ids.npy', test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = np.load(\"../data/x_train.npy\")\n",
    "x_test = np.load(\"../data/x_test.npy\")\n",
    "y_train = np.load(\"../data/y_train.npy\")\n",
    "train_ids = np.load(\"../data/train_ids.npy\")\n",
    "test_ids = np.load(\"../data/test_ids.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['_STATE', 'FMONTH', 'IDATE', 'IMONTH', 'IDAY', 'IYEAR', 'DISPCODE',\n",
       "       'SEQNO', '_PSU', 'CTELENUM', 'PVTRESD1', 'COLGHOUS', 'STATERES',\n",
       "       'CELLFON3', 'LADULT', 'NUMADULT', 'NUMMEN', 'NUMWOMEN', 'CTELNUM1',\n",
       "       'CELLFON2', 'CADULT', 'PVTRESD2', 'CCLGHOUS', 'CSTATE', 'LANDLINE',\n",
       "       'HHADULT', 'GENHLTH', 'PHYSHLTH', 'MENTHLTH', 'POORHLTH',\n",
       "       'HLTHPLN1', 'PERSDOC2', 'MEDCOST', 'CHECKUP1', 'BPHIGH4', 'BPMEDS',\n",
       "       'BLOODCHO', 'CHOLCHK', 'TOLDHI2', 'CVDSTRK3', 'ASTHMA3', 'ASTHNOW',\n",
       "       'CHCSCNCR', 'CHCOCNCR', 'CHCCOPD1', 'HAVARTH3', 'ADDEPEV2',\n",
       "       'CHCKIDNY', 'DIABETE3', 'DIABAGE2', 'SEX', 'MARITAL', 'EDUCA',\n",
       "       'RENTHOM1', 'NUMHHOL2', 'NUMPHON2', 'CPDEMO1', 'VETERAN3',\n",
       "       'EMPLOY1', 'CHILDREN', 'INCOME2', 'INTERNET', 'WEIGHT2', 'HEIGHT3',\n",
       "       'PREGNANT', 'QLACTLM2', 'USEEQUIP', 'BLIND', 'DECIDE', 'DIFFWALK',\n",
       "       'DIFFDRES', 'DIFFALON', 'SMOKE100', 'SMOKDAY2', 'STOPSMK2',\n",
       "       'LASTSMK2', 'USENOW3', 'ALCDAY5', 'AVEDRNK2', 'DRNK3GE5',\n",
       "       'MAXDRNKS', 'FRUITJU1', 'FRUIT1', 'FVBEANS', 'FVGREEN', 'FVORANG',\n",
       "       'VEGETAB1', 'EXERANY2', 'EXRACT11', 'EXEROFT1', 'EXERHMM1',\n",
       "       'EXRACT21', 'EXEROFT2', 'EXERHMM2', 'STRENGTH', 'LMTJOIN3',\n",
       "       'ARTHDIS2', 'ARTHSOCL', 'JOINPAIN', 'SEATBELT', 'FLUSHOT6',\n",
       "       'FLSHTMY2', 'IMFVPLAC', 'PNEUVAC3', 'HIVTST6', 'HIVTSTD3',\n",
       "       'WHRTST10', 'PDIABTST', 'PREDIAB1', 'INSULIN', 'BLDSUGAR',\n",
       "       'FEETCHK2', 'DOCTDIAB', 'CHKHEMO3', 'FEETCHK', 'EYEEXAM',\n",
       "       'DIABEYE', 'DIABEDU', 'CAREGIV1', 'CRGVREL1', 'CRGVLNG1',\n",
       "       'CRGVHRS1', 'CRGVPRB1', 'CRGVPERS', 'CRGVHOUS', 'CRGVMST2',\n",
       "       'CRGVEXPT', 'VIDFCLT2', 'VIREDIF3', 'VIPRFVS2', 'VINOCRE2',\n",
       "       'VIEYEXM2', 'VIINSUR2', 'VICTRCT4', 'VIGLUMA2', 'VIMACDG2',\n",
       "       'CIMEMLOS', 'CDHOUSE', 'CDASSIST', 'CDHELP', 'CDSOCIAL',\n",
       "       'CDDISCUS', 'WTCHSALT', 'LONGWTCH', 'DRADVISE', 'ASTHMAGE',\n",
       "       'ASATTACK', 'ASERVIST', 'ASDRVIST', 'ASRCHKUP', 'ASACTLIM',\n",
       "       'ASYMPTOM', 'ASNOSLEP', 'ASTHMED3', 'ASINHALR', 'HAREHAB1',\n",
       "       'STREHAB1', 'CVDASPRN', 'ASPUNSAF', 'RLIVPAIN', 'RDUCHART',\n",
       "       'RDUCSTRK', 'ARTTODAY', 'ARTHWGT', 'ARTHEXER', 'ARTHEDU',\n",
       "       'TETANUS', 'HPVADVC2', 'HPVADSHT', 'SHINGLE2', 'HADMAM', 'HOWLONG',\n",
       "       'HADPAP2', 'LASTPAP2', 'HPVTEST', 'HPLSTTST', 'HADHYST2',\n",
       "       'PROFEXAM', 'LENGEXAM', 'BLDSTOOL', 'LSTBLDS3', 'HADSIGM3',\n",
       "       'HADSGCO1', 'LASTSIG3', 'PCPSAAD2', 'PCPSADI1', 'PCPSARE1',\n",
       "       'PSATEST1', 'PSATIME', 'PCPSARS1', 'PCPSADE1', 'PCDMDECN',\n",
       "       'SCNTMNY1', 'SCNTMEL1', 'SCNTPAID', 'SCNTWRK1', 'SCNTLPAD',\n",
       "       'SCNTLWK1', 'SXORIENT', 'TRNSGNDR', 'RCSGENDR', 'RCSRLTN2',\n",
       "       'CASTHDX2', 'CASTHNO2', 'EMTSUPRT', 'LSATISFY', 'ADPLEASR',\n",
       "       'ADDOWN', 'ADSLEEP', 'ADENERGY', 'ADEAT1', 'ADFAIL', 'ADTHINK',\n",
       "       'ADMOVE', 'MISTMNT', 'ADANXEV', 'QSTVER', 'QSTLANG', 'MSCODE',\n",
       "       '_STSTR', '_STRWT', '_RAWRAKE', '_WT2RAKE', '_CHISPNC', '_CRACE1',\n",
       "       '_CPRACE', '_CLLCPWT', '_DUALUSE', '_DUALCOR', '_LLCPWT',\n",
       "       '_RFHLTH', '_HCVU651', '_RFHYPE5', '_CHOLCHK', '_RFCHOL',\n",
       "       '_LTASTH1', '_CASTHM1', '_ASTHMS1', '_DRDXAR1', '_PRACE1',\n",
       "       '_MRACE1', '_HISPANC', '_RACE', '_RACEG21', '_RACEGR3', '_RACE_G1',\n",
       "       '_AGEG5YR', '_AGE65YR', '_AGE80', '_AGE_G', 'HTIN4', 'HTM4',\n",
       "       'WTKG3', '_BMI5', '_BMI5CAT', '_RFBMI5', '_CHLDCNT', '_EDUCAG',\n",
       "       '_INCOMG', '_SMOKER3', '_RFSMOK3', 'DRNKANY5', 'DROCDY3_',\n",
       "       '_RFBING5', '_DRNKWEK', '_RFDRHV5', 'FTJUDA1_', 'FRUTDA1_',\n",
       "       'BEANDAY_', 'GRENDAY_', 'ORNGDAY_', 'VEGEDA1_', '_MISFRTN',\n",
       "       '_MISVEGN', '_FRTRESP', '_VEGRESP', '_FRUTSUM', '_VEGESUM',\n",
       "       '_FRTLT1', '_VEGLT1', '_FRT16', '_VEG23', '_FRUITEX', '_VEGETEX',\n",
       "       '_TOTINDA', 'METVL11_', 'METVL21_', 'MAXVO2_', 'FC60_', 'ACTIN11_',\n",
       "       'ACTIN21_', 'PADUR1_', 'PADUR2_', 'PAFREQ1_', 'PAFREQ2_',\n",
       "       '_MINAC11', '_MINAC21', 'STRFREQ_', 'PAMISS1_', 'PAMIN11_',\n",
       "       'PAMIN21_', 'PA1MIN_', 'PAVIG11_', 'PAVIG21_', 'PA1VIGM_',\n",
       "       '_PACAT1', '_PAINDX1', '_PA150R2', '_PA300R2', '_PA30021',\n",
       "       '_PASTRNG', '_PAREC1', '_PASTAE1', '_LMTACT1', '_LMTWRK1',\n",
       "       '_LMTSCL1', '_RFSEAT2', '_RFSEAT3', '_FLSHOT6', '_PNEUMO2',\n",
       "       '_AIDTST3'], dtype='<U8')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features_named all the features names and remove the ID column\n",
    "features_name = np.genfromtxt('../data/x_train.csv', delimiter=',', dtype=str, max_rows=1)[1:] \n",
    "features_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "one paper on internet suggests to use these features : \n",
    "\n",
    " _RFHYPE5, TOLDHI2, _CHOLCHK, _BMI5, SMOKE100, CVDSTRK3, DIABETE3, _TOTINDA, _FRTLT1, _VEGLT1, _RFDRHV5, HLTHPLN1, MEDCOST, GENHLTH, MENTHLTH, PHYSHLTH, DIFFWALK, SEX, _AGEG5YR, EDUCA, and INCOME2\n",
    "\n",
    "We apply a mask to get only these important features.\n",
    "\n",
    "Then using we use our preprocessing function. For feature where the answer is yes or no we make the data binary, ordinal (categorical) variables ares changed to 0,1,2,...,Missing values are replace by the mean of the column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the important features\n",
    "features_list = ['_RFHYPE5', 'TOLDHI2', '_CHOLCHK', '_BMI5', 'SMOKE100', 'CVDSTRK3', 'DIABETE3', '_TOTINDA', '_FRTLT1', '_VEGLT1', '_RFDRHV5', \n",
    "                 'HLTHPLN1', 'MEDCOST', 'GENHLTH', 'MENTHLTH', 'PHYSHLTH', 'DIFFWALK', 'SEX', '_AGEG5YR', 'EDUCA', 'INCOME2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.20628162 0.         ... 1.         1.         0.        ]\n",
      "328135\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "trainMask, testMask = masking((x_train, x_test), features_name, features_list)\n",
    "trainProcessed = d.feature_processing_test(trainMask)\n",
    "\n",
    "#Test data Processing \n",
    "testProcessed  = d.feature_processing_test(testMask)\n",
    "\n",
    "x_train_algo = f.replaceMissingValuesMean(trainProcessed)\n",
    "x_test_algo = f.replaceMissingValuesMean(testProcessed)\n",
    "print(x_train_algo[:,19])\n",
    "print(len(x_train_algo))\n",
    "print(len(features_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n"
     ]
    }
   ],
   "source": [
    "##test chelou\n",
    "#x1_stand=f.standardize(x_train_algo)\n",
    "x_train_stand=np.ones(x_train_algo.shape)\n",
    "x_test_stand=np.ones(x_test_algo.shape)\n",
    "\n",
    "x_trai_stand=f.standardize(x_train_algo)\n",
    "\n",
    "for i in range(len(features_list)):\n",
    "  x_train_stand[i]=f.standardize(x_train_algo[i])\n",
    "  x_test_stand[i]=f.standardize(x_test_algo[i])\n",
    "\n",
    "print(x_trai_stand==x_train_stand)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace missing values by the mean of the column for the training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replaceMissingValuesMean(X):\n",
    "    #compute the mean of the column\n",
    "    mean = np.nanmean(X, axis = 0)\n",
    "\n",
    "    #replace all the NaN values by the mean\n",
    "    X = np.where(np.isnan(X), mean, X)\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing [Necessary] \n",
    "For this to work, Masking has to be done aswell\n",
    "### We want to clean the data for each feature, making them binary for yes/no, etc... and rename them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Data Processing \n",
    "\n",
    "def featureProcessing(dataMasked):\n",
    "    \n",
    "    x_train, x_test = dataMasked\n",
    "    \n",
    "    x_train_processed = d.feature_processing_test(x_train)\n",
    "\n",
    "    #Test data Processing \n",
    "    x_test_processed = d.feature_processing_test(x_test)\n",
    "    \n",
    "    return x_train_processed, x_test_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can apply the processing functions to the data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainMask, testMask = masking((x_train, x_test), features_list)\n",
    "\n",
    "trainProcessed, testProcessed = featureProcessing((trainMask ,testMask))\n",
    "\n",
    "x_train_algo = replaceMissingValuesMean(trainProcessed)\n",
    "x_test_algo = replaceMissingValuesMean(testProcessed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the preprocessing has been done, we can format the data to be used by the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tX_train = np.c_[np.ones((len(x_train_algo), 1)), x_train_algo]\n",
    "tX_test = np.c_[np.ones((len(x_test_algo), 1)), x_test_algo]\n",
    "\n",
    "tX_train = np.c_[np.ones((len(x_train_algo), 1)), x_train_stand]\n",
    "tX_test = np.c_[np.ones((len(x_test_algo), 1)), x_test_stand]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation of set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_w = [random.choice([1, -1]) for i in range(len(tX_train[0]))]\n",
    "initial_w = np.ones(len(tX_train[0]))\n",
    "max_iter = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation of the dataset in a test/train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tX_train_train = tX_train[:int(len(tX_train)*0.7)]\n",
    "y_train_train = y_train[:int(len(tX_train)*0.7)]\n",
    "tX_train_test = tX_train[int(len(tX_train)*0.7):]\n",
    "y_train_test = y_train[int(len(tX_train)*0.7):]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_progression(w):\n",
    "    # Plot progression of the weights in function of the iteration and progression on the test set\n",
    "    plt.figure(0)\n",
    "    plt.plot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And then, we can run the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLossMSE(weights, loss, y, x ):\n",
    "    loss_test_set = []\n",
    "\n",
    "    for w in weights:\n",
    "        loss_test_set.append(f.compute_mse(y, x, w))\n",
    "\n",
    "    plt.figure(0)\n",
    "    plt.semilogy(loss)\n",
    "    plt.semilogy(loss_test_set)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. MSE gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(999/1000): Final loss=0.16322289498404216\n"
     ]
    }
   ],
   "source": [
    "#Compute gradient descent with MSE as loss function (see functions.py for the function)\n",
    "\n",
    "w_mse_gd, loss_mse_gd = f.mean_squared_error_gd(y_train_train, tX_train_train, initial_w, 1000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_test_set = []\n",
    "\n",
    "for w in w_mse_gd:\n",
    "    loss_test_set.append(f.compute_mse(y_train_test, tX_train_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAAGdCAYAAAAmK7htAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA2V0lEQVR4nO3daXjNd/7/8dc3+x5L7GInJBKKlMRO0EX3fVVd0FIUMWbmN9P/zHTaadFqSRVddNPqvkxbRYKgVYqQEPsaW6yJhGwn3/8NaqbTai1JPmd5Pq7LjUaHV6/Tznle532cWLZt2wIAAHACXqYHAAAA/IQwAQAAToMwAQAAToMwAQAAToMwAQAAToMwAQAAToMwAQAAToMwAQAATsPH9ICLVV5erv379ys0NFSWZZmeAwAALoBt2zp58qTq168vL6/zvy7icmGyf/9+RUZGmp4BAAAuwd69e9WwYcPz/rzLhUloaKikM/9gYWFhhtcAAIALkZ+fr8jIyHPP4+fjcmHy0/kmLCyMMAEAwMX83tswePMrAABwGoQJAABwGoQJAABwGoQJAABwGoQJAABwGoQJAABwGoQJAABwGoQJAABwGoQJAABwGi4TJikpKYqOjlZ8fLzpKQAAoJJYtm3bpkdcjPz8fIWHhysvL4+PpAcAwEVc6PO3y7xiAgAA3B9hctbWpR8p68WbVVJ4wvQUAAA8FmEiqehUgWqkjlXb46k6/HyCDm350fQkAAA8EmEiKSAoRNv7ztQB1VQDx36Fz7lK2V+nmJ4FAIDHIUzOurL7AJU/kq4ffTspQKVqs/JPyky5W2VFBaanAQDgMQiT/9KgQUPFjp+nhfWGyGFbij38lfZP6qqju7JMTwMAwCMQJv/D39dXSUMn6ofub+iIHa5GZbsUMLuvNqfONj0NAAC3R5icR2LSTSoYvFjrfGIVrCJFLR2l9TMeVnlJkelpAAC4LcLkNzRp0kytxqUqNeI+SVLcgQ+1a1J3ndi31fAyAADcE2HyOwID/NV3xDSlXzldx+0QNSvZIq9ZPbVt6QempwEA4HYIkwvU45q7deSehdro1UphKlSL1Ee0/vWRsstKTE8DAMBtECYXoWWrNoocu1hp1W6VJMXteVPbJvVWfu4es8MAAHAThMlFCg0OVu9Rr2pxu8k6aQeqZVGWHC93084fvjQ9DQAAl0eYXALLstTrpoeVc/s8bbWaqLry1Pjr+7T+nQmyHWWm5wEA4LIIk8vQJqa9ao1O15KQa+Rl2YrbNl2bJw9Q4bEDpqcBAOCSCJPLVC08XN3HzFFqm7/rlO2v1qd+1OmpidqbkWp6GgAALocwqQBeXpb63jFK2274XDvVQBH2MdX79FZlfvgPybZNzwMAwGUQJhUorkOCQkcu1fLA3vKxyhW7YZI2vjBQRflHTU8DAMAlECYVLKJGTXUZ94lSm01Qse2j6PxlOjElQQeyvzc9DQAAp0eYVAJvby/1vf+P2nD1R8pRbdUtP6Sa7w/Uhs8nc9oBAOA3ECaVqEOX3vJ5dKl+8E+Qn1WmmLV/V9ZLt6mkMM/0NAAAnBJhUsnq1qmrDslfaWHkSJXa3mp7fIFyn09U7rY1pqcBAOB0XCZMUlJSFB0drfj4eNNTLpqvj7eSHvqH1vZ5W4dUQw0dOQp9Z4Cyv3nF9DQAAJyKZduu9aaH/Px8hYeHKy8vT2FhYabnXLScnD3KfXOQOpSeecVkfe3rFfPQDHn7BxleBgBA5bnQ52+XecXEXTRs2EjR475Vat2HVW5bisv9QjkTE3V0z0bT0wAAMI4wMSDA3099h03W991e1RE7XI3Ldsr/9T7anPa26WkAABhFmBjUtd+tOvlAmjK9oxWi04pKH6H1s4aovLTY9DQAAIwgTAxr2rSFmicvUmrNuyVJcfvmaufEHsrbv93wMgAAqh5h4gSCAgLUZ8TLWtppmk7YwWpesknWzB7avvxj09MAAKhShImTsCxL3Qfep8N3L1C2V0uFqUDNFzyodbOfkO0oNT0PAIAqQZg4mZZRMWo4ZrEWh98oSWq363VtndRXJ4/sNTsMAIAqQJg4odCQEPUcPVuLY59VgR2gVqfXqXRaV+1a9bXpaQAAVCrCxElZlqVetwzTnlu/1jarsWooT5H/vlvr5/xZdrnD9DwAACoFYeLkomM7quaodC0NHiBvy1bclmnaPPkqnTpxyPQ0AAAqHGHiAqpXq6auY+cqLepJnbb91LpwpQpfTFTO+sWmpwEAUKEIExfh5WWpz11jtOX6z7Rb9VXLPqI6H9+szI+ellzr2x0BAHBehImLadexq4JHLNX3gT3kazkUm/WsNky5XkUnj5meBgDAZSNMXFBERISuHPe5Upsmq8T2Vkxeuo6/kKADm34wPQ0AgMtCmLgob28v9R30f8oc8IH2q5bqlR9Ujfev1YYvpnDaAQC4LMLExXVMTJI1LF2r/K6Uv0oVs+ZJZU69Q6Wn801PAwDgohEmbqBe3fpqP/4bpTYYrjLbS7HHvtWhSYnK3bbG9DQAAC4KYeImfH181PeRp7W619s6pOpq6Nir0HcGKPvrlzntAABcBmHiZjr3HqjSh9K1xreDAlWiNiv/qMyUu1V6+qTpaQAA/C7CxA01jGykmPHzlVp/qBy2pdgjX+vgpAQd3r7W9DQAAH4TYeKm/H191XfIc/qx55vKtasr0rFXIW/3V/Y3001PAwDgvAgTN9e5zw0qfniJ1vhecea088MEZU67W2WcdgAATogw8QCRkY0VM36BFtYbcva085UOTE7U4R3rTE8DAOBnCBMP4e/rq6ShE7Wyx2wdtqspsmyPQt7qp03zZpqeBgDAOYSJh0noe6NOP7REa33aK1DFar0iWZkp96isqMD0NAAACBNP1KhRE7VJXqCFdR8+c9o5/G/tn5SoIzs57QAAzCJMPFSAv5+Shk3WD93f0BE7XI3Kdiv4zX7a9C2nHQCAOS4TJikpKYqOjlZ8fLzpKW4lMekmnXpwidb6tDtz2vk+WZkv3ytHcaHpaQAAD2TZtmt9Xnl+fr7Cw8OVl5ensLAw03PcRlFxiZa9PkF9Dr4uL8vWHp8mCr73HdVsEmt6GgDADVzo87fLvGKCyhXg76ekR5/X991eO3va2aXA2UnaPP9V09MAAB6EMMHPdO13iwoGL1aGT5yCVKSo78Yqc/r9chSfMj0NAOABCBP8QpMmzdQ6OVULaw9WuW0p9tDn2jcxUUd3ZZqeBgBwc4QJflWAv5+SHpui77q+eva0s1OBs5O0ZcHrpqcBANwYYYLf1K3/rTr5wCKt845VkIrUavkTypw+iNMOAKBSECb4XU2bNler5DQtrPXA2dPOZ8qZlKhjuzeYngYAcDOECS5IYICfkoa/qO8SZuqoHabGpTvl/0YfbVn4hulpAAA3QpjgonS76nblP5Cmdd5tFawitVo2WpmvDOa0AwCoEIQJLlrTpi3VMjlVCyPuP3PaOfiJciZ11bE9G01PAwC4OMIElyQoIEBJI6ZqeZcZZ087O+T/em9tTX3T9DQAgAsjTHBZul99h04MStN67xgFq0gtl45U5owHVV5y2vQ0AIALIkxw2Zo3a6kWyWlaEHGfJCn2wMfaM7Grju/NNrwMAOBqCBNUiKCAAPUbMU3pV87QMTtUTUq3y++13tqW9pbpaQAAF0KYoEL1uOZOHb8vTeu9oxWs02qR/rgyZz7EaQcAcEEIE1S45i1aqfm4RVpQ8x5JUuz+j7RnYjedyNlkeBkAwNkRJqgUwYEBShqRoiXx03XcDlWT0m3yfbWXti9+2/Q0AIATI0xQaSzLUs9r79bR+1LPnXaaLx6hrFmPqLykyPQ8AIATIkxQ6Vq0iFKzcYu0oMbdkqS2+z7QnkndlJez2fAyAICzIUxQJUICA5T0+Mta3OnlM6edkq3yfrWXti95x/Q0AIATIUxQZSzLUq+B9+jwPQuV5dVaITql5ouGK2vWEE47AABJhAkMaNWqtZqMW6wFNe6SJLXdN1d7JnVX3r4thpcBAEwjTGBESFCgkh6frkUdp+m4HaImJVvkPaundqTPMT0NAGAQYQJjLMtS7+vuU+49C5XlFaUQnVKztEeV9eow2aWcdgDAExEmMC6qVRs1HrtYC6rfIUlqm/Oedk3sofz9W80OAwBUOcIETiE0OEhJI2corcNUHbdD1LRks6yZPbQj/T3T0wAAVYgwgdOwLEt9rr9fuXcvUJZXlEJ1Ss3ShinrtUdllxWbngcAqAKECZxOVFS0Go1drIXVzp529s7Rrue6c9oBAA9AmMAphQUHqe+oGVp0xYs6YQefO+1s57QDAG6NMIHTsixLvW94QIfuXqgNZ087zTntAIBbI0zg9DjtAIDnIEzgEkJ/Ou10eInTDgC4McIELsOyLPW+ftAvTjuZr3LaAQB3QZjA5Zw77Zz9QLbYHE47AOAuXCZMUlJSFB0drfj4eNNT4ARCg4PUd+TPTzua2VPbl75vehoA4DJYtm3bpkdcjPz8fIWHhysvL09hYWGm58AJbN60QWUfDFZM+WZJUmbDu9X2gSmyfPwNLwMA/ORCn79d5hUT4HyiWsf84rSz87nuytvHaQcAXA1hArfwv6edZiWbZc3qqW3pnHYAwJUQJnAbP/2pndy7F2iDV5TCVKgWaUOV+dqjKi/lT+0AgCsgTOB2WkX9z2ln7xztmshpBwBcAWECt/TTaWcxpx0AcCmECdyWZVnqdfa0s9Gr1bnTzvpXOe0AgLMiTOD2WkXFKHLsYqVWv12SFJdz9rSzf5vZYQCAXyBM4BFCg4PVZ+RMLekw9T+nnZk9OO0AgJMhTOAxLMtSz+vv1+G7F3LaAQAnRZjA47SMilajcUs47QCAEyJM4JFCgoLOnnZe4rQDAE6EMIHHOnPaGcRpBwCcCGECj3e+084JTjsAUOUIE0D/Oe2kd/zPacdrZg9tW8JpBwCqEmECnGVZlnpc9z+nnUVDte7VxzjtAEAVIUyA//HTaSetxpnTTrucd7VzYg9OOwBQBQgT4FeEBAWp9+P/Oe00L9nEaQcAqgBhApzHudPOPZx2AKCqECbA72jZitMOAFQVwgS4AD+ddpZ2+vlpZysfyAYAFYowAS6QZVnqPvDnp52WaUOVwWkHACoMYQJcpJatotX4v0477TntAECFIUyASxDMaQcAKgVhAlyin047R+7ltAMAFYUwAS5Ti5ZnTjuLatwh6T+nneP7thpeBgCuhzABKkBwUJB6PT5DyzpNPXfa8Z7VU1v4QDYAuCiECVBBLMtSt4H3/+y002rRUGXM4rQDABeKMAEq2C9OO/vOnnZythheBgDOjzABKsGvnXZ8Xu2pLYvnmJ4GAE6NMAEqyU+nnaP3pmqjd5RCdUqtFj+qdTOHqLykyPQ8AHBKhAlQyZq3bKMm45ZoUc07JUnt9s/VronddHTvJsPLAMD5ECZAFQgKDFTvx2doaXyKTtghala6Vf6v9dbmtLdMTwMAp0KYAFWo+7X36vj9acrybqMQnVJU+uNaN+NhOUpOm54GAE6BMAGqWNPmUWo+brHSIu6RJLU78KH2Tuyqo3s2Gl4GAOYRJoABgYEB6jPiZS3v8oqO2aFqUrpdAa/31uaFb5ieBgBGESaAQV2vukt5gxZpvXeMglWkqGWjtX76A3IUnzI9DQCMIEwAw5o2a6lW4xcprfYglduW4g59qpyJiTq6K8v0NACocoQJ4AQC/P3V57GXtKLrLB2xw9W4bKcCZ/fVpvmzTE8DgCpFmABOJLH/bSocvEgZPnEKUpFafzdOmSn3qqyowPQ0AKgShAngZBo3aa7WyalKrTNY5bal2MNfav+kRB3esc70NACodIQJ4IQC/P3U99EpWtH9dR22q6lR2W6FvNVPm+a9YnoaAFQqwgRwYolJN+vUQ0u01qe9AlWs1iv+oMxpd6ns9EnT0wCgUhAmgJNr3KiJ2iQv0MK6j8hhW4o98rUOTkrQ4e1rTE8DgArnMmGSkpKi6OhoxcfHm54CVLkAfz8lDZuklT3e1CG7uho69ir07f7a9FWKZNum5wFAhbFs27X+Xy0/P1/h4eHKy8tTWFiY6TlAldubs0e5bw5Sx9Izr5hk1RygqIdmyTco3PAyADi/C33+dplXTACcEdmwkdqOn6+F9YepzPZS26PfKndygg5t/dH0NAC4bIQJ4IL8fX2VNORZ/dj7bR1UTTVw7FO1d69S9pcvctoB4NIIE8CFdek1UGUPL9Eq33j5q1RtVv9VG6beqtJTJ0xPA4BLQpgALq5hw0i1Gz9PCxqMUKntrZhjC3V4coIObf7B9DQAuGiECeAG/Hx91O+Rf2p133e1XxGq79iv6u9do+zPJ3PaAeBSCBPAjXTpcbXsIen6wa+L/FSmNmv/ro0v3qSSguOmpwHABSFMADfToH4DXZH8tRZEjlSJ7a3oE4t07PnOOpj9nelpAPC7CBPADfn5eqvfQ/9QRtL72qdaqlt+SDXnDlT2p89y2gHg1AgTwI1d2b2/rGHLtMI/Ub5yqM26p5U95XoVnzxqehoA/CrCBHBz9evWVcfkf2t+ozEqtn3UJi9dJ17oooMblpmeBgC/QJgAHsDXx1v9H3xS6wd8oL2qozrluar5wfXK/vhpTjsAnAphAniQ+MS+8nl0qb7z7y5fy6E2mc9q0wvXqij/iOlpACCJMAE8Tr06dRSf/Lm+bTJexbavWucvV/6ULjqQudj0NAAgTABP5OvjrQEP/Fnrr/5Iu1VPtcsPq9ZHN2njh3+XystNzwPgwQgTwIPFd+mlgOHLtCygl3ysckVvmKzNL1ytorxc09MAeCjCBPBwdWpFqMu4T/Rtsz+pyPZV1MkVKpjSRfvXpZqeBsADESYA5OPjrQH3/0EbrvlUu1RfEfZR1f7kVm2c+1dOOwCqFGEC4JyOnbsraMQypQf2OXPayX5RW57vr6ITB01PA+AhCBMAP1M7oqYSx36kb5v/n07bfmpVsEqFLyZo39r5pqcB8ACECYBf8PHx1oD7kpU98HPtUEPVtI+p7me3a+P7f5LKHabnAXBjhAmA8+oQn6iQx9O1OLCfvC1b0ZtStHVykk4f2296GgA3RZgA+E21a9ZU9+QP9W3L/6dTtr9aFq7R6akJyln9jelpANwQYQLgd3l7WRpwzxPafMMX2qZGqmGfUP0v7tKGd/8g21Fmeh4AN0KYALhgV3Toomqj0rUo+Gp5WbZitr6i7ZP76tSRvaanAXAThAmAixJRvbp6jn1P81s/pQI7QC1OZag4pav2rPzC9DQAboAwAXDRvLws9b/zce246SttsZqoup2nRl/fp6y3x8l2lJqeB8CFESYALllc+06KGJWutJCBkqS222dp+6Q+KsjdbXgZAFdFmAC4LDWqhavXmHe0MPoZnbQD1eL0ejle7qpdKz4zPQ2ACyJMAFw2Ly9LSbc/pt23fqPNVjOF66SazBukrNmjZJeVmJ4HwIUQJgAqTNvYK1TniSVKC73hzF/vmq0dk3op/9BOw8sAuArCBECFqhYWpt5j3lRq7ESdtAPVvGiD7OndtHP5R6anAXABhAmACmdZlvreMkQ5t89XttVC4SpQ0wUPKfP1EbLLik3PA+DECBMAlaZNTJzqj1mi1PCbJUmxe97Wzok9lX9gm+FlAJwVYQKgUoWHhqjP6Ne1qP0LyrOD1aw4W5rRQ9uXvm96GgAnRJgAqHSWZan3jQ/q4F3ztcGrlcJUqOapQ5X56jCVl3LaAfAfhAmAKhPVuq0ajV2s1Oq3S5Jic97T7onddGLfFrPDADgNwgRAlQoNDlafkTO1pMNUHbdD1LRki3xm9dS2xe+angbACRAmAKqcZVnqef39OnLPQmV5tVaITqnF4se0fuYjKi85bXoeAIMIEwDGtGzVRk3HLVZqzbskSXH7P9Ceid10bG+24WUATCFMABgVHBSoPiOmKz3+ZR23Q9WkdJv8XuutLalvmp4GwADCBIBxlmWpx7X36Pj9qcr0jlaITqvV0pHKfGWwHMWnTM8DUIUIEwBOo1nzKDVPXqTUiPtUbluKPfiJciYm6ujuLNPTAFQRwgSAUwkKCFDfEdP0XcIMHbXD1LhspwLf6KvN818zPQ1AFSBMADilblfdofwHFmmdT6yCVKSo78Zo/cv3q6yowPQ0AJWIMAHgtJo2baGo5DSl1h6scttSXO7n2j+pqw7vWG96GoBKQpgAcGoB/n7q+9gUrej2mo7Y4WpUtkshbyUpe94M09MAVALCBIBLSOx3i049uERrfdopUMVqs2K81k+7W6WnT5qeBqACESYAXEajxk3VJnmhUus9LIdtKe7IVzo4KVGHtq81PQ1ABSFMALiUAH8/9R06Wat6zlauXV2Rjj0Ke7u/Nv47RbJt0/MAXCbCBIBL6tLnRpU8nK41vh0UqBJF//gnZU69UyWn8k1PA3AZCBMALqthZCPFjJ+vtPpD5bAtxR6bp9zJCTq45UfT0wBcIsIEgEvz9/VVnyHPaXXvd3RINdTQkaNqc65S1hcvctoBXBBhAsAtXNlroBxD0rXar5MCVKq2a/6qzJduVXHhcdPTAFwEwgSA26hfP1Jx479VWuRwldleij2+UEcnJ2r/phWmpwG4QIQJALfi6+OjPg89rYx+c3RANVW/fL9qvjdQmZ9N5rQDuADCBIBb6tTtalnDlmqVX2f5W6WKzfi7MqfcpKKTnHYAZ0aYAHBbdes20BXjv1Fa41Eqtb0Vm7dIx1/oor0bvjM9DcB5ECYA3JqPj7f6DP67MgfM1X7VUr3yg6r7wUCt/+hfnHYAJ0SYAPAIHRL7yeexZVrpnyhfy6G4rGeU9fxAnc47YnoagP9CmADwGLVr11XH8V9pUdNxKrZ91PbkMuVP6aI96xabngbgLMIEgEfx9vZS70F/0aZrPtJe1VUd+7Dqf3KT1r///2SXO0zPAzweYQLAI7Xr3FuBI5br+6Be8rHKFbfpBW2aPECFxw6YngZ4NJcJk5SUFEVHRys+Pt70FABuIiIiQp3HfqpFrf5PRbav2hSu0umpCdr54zzT0wCPZdm2a70tPT8/X+Hh4crLy1NYWJjpOQDcRNba7xX8xcNqaufIYVvKajFMcXc/Jcvbx/Q0wC1c6PO3y7xiAgCVqe0VCao2cpmWhQyQt2Wr3fbp2jqpr/Jz95ieBngUwgQAzqpevbq6jp2rxdFPqdD2V6vTGXK83FU7vv/c9DTAYxAmAPBfLMtSr9sf155bv9FWq4mqK1/Nvr1f694YJbusxPQ8wO0RJgDwK9rEdlTtMcu0JPwGSVK73bO1fWJP5e3fbngZ4N4IEwA4j/DQUPUY/aaWtJukk3agWhRvlDWzh7alv296GuC2CBMA+A2WZannTY/owF3zle3VUmEqUIu0oVo3a5jKS4pMzwPcDmECABegVes4RY5L16Lqt0uS2u17T7sndtWxvdmGlwHuhTABgAsUEhSkXiNnammnaTphh6hp6Tb5vdZbm1Nnm54GuA3CBAAugmVZ6j7wPh27P02Z3tEK0WlFLR2l9dMfkKP4lOl5gMsjTADgEjRrHqXmyYuUWus+lduW4g59qpyJCTqyc73paYBLI0wA4BIFBQSo7/Bp+i5xpo7Y4WpctkvBbyZp07wZpqcBLoswAYDL1G3A7SoYvFhrfdopUMVqvWK8sqbdqdLT+aanAS6HMAGACtCkSTO1SV6ohXWHyGFbanvkG+VOStChrT+anga4FMIEACpIgL+fkoZN1Mqeb+mQXUMNHDmq9u5Vyv5iiuRa38gdMIYwAYAKltDnepU+kq5VvvHyV6narHlSG166WSUFx01PA5weYQIAlaBhw0i1Gz9PCxo+rlLbWzHH03T0+S46uPE709MAp0aYAEAl8fP1Ub+Hn9KapPe1T7VUr/ygas4dqI0fP8NpBzgPwgQAKlnn7v1lDVumFf5d5Ws5FJ35L2W/cK2K8g6bngY4HcIEAKpA/bp11TH5S81vkqxi20dt8pfr5JQu2rd+kelpgFMhTACgivj6eKv/A/+nzGs+0W7VUy37iOp8fLM2zH1SKi83PQ9wCoQJAFSxTp17KnDEMi0N7C0fq1wx2VO0eXJ/nTq23/Q0wDjCBAAMqB0RocRxn2h+y7/otO2nqMJVOj01UXtXzzM9DTCKMAEAQ7y9vdT/nnHadP3n2q5I1bSPq8EXd2rDO+NlO0pNzwOMIEwAwLArOiYqfNRSLQq+Wl6WrZhtM7R9Ul8VHtljehpQ5QgTAHACEdWrq+fY97SgzVMqsAPU4vQ6lU7rql3ff2p6GlClCBMAcBJeXpb63fG4dt7ytTZbTVVN+Wry7QPKmj1KdlmJ6XlAlSBMAMDJxMZ1VO3RS5UadqMkqe2u2do5safyD243OwyoAoQJADih6uGh6vPEbC2MnaQ8O1jNijfKeqW7di593/Q0oFIRJgDgpCzLUtItj2jfHd9qg1crhapQTVOHasOrQ2SXnjY9D6gUhAkAOLno6Fg1HLNEC6rfIUmKyZmrPRO7KS8n2/AyoOIRJgDgAsJDgpQ0coZSO6bomB2qxiXb5PNqL21Pm216GlChCBMAcBGWZanvdffq8L2pWucVo2AVqXn6KG14ZZAcxYWm5wEVgjABABcT1TJKzZPT9G3EIJXblmIOfqb9ExN0dGeG6WnAZSNMAMAFhQQGqP/wF5We8KoO29UUWbZbwW/205Z5L0u2bXoecMkIEwBwUZZlqddVt6pg8CL96HOFAlSiViv+qI0pd6jsVJ7pecAlIUwAwMU1bdJMbccv0Ly6Q1Vmeyn6yLfKnZyg3C0rTU8DLhphAgBuIMDPV1cNe04/9HpbB+yaqu/Yp2pzrlb255M57cClECYA4Ea69h4ox5Cl+sGvs/xUpjZr/66NL96o4oJjpqcBF4QwAQA307BBA12R/I2+jRytEttb0ScW68TznXUgK930NOB3ESYA4Ib8fL014KG/aW2/D7RXdVSnPFcRH96oDR89JZWXm54HnBdhAgBurHO3JPk8ulTfBfSQr+VQTNZEbXr+ap0+fsj0NOBXESYA4Obq1amjK8d9pvnN/qgi21etC1ao8KUuylk73/Q04BcIEwDwAD4+3up//wRtvPZz7VQDRdjHVO+z25U150+yHWWm5wHnECYA4EE6XNlVISOXKT2ov7wtW223pGjr5CQVHs0xPQ2QRJgAgMepVaOGuo37QAtb/12Ftr9anVqrkqmJ2rXiC9PTAMIEADyRl5elpDtHacfNX2mr1VjVlacm8+7T+tlPyC4rMT0PHowwAQAPFtsuXrWeWKa00OslSXG7Xtf2ib2Uf3Cn4WXwVIQJAHi4amFh6j3mLaXFTdRJO1AtijdIr3TTtvS5pqfBAxEmAABZlqU+Nw/RvjvmK9urpcJUoBZpQ7R+1jCVlxSZngcPQpgAAM5pHR2nhmOXKLX67ZKkuH3vafekbjqWs8nwMngKwgQA8DOhwcHqM3Km0jtO0wk7RE1Ltsrv1V7avHC26WnwAIQJAOAXLMtSj+vu09H70rTeO1ohOq2oZaO0fvoDKisqND0PbowwAQCcV/MWUWqZvFiptQap3LYUd+hT7Z+UqMM71pueBjdFmAAAflNggL/6Dn9JK7rO0hE7XI3KdinkrSRt/Hq6ZNum58HNECYAgAuS2P82FT6UrrW+7RWoYkWvnKD10+5Uyal809PgRggTAMAFa9yoiaLHL1Rq/aEqs70Ud3SeDk/uooObV5qeBjdBmAAALoq/r6/6DnlOq/u8rYOqqQaOfar+3jXK+ux5Tju4bIQJAOCSdO45UPbQpfrR70r5q1RtM/6mrCk3qujkMdPT4MIIEwDAJatXr4HajZ+ntMajVWJ7q23eYp14obP2Zi41PQ0uijABAFwWXx9v9Rn8N2Vd9aFyVFt1y3NV96MbtP6Dpzjt4KIRJgCACtEhoa/8hy/XisDu8rUcits4URsmX63C44dMT4MLIUwAABWmVq3aih/3hdJa/FHFtq9iCr7XqZcStGvNAtPT4CIIEwBAhfL29lKfeydoy3WfaZcaqJZ9VJGf36aMd/8s21Fmeh6cHGECAKgUsZ26KWzUMi0P7idvy1b7rdO0eVI/5R/OMT0NTowwAQBUmhrVayhx3IdaEv03nbL91fr0GpWlJGrbii9NT4OTIkwAAJXKsiz1vH209tz6tbZbjVVDeWr2zX1a+8YYlZeVmp4HJ0OYAACqROvYTqo1ZpmWhl8vL8vWFbtf07aJvXTiwE7T0+BECBMAQJUJCw1Tt9FvaWn7iTppB6pVcZasGd20eclc09PgJAgTAECVsixL3W8cotx7FmiTVwuFq0BRi4Zo7YwhcpQUmZ4HwwgTAIARzVvFqlHyUi2peYck6YoDc7XnuUQd3rXB8DKYRJgAAIwJCgxSz8dn6vsu03XMDlXTsu0Knt1bG7+ZYXoaDCFMAADGJVx1twoGL9Z6n1gFqVjRP4zXuql3qeRUvulpqGKECQDAKTRq0kJR49O0qP4jctiW2h39WrmTu2j/ph9MT0MVIkwAAE7D389PvYdM0po+7+igaqqhY58i3rtG6z+ZyHcq9hCECQDA6cT3HCgNW6pV/l3kZ5Upbv1T2vD8QJ3KO2x6GioZYQIAcEp16zbQFclfK63pWBXbPoo5uUwFU7po99qFpqehEhEmAACn5ePjrT6D/qpN136sPaqn2vYRNfzsVq3jOxW7LcIEAOD02l3ZS8Ejl2t5cJK8LVvttk7TlklJys/dY3oaKhhhAgBwCTVr1FTC2I+0uM3fVWj7K+r0Wjle7qrtyz8xPQ0ViDABALgMLy9Lve4YpT23fqOtVhNVV76aLxisda+NUHlpsel5qACECQDA5bSJ7ag6Y5dpcfhNkqR2e9/WronddSxns+FluFyECQDAJYWFhKrn6DeU3vFFnbCD1axks/xe7aXNC2ebnobLQJgAAFyWZVnqcd0DOnpvmrK82yhEpxS1bJTWv3y/yooKTM/DJSBMAAAur3nL1mqevESptQap3LYUl/u5DkxKUO62Naan4SIRJgAAtxAY4K++w1/Siu6v67BdTZFlexT2Tn9t+PJFPs7ehRAmAAC3kph0s4oeTtdq344KUKliVv9VG168WcUFx0xPwwUgTAAAbicysrFix8/XwoYjVGp7K+ZEmo4/30X7s5aanobfQZgAANySn6+Pkh7+pzL6v68c1Vbd8kOq9eENyvrgb1J5uel5OA/CBADg1uK79pfvY8v1XUAP+VoOtd34vDZNHqDCY/tNT8OvIEwAAG6vTu3a6pz8uRa2+D+dtv3UunCliqYmavfKr0xPw/8gTAAAHsHb20tJ9yZryw1faLsVqZr2cUV+dY8y3xoru6zE9DycRZgAADxKuw4Jqj5qmRaHXCsvy1bsjle1Y1Iv5R/YYXoaRJgAADxQjWrV1HPsu0qNfU4n7UA1L9ogzeim7UvmmJ7m8QgTAIBHsixLfW8Zqpw752ujV0uFqVDNFz2qzJkPq7zktOl5HoswAQB4tDZt4hQ5Nl0La9wlSYrd/6FyJibq2K4sw8s8E2ECAPB4ocFB6vv4dC25coaO2mFqVLpDgbP7aPO8V/g4+ypGmAAAoDOnnZ7X3Km8QYu01rudAlWsqBV/0IaUO1V6Ks/0PI9BmAAA8F+aNWuhNuMXan7dISqzvRRzZJ4OT+6iQ5u+Nz3NIxAmAAD8jwB/P/UfNlEre72j/YpQfcd+1Xj/WmV/+i9OO5WMMAEA4DwSe18re0i6vvdLlK8carPuGW164VoV5eWanua2CBMAAH5Dg/oN1Gn8v/Vtk3Eqtn3VOn+5CqZ0Vs7aBaanuaUqD5O9e/eqV69eio6OVlxcnD788MOqngAAwEXx9fHWgAf+ovVXf6xdqq8I+5jqfXabNrw7QbajzPQ8t2LZdtUeyw4cOKBDhw6pffv2OnjwoDp27KgtW7YoODj4gv73+fn5Cg8PV15ensLCwip5LQAAP5d79KiyXxuqnqfOvGKyLaid6g1+W8G1Ghte5twu9Pm7yl8xqVevntq3by9Jqlu3riIiInTs2LGqngEAwCWpXbOmuo37UPOj/q4CO0AtTq1TWUpX7f7uY9PT3MJFh0l6erquu+461a9fX5Zl6bPPPvvF35OSkqImTZooICBAnTt31sqVK3/111q9erUcDociIyMvejgAAKZ4e1nqf9co7bjla22ymilcJ9V4/oPa8NqjskuLTM9zaRcdJoWFhWrXrp1SUlJ+9efnzp2rMWPG6Mknn9SaNWvUrl07DRgwQLm5P38H87Fjx3T//fdr5syZl7YcAADD4uI6qu4T6VoQdoskKWbvHO2Z2FUncrINL3Ndl/UeE8uy9Omnn+rGG28897XOnTsrPj5e06ZNkySVl5crMjJSjz/+uCZMmCBJKi4uVr9+/fTII4/ovvvu+83fo7i4WMXFxef+Oj8/X5GRkbzHBADgNGzbVtoXb6nDmj+runVShQrQ/q7/VMt+D5ue5jSMvMekpKREq1evVlJS0n9+Ay8vJSUl6fvvz3xinm3beuCBB9SnT5/fjRJJeuaZZxQeHn7uB2cfAICzsSxLfW8YpMP3pirDu62CVaSWy8dqQ8pdKjudb3qeS6nQMDly5IgcDofq1Knzs6/XqVNHBw8elCQtX75cc+fO1Weffab27durffv2yszMPO+v+cc//lF5eXnnfuzdu7ciJwMAUGFatYxSq+Q0za/9oBy2pZjDXyt3Uhflbvn191ril3yq+jfs1q2bysvLL/jv9/f3l7+/fyUuAgCg4gQF+Kv/Yy9o2cJearH0CdV37FPJnKuV3X682twwXrIs0xOdWoW+YhIRESFvb28dOnToZ18/dOiQ6tatW5G/FQAATq1b0g0qfWSJVvh1kZ/K1CbjaT7O/gJUaJj4+fmpY8eOSk1NPfe18vJypaamKiEhoSJ/KwAAnF5kw0h1SP5a8xuPVbHto9b5y3VyShflrJ1veprTuugwKSgoUEZGhjIyMiRJO3fuVEZGhvbs2SNJGjNmjGbNmqU333xT2dnZevTRR1VYWKjBgwdX6HAAAFyBn6+3+g/+qzKv+US7VF+17KOq99ntynp3gmxHqel5Tuei/7jw4sWL1bt37198fdCgQZo9e7Ykadq0aZo4caIOHjyo9u3b66WXXlLnzp0rZDAfSQ8AcFWHjx5V9mvD1OPUmVdMtgbGqe7gdxRa2/0/zv5Cn7+r/HvlXC7CBADgysrLbaV+OFUJG/+pEKtIeQrR0aQpatbtNtPTKpXTfq8cAAA8mZeXpX53jNTuW7/RJqu5wlWgZgsfVuasoSovOW16nnGECQAABsTEdlC9sUuVWu3MKyWx+97XnolddWz3BsPLzCJMAAAwJDwkWH1GzdKSTik6ZoeqSel2BbzRR5vnzZBc650WFYYwAQDAIMuy1HPgvToxaJEyvOMUpCJFrRivrGl3qvRUnul5VY4wAQDACTRr1lKtx6dqQd2H5bAttT06T4cnd9GB7O9NT6tShAkAAE4iwN9P/YZN1qpe7+iAaqq+Y78i3r9WGz5+2mNOO4QJAABOpkvvgSofulQ/+CfI13IoJvNZZT9/tU4dP2h6WqUjTAAAcEIN6jVQx+SvtLDpeBXbvmpz8nudeilBu1d/a3papXKZMElJSVF0dLTi4+NNTwEAoEr4+HgradCflT3wU+1SfUXYxxT5xR1a/1ay236cPZ/8CgCACzh67Jg2vPaoehTOkyRtDWir2g+8rfC6zQwvuzB88isAAG6kZo0a6j7ufaXFPK0CO1Ati7JkvdJdW5e8b3pahSJMAABwEZZlqc9tw7Xvjm+1yauFwlSglouGav2Mh+Vwk4+zJ0wAAHAxUdHt1HDcUi2qcYckKe7Ah8p5LkGHd2YaXnb5CBMAAFxQSFCQeo+cqWVXvqKjdpgal+1UyJt9tfGrl136M08IEwAAXFi3a+5SweDFyvCJU6CKFb3qj8p86TYVFx43Pe2SECYAALi4xk2aq834VKXVH6oy20uxxxfo6OQE5WxYbnraRSNMAABwA/5+fuoz5DllJM3RAUWofvkB1fngOq3/4CmpvNz0vAtGmAAA4EY6db9aXo8u1w8BXeVrORS3caI2Tr5KhccOmJ52QQgTAADcTJ06ddUp+d9a1GKCim1fRRf+oNNTE7Rj5demp/0uwgQAADfk7e2l3vf+UVtv/EI7rYaKsI+ryVd3K2P2GKf+OHvCBAAAN9b2ikRVH7VMS0OvkZdlq/2u17TtuR46vm+b6Wm/ijABAMDNVatWXd3GzFF6u+d00g5Uy+KN8p7VQ5sWvWt62i8QJgAAeADLstTjpqE6dPcCZXu1UpgK1XrJY8qY/qDKigpNzzuHMAEAwIO0iIpV4+QlWhJxlySp/aGPtW9Sog5uzzA77CzCBAAADxMUGKSeI17RisRZOmqHq3HZLoW/3U9ZX75k/OPsCRMAADxUl/63q+jhdGX4tlegStR29V+0/sVbVHTS3MfZEyYAAHiwBpFNFPOHVC2OfExltpfiTqRq7bdvGtvjMmGSkpKi6OhoxcfHm54CAIBb8fXxUa+HnlHWgPeVXu0mdb5ppLEtlm271vdGzs/PV3h4uPLy8hQWFmZ6DgAAuAAX+vztMq+YAAAA90eYAAAAp0GYAAAAp0GYAAAAp0GYAAAAp0GYAAAAp0GYAAAAp0GYAAAAp0GYAAAAp0GYAAAAp0GYAAAAp0GYAAAAp0GYAAAAp+FjesDF+umbIefn5xteAgAALtRPz9s/PY+fj8uFycmTJyVJkZGRhpcAAICLdfLkSYWHh5/35y3799LFyZSXl2v//v0KDQ2VZVkV9uvm5+crMjJSe/fuVVhYWIX9urg0PB7Oh8fEufB4OBcej99n27ZOnjyp+vXry8vr/O8kcblXTLy8vNSwYcNK+/XDwsL4l8qJ8Hg4Hx4T58Lj4Vx4PH7bb71S8hPe/AoAAJwGYQIAAJwGYXKWv7+/nnzySfn7+5ueAvF4OCMeE+fC4+FceDwqjsu9+RUAALgvXjEBAABOgzABAABOgzABAABOgzABAABOgzA5KyUlRU2aNFFAQIA6d+6slStXmp7kkZ555hnFx8crNDRUtWvX1o033qjNmzebnoWz/vWvf8myLI0ePdr0FI+1b98+3XvvvapZs6YCAwMVGxurH3/80fQsj+VwOPSXv/xFTZs2VWBgoJo3b65//OMfv/v9YHB+hImkuXPnasyYMXryySe1Zs0atWvXTgMGDFBubq7paR5nyZIlGj58uFasWKEFCxaotLRU/fv3V2FhoelpHm/VqlWaMWOG4uLiTE/xWMePH1fXrl3l6+urb775Rhs3btTkyZNVvXp109M81rPPPqvp06dr2rRpys7O1rPPPqvnnntOU6dONT3NZfHHhSV17txZ8fHxmjZtmqQz348nMjJSjz/+uCZMmGB4nWc7fPiwateurSVLlqhHjx6m53isgoICdejQQS+//LKeeuoptW/fXlOmTDE9y+NMmDBBy5cv19KlS01PwVkDBw5UnTp19Nprr5372i233KLAwEC98847Bpe5Lo9/xaSkpESrV69WUlLSua95eXkpKSlJ33//vcFlkKS8vDxJUo0aNQwv8WzDhw/Xtdde+7P/TlD1vvjiC3Xq1Em33XabateurSuuuEKzZs0yPcujJSYmKjU1VVu2bJEkrVu3TsuWLdPVV19teJnrcrlv4lfRjhw5IofDoTp16vzs63Xq1NGmTZsMrYJ05pWr0aNHq2vXrmrbtq3pOR7r/fff15o1a7Rq1SrTUzzejh07NH36dI0ZM0Z/+tOftGrVKo0cOVJ+fn4aNGiQ6XkeacKECcrPz1fr1q3l7e0th8Ohf/7zn7rnnntMT3NZHh8mcF7Dhw9XVlaWli1bZnqKx9q7d69GjRqlBQsWKCAgwPQcj1deXq5OnTrp6aefliRdccUVysrK0iuvvEKYGPLBBx/o3Xff1Zw5cxQTE6OMjAyNHj1a9evX5zG5RB4fJhEREfL29tahQ4d+9vVDhw6pbt26hlZhxIgR+ve//6309HQ1bNjQ9ByPtXr1auXm5qpDhw7nvuZwOJSenq5p06apuLhY3t7eBhd6lnr16ik6OvpnX2vTpo0+/vhjQ4uQnJysCRMm6M4775QkxcbGavfu3XrmmWcIk0vk8e8x8fPzU8eOHZWamnrua+Xl5UpNTVVCQoLBZZ7Jtm2NGDFCn376qdLS0tS0aVPTkzxa3759lZmZqYyMjHM/OnXqpHvuuUcZGRlESRXr2rXrL/74/JYtW9S4cWNDi3Dq1Cl5ef38qdTb21vl5eWGFrk+j3/FRJLGjBmjQYMGqVOnTrryyis1ZcoUFRYWavDgwaaneZzhw4drzpw5+vzzzxUaGqqDBw9KksLDwxUYGGh4necJDQ39xft7goODVbNmTd73Y8ATTzyhxMREPf3007r99tu1cuVKzZw5UzNnzjQ9zWNdd911+uc//6lGjRopJiZGa9eu1fPPP68HH3zQ9DTXZcO2bdueOnWq3ahRI9vPz8++8sor7RUrVpie5JEk/eqPN954w/Q0nNWzZ0971KhRpmd4rC+//NJu27at7e/vb7du3dqeOXOm6UkeLT8/3x41apTdqFEjOyAgwG7WrJn95z//2S4uLjY9zWXxOSYAAMBpePx7TAAAgPMgTAAAgNMgTAAAgNMgTAAAgNMgTAAAgNMgTAAAgNMgTAAAgNMgTAAAgNMgTAAAgNMgTAAAgNMgTAAAgNMgTAAAgNP4/xCQsU3+s1eRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(0)\n",
    "plt.semilogy(loss_mse_gd)\n",
    "plt.semilogy(loss_test_set)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.08906857914893185\n",
      "F1 score:  0.16356835713419582\n"
     ]
    }
   ],
   "source": [
    "y_pred = tX_train_test.dot(w_mse_gd[-1])\n",
    "y_pred = np.where(y_pred > 0, 1, -1)\n",
    "\n",
    "_,_,_,_,f1 = f.confusion_matrix(y_train_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", np.sum(y_pred == y_train_test)/len(y_train_test))\n",
    "print(\"F1 score: \", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#h.create_csv_submission(test_ids, y_test_rounded, 'submission_gd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      "\n",
      " [ 9.20167666e-02 -7.71490238e-02  5.86797876e-03 -8.43936422e-04\n",
      "  3.83395852e-01  1.22940270e-01 -1.21100642e-01 -1.21896949e-01\n",
      " -1.67716021e-01 -8.54598525e-02 -9.62527225e-02 -2.57862282e-02\n",
      " -7.42547553e-02 -5.06910131e-02 -1.16309687e-01  2.50926875e-01\n",
      " -1.51153836e-04  7.01061785e-01  1.05783835e-02 -3.64603987e-02\n",
      "  7.86555181e-02  6.24983197e-02] \n",
      "\n",
      " Loss =  0.16322289498404216 \n",
      "\n",
      "*****************************************************************************  \n",
      "\n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.008813391967379479\n"
     ]
    }
   ],
   "source": [
    "#Test the model on the test sample. Do we need to standardize ?\n",
    "\n",
    "y_test = tX_test.dot(w_mse_gd[-1])\n",
    "y_test_rounded = np.where(y_test > 0, 1, -1) #not sure about this line\n",
    "\n",
    "print('weights = \\n\\n', w_mse_gd[-1],'\\n\\n Loss = ', loss_mse_gd[-1],'\\n\\n*****************************************************************************',\n",
    "      ' \\n\\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train == 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_rounded == 1)/len(y_test_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run some cross validation to see the best initial weights (as a function of the proportion of 1, -1 and 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. MSE SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 1999/1999: loss=0.5308156725767825, w0=0.6727209105777945, w1=0.19167095489707042\n"
     ]
    }
   ],
   "source": [
    "w_mse_sgd, loss_mse_sgd = f.mean_squared_error_sgd(y_train_train, tX_train_train, initial_w, 2000, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLossMSE(weights, loss, y, x ):\n",
    "    loss_test_set = []\n",
    "\n",
    "    for w in weights:\n",
    "        loss_test_set.append(f.compute_mse(y, x, w))\n",
    "\n",
    "    plt.figure(0)\n",
    "    plt.semilogy(loss)\n",
    "    plt.semilogy(loss_test_set)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 22)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_mse_gd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotLossMSE(w_mse_sgd, loss_mse_sgd, y_train_test, tX_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5536311089891407\n",
      "F1 score:  0.08754698174720187\n"
     ]
    }
   ],
   "source": [
    "y_pred = tX_train_test.dot(w_mse_sgd[-1])\n",
    "y_pred = np.where(y_pred > 0, 1, -1)\n",
    "\n",
    "_,_,_,_,f1 = f.confusion_matrix(y_train_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", np.sum(y_pred == y_train_test)/len(y_train_test))\n",
    "\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [ 0.67272091  0.19167095  0.01685251 -0.06640884  0.93035369  0.71186752\n",
      "  0.72274045  0.66111226  0.37007749  0.77780544 -0.50236852 -0.54662024\n",
      "  0.69056636  0.80070849  0.70249968  0.77342045 -0.31979801  0.9077714\n",
      "  0.69292927  0.79724999  0.87622862  0.88066828] \n",
      " Loss =  0.5308156725767825 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.40006765466862926\n"
     ]
    }
   ],
   "source": [
    "y_test_sgd = tX_test.dot(w_mse_sgd[-1])\n",
    "y_test_rounded_sgd = np.where(y_test_sgd > 0, 1, -1)\n",
    "\n",
    "print('weights = \\n', w_mse_sgd[-1],'\\n Loss = ', loss_mse_sgd[-1],'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train == 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_rounded_sgd == 1)/len(y_test_rounded_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls, loss_ls = f.least_squares(y_train_train, tX_train_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9117034568929613\n",
      "F1 score:  0.03529411764705882\n"
     ]
    }
   ],
   "source": [
    "y_pred = tX_train_test.dot(w_ls)\n",
    "y_pred = np.where(y_pred > 0, 1, -1)\n",
    "\n",
    "_,_,_,_,f1 = f.confusion_matrix(y_train_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", np.sum(y_pred == y_train_test)/len(y_train_test))\n",
    "\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [-3.54515184e-01  5.76100349e-02  2.15645760e-03 -1.88754185e-04\n",
      " -3.22325550e-02 -9.15316093e-03 -9.19649704e-02 -3.69989496e-01\n",
      " -4.77401102e-02 -9.79382213e-02 -2.50199111e-03 -5.79831694e-03\n",
      " -1.02170381e-01 -4.18711437e-02 -7.19620069e-02  3.78987972e-02\n",
      "  1.72309041e-02 -2.14401256e-01  2.99498822e-02 -5.64090919e-03\n",
      " -5.15795614e-03 -9.86993155e-03] \n",
      " Loss =  0.1368442031431572 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.0025873339489298677\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_test_ls = tX_test.dot(w_ls)\n",
    "y_test_ls = np.where(y_test_ls > 0, 1, -1)\n",
    "\n",
    "print('weights = \\n', w_ls,'\\n Loss = ', loss_ls,'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train == 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_ls == 1)/len(y_test_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ridge, loss_ridge = f.ridge_regression(y_train, tX_train, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.zeros(max_iters)\n",
    "    weights = np.zeros((max_iters, tx.shape[1]))\n",
    "\n",
    "for l in np.arange(1*10**-6, 1, 7):\n",
    "    w, l = f.ridge_regression(y_train, tX_train, l)\n",
    "    weights.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [-0.16807427  0.03917705  0.00263183 -0.00060332 -0.04608669 -0.03046109\n",
      " -0.09241493 -0.30090331 -0.06105668 -0.09886347 -0.01771055 -0.00968188\n",
      " -0.11182754 -0.0440128  -0.08040227  0.01886485  0.01556599 -0.07420305\n",
      " -0.02540871 -0.01530144 -0.01029129 -0.01434477] \n",
      " Loss =  0.13792607978063323 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.001307380758646541\n"
     ]
    }
   ],
   "source": [
    "y_test_ridge = tX_test.dot(w_ridge)\n",
    "y_test_ridge = np.where(y_test_ridge > 0, 1, -1)\n",
    "\n",
    "print('weights = \\n', w_ridge,'\\n Loss = ', loss_ridge,'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train == 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_ridge == 1)/len(y_test_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_processed_logreg = np.where(y_train == 1, 1, 0)\n",
    "y_train_train_lg = np.where(y_train == 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=22.228446953619866, w0=0.9088319282565185, w1=0.774423255619819\n",
      "Gradient Descent(1/499): loss=9.611718774381371, w0=0.8176679123012238, w1=0.5488481612199283\n",
      "Gradient Descent(2/499): loss=1.28687327883213, w0=0.7266558084004003, w1=0.323610439031915\n",
      "Gradient Descent(3/499): loss=1.0905442714743465, w0=0.7117236550240043, w1=0.292093444421554\n",
      "Gradient Descent(4/499): loss=1.0209680298350716, w0=0.7050843262580097, w1=0.28412996682435854\n",
      "Gradient Descent(5/499): loss=0.9651266706147152, w0=0.6994541056665075, w1=0.2781157575578648\n",
      "Gradient Descent(6/499): loss=0.9196805520798447, w0=0.6942399818510507, w1=0.27240817064918965\n",
      "Gradient Descent(7/499): loss=0.8819683110273338, w0=0.6892627378443773, w1=0.2665683889722687\n",
      "Gradient Descent(8/499): loss=0.8496157601200154, w0=0.6845022601659031, w1=0.2606701030212248\n",
      "Gradient Descent(9/499): loss=0.8209881234992175, w0=0.6799539292963054, w1=0.2548381813401633\n",
      "Gradient Descent(10/499): loss=0.7950910548661506, w0=0.6755910806580318, w1=0.24910654626740458\n",
      "Gradient Descent(11/499): loss=0.7713430891562554, w0=0.6713768968233914, w1=0.24345089804999467\n",
      "Gradient Descent(12/499): loss=0.7494052907433914, w0=0.667275676458805, w1=0.2378284051560484\n",
      "Gradient Descent(13/499): loss=0.7290747586183076, w0=0.6632570227636764, w1=0.23219599886334866\n",
      "Gradient Descent(14/499): loss=0.7102213222350677, w0=0.6592968271479897, w1=0.22651761649439722\n",
      "Gradient Descent(15/499): loss=0.6927506184594613, w0=0.6553771179985042, w1=0.22076683953867748\n",
      "Gradient Descent(16/499): loss=0.6765831069969753, w0=0.6514854861294337, w1=0.21492740409236155\n",
      "Gradient Descent(17/499): loss=0.6616429078039279, w0=0.6476143318625053, w1=0.20899258882545646\n",
      "Gradient Descent(18/499): loss=0.6478527692700583, w0=0.6437600285067161, w1=0.2029639511183989\n",
      "Gradient Descent(19/499): loss=0.6351327712296176, w0=0.6399220563446495, w1=0.19684969352311024\n",
      "Gradient Descent(20/499): loss=0.623401109135539, w0=0.6361021561238983, w1=0.19066288103780282\n",
      "Gradient Descent(21/499): loss=0.6125758110555022, w0=0.6323035502087309, w1=0.1844196974445413\n",
      "Gradient Descent(22/499): loss=0.6025766374861247, w0=0.6285302716389773, w1=0.1781378882004095\n",
      "Gradient Descent(23/499): loss=0.5933267354352844, w0=0.6247866260729835, w1=0.1718354836639065\n",
      "Gradient Descent(24/499): loss=0.584753858909778, w0=0.621076793575684, w1=0.1655298402579641\n",
      "Gradient Descent(25/499): loss=0.5767911261737454, w0=0.617404561431678, w1=0.1592369903321275\n",
      "Gradient Descent(26/499): loss=0.5693773707776038, w0=0.6137731685654546, w1=0.15297126042816037\n",
      "Gradient Descent(27/499): loss=0.5624571777588648, w0=0.6101852372940005, w1=0.1467451027331751\n",
      "Gradient Descent(28/499): loss=0.5559806989180265, w0=0.606642767963793, w1=0.14056908226759696\n",
      "Gradient Descent(29/499): loss=0.5499033279018578, w0=0.603147174855594, w1=0.13445196813115975\n",
      "Gradient Descent(30/499): loss=0.5441852975661645, w0=0.5996993459864705, w1=0.12840088673855812\n",
      "Gradient Descent(31/499): loss=0.5387912444418679, w0=0.5962997139293137, w1=0.1224215054303093\n",
      "Gradient Descent(32/499): loss=0.5336897704747617, w0=0.5929483288088864, w1=0.11651822438466317\n",
      "Gradient Descent(33/499): loss=0.5288530210616451, w0=0.5896449279073259, w1=0.11069436258260851\n",
      "Gradient Descent(34/499): loss=0.5242562904101007, w0=0.5863889987639495, w1=0.10495232951528843\n",
      "Gradient Descent(35/499): loss=0.5198776597678892, w0=0.5831798343677659, w1=0.09929377853986376\n",
      "Gradient Descent(36/499): loss=0.5156976704589129, w0=0.5800165801582919, w1=0.09371974060006183\n",
      "Gradient Descent(37/499): loss=0.5116990313874206, w0=0.5768982732212132, w1=0.08823073877157755\n",
      "Gradient Descent(38/499): loss=0.5078663593171249, w0=0.5738238744229252, w1=0.08282688507557441\n",
      "Gradient Descent(39/499): loss=0.5041859494952527, w0=0.5707922943780107, w1=0.07750796147270703\n",
      "Gradient Descent(40/499): loss=0.5006455738598053, w0=0.5678024141649946, w1=0.07227348709128667\n",
      "Gradient Descent(41/499): loss=0.4972343039936514, w0=0.564853101653446, w1=0.06712277368886199\n",
      "Gradient Descent(42/499): loss=0.49394235607110115, w0=0.5619432242164272, w1=0.06205497118643444\n",
      "Gradient Descent(43/499): loss=0.4907609552140502, w0=0.5590716584995662, w1=0.057069104906882134\n",
      "Gradient Descent(44/499): loss=0.4876822168915404, w0=0.5562372978153979, w1=0.05216410592956735\n",
      "Gradient Descent(45/499): loss=0.48469904323030183, w0=0.5534390576364647, w1=0.047338835761944455\n",
      "Gradient Descent(46/499): loss=0.4818050323369739, w0=0.5506758795764324, w1=0.04259210633674574\n",
      "Gradient Descent(47/499): loss=0.47899439895484, w0=0.5479467341761336, w1=0.037922696174300344\n",
      "Gradient Descent(48/499): loss=0.4762619049834334, w0=0.5452506227506501, w1=0.03332936340439237\n",
      "Gradient Descent(49/499): loss=0.47360279857571697, w0=0.5425865785032122, w1=0.028810856219445416\n",
      "Gradient Descent(50/499): loss=0.4710127606940892, w0=0.5399536670705061, w1=0.024365921228432508\n",
      "Gradient Descent(51/499): loss=0.46848785815379435, w0=0.5373509866305445, w1=0.019993310096114582\n",
      "Gradient Descent(52/499): loss=0.4660245023116055, w0=0.5347776676772916, w1=0.01569178478238693\n",
      "Gradient Descent(53/499): loss=0.4636194126705026, w0=0.5322328725445711, w1=0.011460121639242142\n",
      "Gradient Descent(54/499): loss=0.46126958476912944, w0=0.5297157947444595, w1=0.007297114576000042\n",
      "Gradient Descent(55/499): loss=0.45897226180976863, w0=0.5272256581715249, w1=0.003201577465172436\n",
      "Gradient Descent(56/499): loss=0.4567249095520164, w0=0.5247617162132419, w1=-0.0008276540699250542\n",
      "Gradient Descent(57/499): loss=0.4545251940627269, w0=0.5223232507981378, w1=-0.00479172137022095\n",
      "Gradient Descent(58/499): loss=0.45237096196745513, w0=0.5199095714062406, w1=-0.008691741863843966\n",
      "Gradient Descent(59/499): loss=0.45026022289572665, w0=0.5175200140608588, w1=-0.012528808519270816\n",
      "Gradient Descent(60/499): loss=0.44819113385306775, w0=0.5151539403163188, w1=-0.01630398956488536\n",
      "Gradient Descent(61/499): loss=0.4461619852877023, w0=0.5128107362528002, w1=-0.02001832842258254\n",
      "Gradient Descent(62/499): loss=0.4441711886499919, w0=0.5104898114866424, w1=-0.023672843812408324\n",
      "Gradient Descent(63/499): loss=0.44221726526871685, w0=0.5081905982023165, w1=-0.02726852999290911\n",
      "Gradient Descent(64/499): loss=0.4402988363907548, w0=0.5059125502105302, w1=-0.03080635710817744\n",
      "Gradient Descent(65/499): loss=0.4384146142501353, w0=0.5036551420355808, w1=-0.034287271617775075\n",
      "Gradient Descent(66/499): loss=0.436563394049233, w0=0.5014178680340078, w1=-0.03771219678999299\n",
      "Gradient Descent(67/499): loss=0.4347440467494069, w0=0.4992002415457682, w1=-0.04108203324243448\n",
      "Gradient Descent(68/499): loss=0.4329555125810003, w0=0.4970017940785166, w1=-0.04439765951681802\n",
      "Gradient Descent(69/499): loss=0.4311967951935532, w0=0.49482207452507626, w1=-0.0476599326772989\n",
      "Gradient Descent(70/499): loss=0.4294669563765971, w0=0.492660648413812, w1=-0.05086968892359414\n",
      "Gradient Descent(71/499): loss=0.42776511128967454, w0=0.49051709719133324, w1=-0.05402774421183526\n",
      "Gradient Descent(72/499): loss=0.4260904241474434, w0=0.48839101753674696, w1=-0.05713489487743024\n",
      "Gradient Descent(73/499): loss=0.424442104312018, w0=0.48628202070653215, w1=-0.06019191825533669\n",
      "Gradient Descent(74/499): loss=0.42281940275020513, w0=0.48418973190900305, w1=-0.06319957329407545\n",
      "Gradient Descent(75/499): loss=0.4212216088181064, w0=0.4821137897072632, w1=-0.06615860116057896\n",
      "Gradient Descent(76/499): loss=0.419648047339779, w0=0.48005384544951407, w1=-0.06906972583360156\n",
      "Gradient Descent(77/499): loss=0.4180980759503519, w0=0.4780095627255663, w1=-0.07193365468393989\n",
      "Gradient Descent(78/499): loss=0.4165710826772496, w0=0.47598061684840226, w1=-0.07475107904014155\n",
      "Gradient Descent(79/499): loss=0.4150664837360422, w0=0.473966694359654, w1=-0.07752267473873331\n",
      "Gradient Descent(80/499): loss=0.41358372151996425, w0=0.47196749255788273, w1=-0.08024910265829024\n",
      "Gradient Descent(81/499): loss=0.4121222627643811, w0=0.46998271904857813, w1=-0.0829310092369047\n",
      "Gradient Descent(82/499): loss=0.41068159686944244, w0=0.46801209131483107, w1=-0.08556902697280766\n",
      "Gradient Descent(83/499): loss=0.40926123436591544, w0=0.46605533630767215, w1=-0.0881637749080523\n",
      "Gradient Descent(84/499): loss=0.40786070551073567, w0=0.4641121900551105, w1=-0.09071585909529709\n",
      "Gradient Descent(85/499): loss=0.4064795590001822, w0=0.46218239728894867, w1=-0.09322587304782796\n",
      "Gradient Descent(86/499): loss=0.40511736078980665, w0=0.46026571108849323, w1=-0.09569439817304048\n",
      "Gradient Descent(87/499): loss=0.40377369301133287, w0=0.45836189254032195, w1=-0.09812200418966761\n",
      "Gradient Descent(88/499): loss=0.40244815297771025, w0=0.45647071041331116, w1=-0.10050924952908832\n",
      "Gradient Descent(89/499): loss=0.40114035226836314, w0=0.4545919408481666, w1=-0.10285668172109141\n",
      "Gradient Descent(90/499): loss=0.39984991588746266, w0=0.4527253670607412, w1=-0.1051648377644967\n",
      "Gradient Descent(91/499): loss=0.398576481488721, w0=0.4508707790584602, w1=-0.10743424448305748\n",
      "Gradient Descent(92/499): loss=0.3973196986608406, w0=0.4490279733692115, w1=-0.10966541886708121\n",
      "Gradient Descent(93/499): loss=0.3960792282682984, w0=0.4471967527820927, w1=-0.11185886840121531\n",
      "Gradient Descent(94/499): loss=0.394854741842643, w0=0.4453769260994391, w1=-0.11401509137884838\n",
      "Gradient Descent(95/499): loss=0.39364592101993506, w0=0.44356830789958984, w1=-0.11613457720357885\n",
      "Gradient Descent(96/499): loss=0.3924524570203566, w0=0.4417707183098775, w1=-0.11821780667820045\n",
      "Gradient Descent(97/499): loss=0.39127405016638767, w0=0.4399839827893546, w1=-0.12026525228165\n",
      "Gradient Descent(98/499): loss=0.39011040943626407, w0=0.4382079319207973, w1=-0.12227737843435665\n",
      "Gradient Descent(99/499): loss=0.38896125204973925, w0=0.4364424012115529, w1=-0.12425464175242433\n",
      "Gradient Descent(100/499): loss=0.3878263030834276, w0=0.43468723090281747, w1=-0.12619749129107016\n",
      "Gradient Descent(101/499): loss=0.38670529511325674, w0=0.4329422657869567, w1=-0.12810636877773238\n",
      "Gradient Descent(102/499): loss=0.3855979678817738, w0=0.4312073550325008, w1=-0.1299817088352509\n",
      "Gradient Descent(103/499): loss=0.3845040679882449, w0=0.4294823520164641, w1=-0.13182393919551308\n",
      "Gradient Descent(104/499): loss=0.3834233485996717, w0=0.4277671141636605, w1=-0.1336334809039462\n",
      "Gradient Descent(105/499): loss=0.38235556918101316, w0=0.42606150279269983, w1=-0.13541074851522703\n",
      "Gradient Descent(106/499): loss=0.3813004952430427, w0=0.4243653829683704, w1=-0.1371561502805678\n",
      "Gradient Descent(107/499): loss=0.3802578981064116, w0=0.4226786233601243, w1=-0.13887008832692627\n",
      "Gradient Descent(108/499): loss=0.37922755468061453, w0=0.4210010961063991, w1=-0.14055295882847668\n",
      "Gradient Descent(109/499): loss=0.37820924725665617, w0=0.41933267668452195, w1=-0.14220515217066698\n",
      "Gradient Descent(110/499): loss=0.3772027633123305, w0=0.41767324378595516, w1=-0.14382705310717714\n",
      "Gradient Descent(111/499): loss=0.37620789532910787, w0=0.4160226791966535, w1=-0.1454190409100818\n",
      "Gradient Descent(112/499): loss=0.37522444061971955, w0=0.41438086768231513, w1=-0.14698148951351114\n",
      "Gradient Descent(113/499): loss=0.3742522011655974, w0=0.4127476968783194, w1=-0.14851476765109176\n",
      "Gradient Descent(114/499): loss=0.37329098346340217, w0=0.41112305718415254, w1=-0.1500192389874409\n",
      "Gradient Descent(115/499): loss=0.3723405983799428, w0=0.40950684166213364, w1=-0.15149526224397614\n",
      "Gradient Descent(116/499): loss=0.3714008610148366, w0=0.4078989459402615, w1=-0.1529431913192936\n",
      "Gradient Descent(117/499): loss=0.3704715905703259, w0=0.4062992681190102, w1=-0.15436337540435824\n",
      "Gradient Descent(118/499): loss=0.3695526102277109, w0=0.40470770868191036, w1=-0.15575615909274032\n",
      "Gradient Descent(119/499): loss=0.3686437470299017, w0=0.40312417040976023, w1=-0.15712188248612352\n",
      "Gradient Descent(120/499): loss=0.3677448317696413, w0=0.4015485582983165, w1=-0.1584608812953013\n",
      "Gradient Descent(121/499): loss=0.36685569888297886, w0=0.3999807794793231, w1=-0.15977348693686982\n",
      "Gradient Descent(122/499): loss=0.36597618634761836, w0=0.39842074314474113, w1=-0.16106002662581723\n",
      "Gradient Descent(123/499): loss=0.3651061355857901, w0=0.39686836047404944, w1=-0.16232082346420176\n",
      "Gradient Descent(124/499): loss=0.36424539137132794, w0=0.39532354456449115, w1=-0.1635561965261028\n",
      "Gradient Descent(125/499): loss=0.36339380174065844, w0=0.39378621036414607, w1=-0.16476646093902175\n",
      "Gradient Descent(126/499): loss=0.3625512179074346, w0=0.39225627460771506, w1=-0.16595192796190297\n",
      "Gradient Descent(127/499): loss=0.36171749418056737, w0=0.3907336557549059, w1=-0.1671129050599368\n",
      "Gradient Descent(128/499): loss=0.36089248788542827, w0=0.3892182739313157, w1=-0.16824969597630102\n",
      "Gradient Descent(129/499): loss=0.3600760592880195, w0=0.38771005087170907, w1=-0.16936260080099025\n",
      "Gradient Descent(130/499): loss=0.3592680715219192, w0=0.3862089098655948, w1=-0.170451916036876\n",
      "Gradient Descent(131/499): loss=0.35846839051782886, w0=0.3847147757050083, w1=-0.17151793466313472\n",
      "Gradient Descent(132/499): loss=0.35767688493556343, w0=0.3832275746344109, w1=-0.1725609461961745\n",
      "Gradient Descent(133/499): loss=0.3568934260983363, w0=0.38174723430262003, w1=-0.1735812367481861\n",
      "Gradient Descent(134/499): loss=0.35611788792920834, w0=0.38027368371668835, w1=-0.17457908908343775\n",
      "Gradient Descent(135/499): loss=0.355350146889572, w0=0.37880685319765267, w1=-0.17555478267242872\n",
      "Gradient Descent(136/499): loss=0.35459008191956337, w0=0.3773466743380771, w1=-0.1765085937440103\n",
      "Gradient Descent(137/499): loss=0.35383757438029234, w0=0.37589307996131743, w1=-0.17744079533557916\n",
      "Gradient Descent(138/499): loss=0.35309250799779973, w0=0.37444600408243683, w1=-0.17835165734144273\n",
      "Gradient Descent(139/499): loss=0.3523547688086489, w0=0.3730053818707057, w1=-0.17924144655945096\n",
      "Gradient Descent(140/499): loss=0.3516242451070755, w0=0.3715711496136208, w1=-0.18011042673598598\n",
      "Gradient Descent(141/499): loss=0.35090082739361617, w0=0.3701432446823821, w1=-0.18095885860939523\n",
      "Gradient Descent(142/499): loss=0.3501844083251498, w0=0.3687216054987669, w1=-0.18178699995195075\n",
      "Gradient Descent(143/499): loss=0.349474882666288, w0=0.36730617150334505, w1=-0.18259510561041264\n",
      "Gradient Descent(144/499): loss=0.3487721472420547, w0=0.3658968831249785, w1=-0.1833834275452715\n",
      "Gradient Descent(145/499): loss=0.3480761008918004, w0=0.36449368175155394, w1=-0.18415221486874037\n",
      "Gradient Descent(146/499): loss=0.34738664442430317, w0=0.3630965097018966, w1=-0.18490171388156368\n",
      "Gradient Descent(147/499): loss=0.34670368057400675, w0=0.36170531019881635, w1=-0.1856321681087073\n",
      "Gradient Descent(148/499): loss=0.3460271139583549, w0=0.3603200273432394, w1=-0.18634381833399033\n",
      "Gradient Descent(149/499): loss=0.34535685103618086, w0=0.35894060608938033, w1=-0.18703690263371628\n",
      "Gradient Descent(150/499): loss=0.3446928000671126, w0=0.35756699222091076, w1=-0.18771165640935855\n",
      "Gradient Descent(151/499): loss=0.3440348710719643, w0=0.3561991323280835, w1=-0.18836831241935206\n",
      "Gradient Descent(152/499): loss=0.3433829757940742, w0=0.3548369737857713, w1=-0.18900710081003982\n",
      "Gradient Descent(153/499): loss=0.34273702766156444, w0=0.3534804647323826, w1=-0.1896282491458217\n",
      "Gradient Descent(154/499): loss=0.34209694175049066, w0=0.35212955404961666, w1=-0.19023198243854877\n",
      "Gradient Descent(155/499): loss=0.3414626347488562, w0=0.35078419134302263, w1=-0.19081852317620557\n",
      "Gradient Descent(156/499): loss=0.3408340249214645, w0=0.3494443269233287, w1=-0.19138809135091908\n",
      "Gradient Descent(157/499): loss=0.34021103207558584, w0=0.34810991178850853, w1=-0.1919409044863322\n",
      "Gradient Descent(158/499): loss=0.33959357752741726, w0=0.34678089760655306, w1=-0.19247717766437664\n",
      "Gradient Descent(159/499): loss=0.33898158406931217, w0=0.3454572366989184, w1=-0.19299712355147838\n",
      "Gradient Descent(160/499): loss=0.33837497593776006, w0=0.3441388820246198, w1=-0.19350095242422743\n",
      "Gradient Descent(161/499): loss=0.337773678782098, w0=0.3428257871649446, w1=-0.19398887219454064\n",
      "Gradient Descent(162/499): loss=0.33717761963393345, w0=0.3415179063087574, w1=-0.19446108843434629\n",
      "Gradient Descent(163/499): loss=0.3365867268772631, w0=0.34021519423837154, w1=-0.19491780439981587\n",
      "Gradient Descent(164/499): loss=0.3360009302192675, w0=0.33891760631596257, w1=-0.19535922105516823\n",
      "Gradient Descent(165/499): loss=0.3354201606617693, w0=0.33762509847050026, w1=-0.19578553709606875\n",
      "Gradient Descent(166/499): loss=0.33484435047333533, w0=0.33633762718517635, w1=-0.1961969489726457\n",
      "Gradient Descent(167/499): loss=0.33427343316201175, w0=0.33505514948530646, w1=-0.19659365091214404\n",
      "Gradient Descent(168/499): loss=0.3337073434486732, w0=0.3337776229266855, w1=-0.1969758349412357\n",
      "Gradient Descent(169/499): loss=0.3331460172409757, w0=0.3325050055843769, w1=-0.19734369090800447\n",
      "Gradient Descent(170/499): loss=0.3325893916078978, w0=0.3312372560419159, w1=-0.1976974065036221\n",
      "Gradient Descent(171/499): loss=0.3320374047548565, w0=0.3299743333809096, w1=-0.19803716728373166\n",
      "Gradient Descent(172/499): loss=0.3314899959993872, w0=0.3287161971710155, w1=-0.19836315668955246\n",
      "Gradient Descent(173/499): loss=0.33094710574737213, w0=0.3274628074602826, w1=-0.1986755560687204\n",
      "Gradient Descent(174/499): loss=0.3304086754698084, w0=0.32621412476583783, w1=-0.19897454469587697\n",
      "Gradient Descent(175/499): loss=0.32987464768010116, w0=0.32497011006490395, w1=-0.19926029979301813\n",
      "Gradient Descent(176/499): loss=0.3293449659118719, w0=0.32373072478613324, w1=-0.1995329965496147\n",
      "Gradient Descent(177/499): loss=0.3288195746972705, w0=0.3224959308012431, w1=-0.19979280814251463\n",
      "Gradient Descent(178/499): loss=0.3282984195457793, w0=0.3212656904169404, w1=-0.20003990575563635\n",
      "Gradient Descent(179/499): loss=0.32778144692349925, w0=0.3200399663671217, w1=-0.20027445859946272\n",
      "Gradient Descent(180/499): loss=0.3272686042329069, w0=0.3188187218053365, w1=-0.20049663393034334\n",
      "Gradient Descent(181/499): loss=0.3267598397930729, w0=0.31760192029750284, w1=-0.20070659706961338\n",
      "Gradient Descent(182/499): loss=0.3262551028203308, w0=0.3163895258148629, w1=-0.20090451142253551\n",
      "Gradient Descent(183/499): loss=0.32575434340938747, w0=0.31518150272716855, w1=-0.20109053849707206\n",
      "Gradient Descent(184/499): loss=0.32525751251486423, w0=0.3139778157960864, w1=-0.20126483792249286\n",
      "Gradient Descent(185/499): loss=0.3247645619332599, w0=0.3127784301688128, w1=-0.20142756746782506\n",
      "Gradient Descent(186/499): loss=0.324275444285327, w0=0.3115833113718886, w1=-0.20157888306014934\n",
      "Gradient Descent(187/499): loss=0.3237901129988508, w0=0.31039242530520617, w1=-0.20171893880274774\n",
      "Gradient Descent(188/499): loss=0.32330852229182266, w0=0.3092057382361987, w1=-0.20184788699310713\n",
      "Gradient Descent(189/499): loss=0.32283062715599986, w0=0.30802321679420447, w1=-0.20196587814078246\n",
      "Gradient Descent(190/499): loss=0.32235638334084254, w0=0.30684482796499835, w1=-0.20207306098512334\n",
      "Gradient Descent(191/499): loss=0.32188574733781716, w0=0.3056705390854825, w1=-0.2021695825128673\n",
      "Gradient Descent(192/499): loss=0.32141867636506377, w0=0.3045003178385297, w1=-0.20225558797560236\n",
      "Gradient Descent(193/499): loss=0.3209551283524128, w0=0.30333413224797284, w1=-0.20233122090710243\n",
      "Gradient Descent(194/499): loss=0.3204950619267469, w0=0.3021719506737331, w1=-0.20239662314053702\n",
      "Gradient Descent(195/499): loss=0.32003843639770035, w0=0.30101374180708185, w1=-0.20245193482555823\n",
      "Gradient Descent(196/499): loss=0.31958521174368437, w0=0.2998594746660297, w1=-0.2024972944452667\n",
      "Gradient Descent(197/499): loss=0.31913534859823767, w0=0.29870911859083726, w1=-0.20253283883305823\n",
      "Gradient Descent(198/499): loss=0.31868880823668705, w0=0.29756264323964254, w1=-0.20255870318935298\n",
      "Gradient Descent(199/499): loss=0.3182455525631166, w0=0.29642001858419975, w1=-0.20257502109820824\n",
      "Gradient Descent(200/499): loss=0.31780554409763667, w0=0.29528121490572445, w1=-0.20258192454381657\n",
      "Gradient Descent(201/499): loss=0.3173687459639439, w0=0.294146202790841, w1=-0.20257954392688973\n",
      "Gradient Descent(202/499): loss=0.31693512187716855, w0=0.29301495312762743, w1=-0.20256800808093023\n",
      "Gradient Descent(203/499): loss=0.3165046361319994, w0=0.29188743710175385, w1=-0.20254744428839036\n",
      "Gradient Descent(204/499): loss=0.31607725359108285, w0=0.2907636261927104, w1=-0.20251797829672033\n",
      "Gradient Descent(205/499): loss=0.31565293967368674, w0=0.2896434921701208, w1=-0.2024797343343056\n",
      "Gradient Descent(206/499): loss=0.31523166034462424, w0=0.28852700709013807, w1=-0.20243283512629415\n",
      "Gradient Descent(207/499): loss=0.3148133821034322, w0=0.28741414329191894, w1=-0.20237740191031395\n",
      "Gradient Descent(208/499): loss=0.3143980719737962, w0=0.28630487339417404, w1=-0.20231355445208124\n",
      "Gradient Descent(209/499): loss=0.31398569749321775, w0=0.28519917029178976, w1=-0.20224141106089982\n",
      "Gradient Descent(210/499): loss=0.31357622670291785, w0=0.2840970071525203, w1=-0.20216108860505141\n",
      "Gradient Descent(211/499): loss=0.31316962813797017, w0=0.2829983574137458, w1=-0.20207270252707757\n",
      "Gradient Descent(212/499): loss=0.31276587081765905, w0=0.28190319477929476, w1=-0.20197636685895295\n",
      "Gradient Descent(213/499): loss=0.3123649242360583, w0=0.28081149321632776, w1=-0.20187219423715036\n",
      "Gradient Descent(214/499): loss=0.3119667583528217, w0=0.2797232269522801, w1=-0.20176029591759737\n",
      "Gradient Descent(215/499): loss=0.3115713435841854, w0=0.2786383704718614, w1=-0.20164078179052455\n",
      "Gradient Descent(216/499): loss=0.31117865079417095, w0=0.27755689851410986, w1=-0.2015137603952055\n",
      "Gradient Descent(217/499): loss=0.3107886512859902, w0=0.27647878606949855, w1=-0.20137933893458826\n",
      "Gradient Descent(218/499): loss=0.3104013167936429, w0=0.2754040083770927, w1=-0.20123762328981856\n",
      "Gradient Descent(219/499): loss=0.3100166194737035, w0=0.27433254092175563, w1=-0.20108871803465433\n",
      "Gradient Descent(220/499): loss=0.3096345318972936, w0=0.2732643594314012, w1=-0.2009327264497717\n",
      "Gradient Descent(221/499): loss=0.30925502704223434, w0=0.272199439874292, w1=-0.20076975053696236\n",
      "Gradient Descent(222/499): loss=0.3088780782853744, w0=0.2711377584563809, w1=-0.20059989103322198\n",
      "Gradient Descent(223/499): loss=0.3085036593950898, w0=0.2700792916186946, w1=-0.20042324742472994\n",
      "Gradient Descent(224/499): loss=0.30813174452395165, w0=0.26902401603475806, w1=-0.20023991796071985\n",
      "Gradient Descent(225/499): loss=0.307762308201556, w0=0.26797190860805814, w1=-0.20004999966724119\n",
      "Gradient Descent(226/499): loss=0.30739532532751457, w0=0.2669229464695452, w1=-0.1998535883608116\n",
      "Gradient Descent(227/499): loss=0.3070307711645994, w0=0.2658771069751714, w1=-0.19965077866196\n",
      "Gradient Descent(228/499): loss=0.3066686213320399, w0=0.2648343677034649, w1=-0.1994416640086603\n",
      "Gradient Descent(229/499): loss=0.30630885179896783, w0=0.2637947064531379, w1=-0.19922633666965578\n",
      "Gradient Descent(230/499): loss=0.3059514388780059, w0=0.2627581012407287, w1=-0.19900488775767383\n",
      "Gradient Descent(231/499): loss=0.30559635921899797, w0=0.26172453029827575, w1=-0.19877740724253132\n",
      "Gradient Descent(232/499): loss=0.30524358980287536, w0=0.26069397207102346, w1=-0.1985439839641303\n",
      "Gradient Descent(233/499): loss=0.3048931079356591, w0=0.2596664052151581, w1=-0.19830470564534422\n",
      "Gradient Descent(234/499): loss=0.30454489124258943, w0=0.2586418085955738, w1=-0.19805965890479446\n",
      "Gradient Descent(235/499): loss=0.3041989176623865, w0=0.25762016128366705, w1=-0.19780892926951738\n",
      "Gradient Descent(236/499): loss=0.30385516544163216, w0=0.2566014425551595, w1=-0.19755260118752171\n",
      "Gradient Descent(237/499): loss=0.30351361312927494, w0=0.2555856318879479, w1=-0.19729075804023669\n",
      "Gradient Descent(238/499): loss=0.3031742395712532, w0=0.2545727089599809, w1=-0.19702348215485035\n",
      "Gradient Descent(239/499): loss=0.3028370239052318, w0=0.25356265364716124, w1=-0.19675085481653903\n",
      "Gradient Descent(240/499): loss=0.3025019455554538, w0=0.252555446021274, w1=-0.19647295628058703\n",
      "Gradient Descent(241/499): loss=0.30216898422769994, w0=0.25155106634793883, w1=-0.19618986578439768\n",
      "Gradient Descent(242/499): loss=0.30183811990435677, w0=0.25054949508458685, w1=-0.19590166155939504\n",
      "Gradient Descent(243/499): loss=0.30150933283958814, w0=0.24955071287846067, w1=-0.19560842084281696\n",
      "Gradient Descent(244/499): loss=0.30118260355460896, w0=0.24855470056463785, w1=-0.19531021988939948\n",
      "Gradient Descent(245/499): loss=0.3008579128330588, w0=0.24756143916407658, w1=-0.1950071339829527\n",
      "Gradient Descent(246/499): loss=0.30053524171647195, w0=0.24657090988168354, w1=-0.1946992374478285\n",
      "Gradient Descent(247/499): loss=0.30021457149984315, w0=0.24558309410440343, w1=-0.19438660366028024\n",
      "Gradient Descent(248/499): loss=0.2998958837272838, w0=0.24459797339932965, w1=-0.19406930505971465\n",
      "Gradient Descent(249/499): loss=0.2995791601877708, w0=0.24361552951183565, w1=-0.1937474131598364\n",
      "Gradient Descent(250/499): loss=0.29926438291098134, w0=0.2426357443637268, w1=-0.19342099855968523\n",
      "Gradient Descent(251/499): loss=0.29895153416321457, w0=0.24165860005141218, w1=-0.19309013095456654\n",
      "Gradient Descent(252/499): loss=0.2986405964433973, w0=0.24068407884409598, w1=-0.19275487914687509\n",
      "Gradient Descent(253/499): loss=0.29833155247916987, w0=0.23971216318198832, w1=-0.19241531105681292\n",
      "Gradient Descent(254/499): loss=0.298024385223054, w0=0.23874283567453486, w1=-0.1920714937330009\n",
      "Gradient Descent(255/499): loss=0.29771907784869756, w0=0.23777607909866527, w1=-0.19172349336298536\n",
      "Gradient Descent(256/499): loss=0.29741561374719416, w0=0.2368118763970599, w1=-0.19137137528363926\n",
      "Gradient Descent(257/499): loss=0.29711397652347915, w0=0.2358502106764345, w1=-0.191015203991459\n",
      "Gradient Descent(258/499): loss=0.2968141499927972, w0=0.23489106520584294, w1=-0.1906550431527567\n",
      "Gradient Descent(259/499): loss=0.2965161181772395, w0=0.23393442341499718, w1=-0.19029095561374912\n",
      "Gradient Descent(260/499): loss=0.29621986530235256, w0=0.23298026889260465, w1=-0.1899230034105429\n",
      "Gradient Descent(261/499): loss=0.295925375793811, w0=0.23202858538472274, w1=-0.1895512477790171\n",
      "Gradient Descent(262/499): loss=0.2956326342741597, w0=0.23107935679312985, w1=-0.18917574916460347\n",
      "Gradient Descent(263/499): loss=0.29534162555961824, w0=0.23013256717371325, w1=-0.18879656723196464\n",
      "Gradient Descent(264/499): loss=0.29505233465694813, w0=0.22918820073487312, w1=-0.1884137608745713\n",
      "Gradient Descent(265/499): loss=0.29476474676038217, w0=0.22824624183594272, w1=-0.18802738822417828\n",
      "Gradient Descent(266/499): loss=0.29447884724861373, w0=0.22730667498562468, w1=-0.18763750666020057\n",
      "Gradient Descent(267/499): loss=0.29419462168184324, w0=0.22636948484044273, w1=-0.1872441728189895\n",
      "Gradient Descent(268/499): loss=0.293912055798884, w0=0.2254346562032092, w1=-0.1868474426030098\n",
      "Gradient Descent(269/499): loss=0.29363113551432113, w0=0.22450217402150788, w1=-0.18644737118991822\n",
      "Gradient Descent(270/499): loss=0.29335184691572797, w0=0.22357202338619186, w1=-0.18604401304154392\n",
      "Gradient Descent(271/499): loss=0.2930741762609335, w0=0.22264418952989665, w1=-0.18563742191277172\n",
      "Gradient Descent(272/499): loss=0.2927981099753431, w0=0.22171865782556785, w1=-0.18522765086032844\n",
      "Gradient Descent(273/499): loss=0.2925236346493099, w0=0.22079541378500384, w1=-0.1848147522514731\n",
      "Gradient Descent(274/499): loss=0.2922507370355548, w0=0.21987444305741263, w1=-0.18439877777259156\n",
      "Gradient Descent(275/499): loss=0.2919794040466378, w0=0.2189557314279834, w1=-0.18397977843769636\n",
      "Gradient Descent(276/499): loss=0.2917096227524734, w0=0.21803926481647207, w1=-0.18355780459683216\n",
      "Gradient Descent(277/499): loss=0.29144138037789574, w0=0.21712502927580085, w1=-0.18313290594438764\n",
      "Gradient Descent(278/499): loss=0.29117466430026645, w0=0.21621301099067208, w1=-0.1827051315273144\n",
      "Gradient Descent(279/499): loss=0.29090946204712836, w0=0.21530319627619549, w1=-0.18227452975325373\n",
      "Gradient Descent(280/499): loss=0.29064576129390257, w0=0.2143955715765293, w1=-0.18184114839857146\n",
      "Gradient Descent(281/499): loss=0.2903835498616273, w0=0.2134901234635348, w1=-0.18140503461630222\n",
      "Gradient Descent(282/499): loss=0.29012281571473914, w0=0.21258683863544428, w1=-0.1809662349440031\n",
      "Gradient Descent(283/499): loss=0.2898635469588937, w0=0.2116857039155423, w1=-0.18052479531151797\n",
      "Gradient Descent(284/499): loss=0.2896057318388273, w0=0.2107867062508601, w1=-0.18008076104865287\n",
      "Gradient Descent(285/499): loss=0.2893493587362564, w0=0.20988983271088288, w1=-0.17963417689276306\n",
      "Gradient Descent(286/499): loss=0.2890944161678154, w0=0.2089950704862701, w1=-0.1791850869962529\n",
      "Gradient Descent(287/499): loss=0.2888408927830325, w0=0.20810240688758866, w1=-0.17873353493398855\n",
      "Gradient Descent(288/499): loss=0.2885887773623398, w0=0.20721182934405832, w1=-0.17827956371062503\n",
      "Gradient Descent(289/499): loss=0.28833805881512115, w0=0.2063233254023103, w1=-0.17782321576784743\n",
      "Gradient Descent(290/499): loss=0.28808872617779396, w0=0.20543688272515764, w1=-0.17736453299152805\n",
      "Gradient Descent(291/499): loss=0.2878407686119244, w0=0.2045524890903784, w1=-0.17690355671879907\n",
      "Gradient Descent(292/499): loss=0.2875941754023773, w0=0.20367013238951073, w1=-0.17644032774504242\n",
      "Gradient Descent(293/499): loss=0.2873489359554979, w0=0.20278980062666027, w1=-0.17597488633079686\n",
      "Gradient Descent(294/499): loss=0.2871050397973251, w0=0.2019114819173193, w1=-0.1755072722085836\n",
      "Gradient Descent(295/499): loss=0.28686247657183733, w0=0.20103516448719808, w1=-0.17503752458965052\n",
      "Gradient Descent(296/499): loss=0.2866212360392277, w0=0.2001608366710676, w1=-0.17456568217063653\n",
      "Gradient Descent(297/499): loss=0.2863813080742098, w0=0.19928848691161435, w1=-0.1740917831401558\n",
      "Gradient Descent(298/499): loss=0.28614268266435355, w0=0.19841810375830646, w1=-0.17361586518530375\n",
      "Gradient Descent(299/499): loss=0.2859053499084478, w0=0.19754967586627137, w1=-0.17313796549808427\n",
      "Gradient Descent(300/499): loss=0.28566930001489343, w0=0.19668319199518483, w1=-0.17265812078176013\n",
      "Gradient Descent(301/499): loss=0.2854345233001224, w0=0.19581864100817126, w1=-0.17217636725712615\n",
      "Gradient Descent(302/499): loss=0.2852010101870448, w0=0.19495601187071518, w1=-0.1716927406687071\n",
      "Gradient Descent(303/499): loss=0.2849687512035214, w0=0.19409529364958383, w1=-0.1712072762908795\n",
      "Gradient Descent(304/499): loss=0.28473773698086324, w0=0.1932364755117606, w1=-0.17072000893391995\n",
      "Gradient Descent(305/499): loss=0.28450795825235636, w0=0.19237954672338953, w1=-0.17023097294997858\n",
      "Gradient Descent(306/499): loss=0.28427940585181033, w0=0.1915244966487304, w1=-0.1697402022389805\n",
      "Gradient Descent(307/499): loss=0.28405207071213334, w0=0.19067131474912472, w1=-0.16924773025445414\n",
      "Gradient Descent(308/499): loss=0.28382594386392845, w0=0.1898199905819722, w1=-0.16875359000928844\n",
      "Gradient Descent(309/499): loss=0.28360101643411634, w0=0.18897051379971763, w1=-0.16825781408141888\n",
      "Gradient Descent(310/499): loss=0.28337727964457843, w0=0.18812287414884843, w1=-0.16776043461944354\n",
      "Gradient Descent(311/499): loss=0.2831547248108245, w0=0.18727706146890225, w1=-0.16726148334816954\n",
      "Gradient Descent(312/499): loss=0.28293334334068043, w0=0.18643306569148496, w1=-0.16676099157409083\n",
      "Gradient Descent(313/499): loss=0.282713126733, w0=0.18559087683929876, w1=-0.16625899019079787\n",
      "Gradient Descent(314/499): loss=0.2824940665763958, w0=0.18475048502518024, w1=-0.1657555096843201\n",
      "Gradient Descent(315/499): loss=0.2822761545479922, w0=0.18391188045114856, w1=-0.16525058013840155\n",
      "Gradient Descent(316/499): loss=0.28205938241219813, w0=0.18307505340746333, w1=-0.16474423123971085\n",
      "Gradient Descent(317/499): loss=0.28184374201950096, w0=0.1822399942716924, w1=-0.16423649228298562\n",
      "Gradient Descent(318/499): loss=0.28162922530527773, w0=0.18140669350778926, w1=-0.1637273921761127\n",
      "Gradient Descent(319/499): loss=0.28141582428862866, w0=0.18057514166518016, w1=-0.16321695944514422\n",
      "Gradient Descent(320/499): loss=0.2812035310712277, w0=0.1797453293778605, w1=-0.16270522223925077\n",
      "Gradient Descent(321/499): loss=0.2809923378361916, w0=0.17891724736350104, w1=-0.16219220833561185\n",
      "Gradient Descent(322/499): loss=0.2807822368469681, w0=0.1780908864225631, w1=-0.16167794514424477\n",
      "Gradient Descent(323/499): loss=0.28057322044624144, w0=0.1772662374374232, w1=-0.16116245971277218\n",
      "Gradient Descent(324/499): loss=0.28036528105485564, w0=0.17644329137150688, w1=-0.1606457787311294\n",
      "Gradient Descent(325/499): loss=0.28015841117075463, w0=0.17562203926843153, w1=-0.16012792853621174\n",
      "Gradient Descent(326/499): loss=0.27995260336793965, w0=0.17480247225115836, w1=-0.15960893511646293\n",
      "Gradient Descent(327/499): loss=0.27974785029544286, w0=0.1739845815211531, w1=-0.15908882411640493\n",
      "Gradient Descent(328/499): loss=0.2795441446763184, w0=0.17316835835755578, w1=-0.15856762084110995\n",
      "Gradient Descent(329/499): loss=0.2793414793066472, w0=0.17235379411635918, w1=-0.15804535026061547\n",
      "Gradient Descent(330/499): loss=0.2791398470545597, w0=0.17154088022959582, w1=-0.1575220370142826\n",
      "Gradient Descent(331/499): loss=0.27893924085927296, w0=0.17072960820453378, w1=-0.15699770541509867\n",
      "Gradient Descent(332/499): loss=0.2787396537301425, w0=0.16991996962288095, w1=-0.15647237945392456\n",
      "Gradient Descent(333/499): loss=0.27854107874573036, w0=0.16911195613999763, w1=-0.15594608280368755\n",
      "Gradient Descent(334/499): loss=0.27834350905288613, w0=0.16830555948411768, w1=-0.15541883882352\n",
      "Gradient Descent(335/499): loss=0.2781469378658435, w0=0.1675007714555778, w1=-0.154890670562845\n",
      "Gradient Descent(336/499): loss=0.27795135846533026, w0=0.166697583926055, w1=-0.15436160076540903\n",
      "Gradient Descent(337/499): loss=0.27775676419769263, w0=0.16589598883781242, w1=-0.15383165187326264\n",
      "Gradient Descent(338/499): loss=0.27756314847403263, w0=0.16509597820295294, w1=-0.15330084603068977\n",
      "Gradient Descent(339/499): loss=0.27737050476935876, w0=0.16429754410268088, w1=-0.15276920508808595\n",
      "Gradient Descent(340/499): loss=0.2771788266217504, w0=0.1635006786865716, w1=-0.15223675060578637\n",
      "Gradient Descent(341/499): loss=0.2769881076315356, w0=0.16270537417184885, w1=-0.15170350385784434\n",
      "Gradient Descent(342/499): loss=0.2767983414604784, w0=0.1619116228426701, w1=-0.15116948583576045\n",
      "Gradient Descent(343/499): loss=0.2766095218309832, w0=0.16111941704941918, w1=-0.1506347172521634\n",
      "Gradient Descent(344/499): loss=0.27642164252530765, w0=0.16032874920800672, w1=-0.1500992185444428\n",
      "Gradient Descent(345/499): loss=0.2762346973847889, w0=0.15953961179917822, w1=-0.1495630098783347\n",
      "Gradient Descent(346/499): loss=0.2760486803090828, w0=0.15875199736782933, w1=-0.1490261111514603\n",
      "Gradient Descent(347/499): loss=0.275863585255412, w0=0.15796589852232867, w1=-0.14848854199681846\n",
      "Gradient Descent(348/499): loss=0.27567940623782794, w0=0.1571813079338479, w1=-0.14795032178623255\n",
      "Gradient Descent(349/499): loss=0.2754961373264826, w0=0.15639821833569903, w1=-0.14741146963375215\n",
      "Gradient Descent(350/499): loss=0.27531377264691215, w0=0.15561662252267888, w1=-0.14687200439901038\n",
      "Gradient Descent(351/499): loss=0.27513230637933084, w0=0.15483651335042067, w1=-0.14633194469053695\n",
      "Gradient Descent(352/499): loss=0.27495173275793555, w0=0.15405788373475254, w1=-0.14579130886902814\n",
      "Gradient Descent(353/499): loss=0.27477204607022104, w0=0.1532807266510632, w1=-0.14525011505057325\n",
      "Gradient Descent(354/499): loss=0.2745932406563054, w0=0.15250503513367422, w1=-0.14470838110983955\n",
      "Gradient Descent(355/499): loss=0.2744153109082662, w0=0.15173080227521935, w1=-0.14416612468321427\n",
      "Gradient Descent(356/499): loss=0.2742382512694845, w0=0.1509580212260304, w1=-0.1436233631719063\n",
      "Gradient Descent(357/499): loss=0.27406205623400187, w0=0.15018668519352996, w1=-0.14308011374500615\n",
      "Gradient Descent(358/499): loss=0.2738867203458843, w0=0.14941678744163067, w1=-0.14253639334250637\n",
      "Gradient Descent(359/499): loss=0.2737122381985967, w0=0.14864832129014102, w1=-0.1419922186782814\n",
      "Gradient Descent(360/499): loss=0.273538604434387, w0=0.14788128011417753, w1=-0.14144760624302893\n",
      "Gradient Descent(361/499): loss=0.273365813743678, w0=0.1471156573435836, w1=-0.14090257230717138\n",
      "Gradient Descent(362/499): loss=0.27319386086447073, w0=0.14635144646235432, w1=-0.14035713292372023\n",
      "Gradient Descent(363/499): loss=0.27302274058175324, w0=0.14558864100806806, w1=-0.13981130393110142\n",
      "Gradient Descent(364/499): loss=0.27285244772692135, w0=0.14482723457132382, w1=-0.1392651009559443\n",
      "Gradient Descent(365/499): loss=0.27268297717720646, w0=0.144067220795185, w1=-0.1387185394158327\n",
      "Gradient Descent(366/499): loss=0.27251432385511104, w0=0.14330859337462903, w1=-0.13817163452202075\n",
      "Gradient Descent(367/499): loss=0.2723464827278541, w0=0.14255134605600348, w1=-0.13762440128211134\n",
      "Gradient Descent(368/499): loss=0.27217944880682376, w0=0.14179547263648745, w1=-0.13707685450270063\n",
      "Gradient Descent(369/499): loss=0.2720132171470376, w0=0.1410409669635596, w1=-0.1365290087919859\n",
      "Gradient Descent(370/499): loss=0.27184778284661204, w0=0.1402878229344714, w1=-0.13598087856234037\n",
      "Gradient Descent(371/499): loss=0.27168314104623853, w0=0.13953603449572657, w1=-0.13543247803285227\n",
      "Gradient Descent(372/499): loss=0.2715192869286672, w0=0.1387855956425661, w1=-0.13488382123183185\n",
      "Gradient Descent(373/499): loss=0.27135621571819984, w0=0.13803650041845889, w1=-0.1343349219992834\n",
      "Gradient Descent(374/499): loss=0.27119392268018655, w0=0.13728874291459803, w1=-0.13378579398934615\n",
      "Gradient Descent(375/499): loss=0.2710324031205344, w0=0.13654231726940277, w1=-0.13323645067270098\n",
      "Gradient Descent(376/499): loss=0.27087165238521976, w0=0.1357972176680256, w1=-0.13268690533894695\n",
      "Gradient Descent(377/499): loss=0.2707116658598086, w0=0.13505343834186528, w1=-0.13213717109894427\n",
      "Gradient Descent(378/499): loss=0.27055243896898434, w0=0.1343109735680847, w1=-0.13158726088712824\n",
      "Gradient Descent(379/499): loss=0.2703939671760824, w0=0.1335698176691346, w1=-0.13103718746379012\n",
      "Gradient Descent(380/499): loss=0.2702362459826299, w0=0.13282996501228195, w1=-0.13048696341732993\n",
      "Gradient Descent(381/499): loss=0.2700792709278948, w0=0.1320914100091442, w1=-0.1299366011664771\n",
      "Gradient Descent(382/499): loss=0.26992303758843916, w0=0.13135414711522803, w1=-0.12938611296248362\n",
      "Gradient Descent(383/499): loss=0.26976754157767957, w0=0.13061817082947377, w1=-0.12883551089128586\n",
      "Gradient Descent(384/499): loss=0.26961277854545396, w0=0.12988347569380432, w1=-0.12828480687563987\n",
      "Gradient Descent(385/499): loss=0.26945874417759497, w0=0.1291500562926796, w1=-0.12773401267722576\n",
      "Gradient Descent(386/499): loss=0.2693054341955089, w0=0.12841790725265537, w1=-0.12718313989872668\n",
      "Gradient Descent(387/499): loss=0.2691528443557606, w0=0.1276870232419476, w1=-0.12663219998587738\n",
      "Gradient Descent(388/499): loss=0.26900097044966426, w0=0.12695739897000088, w1=-0.12608120422948826\n",
      "Gradient Descent(389/499): loss=0.26884980830288124, w0=0.12622902918706239, w1=-0.12553016376743947\n",
      "Gradient Descent(390/499): loss=0.26869935377502185, w0=0.12550190868375988, w1=-0.12497908958665162\n",
      "Gradient Descent(391/499): loss=0.2685496027592536, w0=0.12477603229068504, w1=-0.12442799252502686\n",
      "Gradient Descent(392/499): loss=0.26840055118191525, w0=0.12405139487798075, w1=-0.12387688327336754\n",
      "Gradient Descent(393/499): loss=0.26825219500213604, w0=0.12332799135493372, w1=-0.1233257723772658\n",
      "Gradient Descent(394/499): loss=0.2681045302114604, w0=0.1226058166695707, w1=-0.12277467023897147\n",
      "Gradient Descent(395/499): loss=0.2679575528334775, w0=0.12188486580826023, w1=-0.12222358711923151\n",
      "Gradient Descent(396/499): loss=0.2678112589234569, w0=0.12116513379531776, w1=-0.1216725331391086\n",
      "Gradient Descent(397/499): loss=0.2676656445679886, w0=0.12044661569261622, w1=-0.12112151828177166\n",
      "Gradient Descent(398/499): loss=0.2675207058846287, w0=0.11972930659919974, w1=-0.1205705523942665\n",
      "Gradient Descent(399/499): loss=0.26737643902154906, w0=0.11901320165090297, w1=-0.12001964518925871\n",
      "Gradient Descent(400/499): loss=0.2672328401571935, w0=0.11829829601997328, w1=-0.11946880624675763\n",
      "Gradient Descent(401/499): loss=0.26708990549993716, w0=0.11758458491469853, w1=-0.11891804501581275\n",
      "Gradient Descent(402/499): loss=0.2669476312877518, w0=0.11687206357903754, w1=-0.11836737081619257\n",
      "Gradient Descent(403/499): loss=0.26680601378787533, w0=0.11616072729225622, w1=-0.11781679284003553\n",
      "Gradient Descent(404/499): loss=0.26666504929648527, w0=0.11545057136856612, w1=-0.11726632015348487\n",
      "Gradient Descent(405/499): loss=0.26652473413837835, w0=0.11474159115676871, w1=-0.11671596169829547\n",
      "Gradient Descent(406/499): loss=0.2663850646666531, w0=0.11403378203990178, w1=-0.11616572629342592\n",
      "Gradient Descent(407/499): loss=0.2662460372623979, w0=0.11332713943489178, w1=-0.11561562263660229\n",
      "Gradient Descent(408/499): loss=0.2661076483343824, w0=0.1126216587922079, w1=-0.11506565930586868\n",
      "Gradient Descent(409/499): loss=0.26596989431875384, w0=0.11191733559552224, w1=-0.11451584476110903\n",
      "Gradient Descent(410/499): loss=0.2658327716787376, w0=0.11121416536137141, w1=-0.11396618734555727\n",
      "Gradient Descent(411/499): loss=0.26569627690434133, w0=0.11051214363882442, w1=-0.11341669528727827\n",
      "Gradient Descent(412/499): loss=0.2655604065120645, w0=0.10981126600915153, w1=-0.11286737670063876\n",
      "Gradient Descent(413/499): loss=0.2654251570446095, w0=0.1091115280854998, w1=-0.1123182395877484\n",
      "Gradient Descent(414/499): loss=0.26529052507059975, w0=0.10841292551256897, w1=-0.11176929183989268\n",
      "Gradient Descent(415/499): loss=0.2651565071842994, w0=0.10771545396629445, w1=-0.11122054123893498\n",
      "Gradient Descent(416/499): loss=0.26502310000533724, w0=0.1070191091535302, w1=-0.11067199545871263\n",
      "Gradient Descent(417/499): loss=0.2648903001784361, w0=0.10632388681173902, w1=-0.110123662066401\n",
      "Gradient Descent(418/499): loss=0.26475810437314323, w0=0.10562978270868205, w1=-0.10957554852387377\n",
      "Gradient Descent(419/499): loss=0.26462650928356674, w0=0.10493679264211636, w1=-0.10902766218902978\n",
      "Gradient Descent(420/499): loss=0.26449551162811424, w0=0.10424491243949098, w1=-0.1084800103171188\n",
      "Gradient Descent(421/499): loss=0.2643651081492357, w0=0.10355413795765152, w1=-0.10793260006203206\n",
      "Gradient Descent(422/499): loss=0.26423529561316966, w0=0.10286446508254256, w1=-0.10738543847759448\n",
      "Gradient Descent(423/499): loss=0.2641060708096923, w0=0.1021758897289193, w1=-0.10683853251881963\n",
      "Gradient Descent(424/499): loss=0.2639774305518713, w0=0.10148840784005606, w1=-0.10629188904316966\n",
      "Gradient Descent(425/499): loss=0.2638493716758215, w0=0.10080201538746476, w1=-0.10574551481177545\n",
      "Gradient Descent(426/499): loss=0.26372189104046473, w0=0.10011670837060944, w1=-0.10519941649066523\n",
      "Gradient Descent(427/499): loss=0.2635949855272932, w0=0.09943248281663156, w1=-0.10465360065195069\n",
      "Gradient Descent(428/499): loss=0.26346865204013437, w0=0.09874933478007014, w1=-0.1041080737750254\n",
      "Gradient Descent(429/499): loss=0.26334288750492146, w0=0.0980672603425939, w1=-0.10356284224771743\n",
      "Gradient Descent(430/499): loss=0.26321768886946545, w0=0.09738625561272693, w1=-0.1030179123674589\n",
      "Gradient Descent(431/499): loss=0.2630930531032301, w0=0.09670631672558741, w1=-0.10247329034240554\n",
      "Gradient Descent(432/499): loss=0.2629689771971105, w0=0.09602743984261865, w1=-0.10192898229257846\n",
      "Gradient Descent(433/499): loss=0.26284545816321464, w0=0.09534962115133447, w1=-0.10138499425095127\n",
      "Gradient Descent(434/499): loss=0.262722493034648, w0=0.0946728568650552, w1=-0.10084133216456526\n",
      "Gradient Descent(435/499): loss=0.26260007886529974, w0=0.09399714322265978, w1=-0.10029800189558441\n",
      "Gradient Descent(436/499): loss=0.26247821272963373, w0=0.09332247648832655, w1=-0.09975500922238525\n",
      "Gradient Descent(437/499): loss=0.2623568917224808, w0=0.09264885295129192, w1=-0.09921235984058013\n",
      "Gradient Descent(438/499): loss=0.26223611295883464, w0=0.09197626892559571, w1=-0.09867005936408298\n",
      "Gradient Descent(439/499): loss=0.26211587357364957, w0=0.09130472074984658, w1=-0.09812811332610102\n",
      "Gradient Descent(440/499): loss=0.26199617072164255, w0=0.09063420478697143, w1=-0.09758652718017793\n",
      "Gradient Descent(441/499): loss=0.2618770015770948, w0=0.08996471742398768, w1=-0.09704530630115402\n",
      "Gradient Descent(442/499): loss=0.26175836333366026, w0=0.08929625507175654, w1=-0.09650445598618812\n",
      "Gradient Descent(443/499): loss=0.2616402532041726, w0=0.08862881416476212, w1=-0.09596398145568634\n",
      "Gradient Descent(444/499): loss=0.26152266842045757, w0=0.08796239116086825, w1=-0.09542388785430425\n",
      "Gradient Descent(445/499): loss=0.2614056062331457, w0=0.08729698254110455, w1=-0.0948841802518439\n",
      "Gradient Descent(446/499): loss=0.26128906391148915, w0=0.08663258480942633, w1=-0.09434486364423807\n",
      "Gradient Descent(447/499): loss=0.2611730387431802, w0=0.08596919449250798, w1=-0.09380594295441505\n",
      "Gradient Descent(448/499): loss=0.26105752803417176, w0=0.0853068081395054, w1=-0.09326742303326696\n",
      "Gradient Descent(449/499): loss=0.26094252910850113, w0=0.08464542232185696, w1=-0.09272930866048167\n",
      "Gradient Descent(450/499): loss=0.2608280393081152, w0=0.0839850336330479, w1=-0.09219160454549707\n",
      "Gradient Descent(451/499): loss=0.26071405599269853, w0=0.08332563868841926, w1=-0.09165431532829946\n",
      "Gradient Descent(452/499): loss=0.26060057653950297, w0=0.08266723412493375, w1=-0.091117445580366\n",
      "Gradient Descent(453/499): loss=0.2604875983431806, w0=0.08200981660099281, w1=-0.0905809998054283\n",
      "Gradient Descent(454/499): loss=0.2603751188156173, w0=0.08135338279620347, w1=-0.09004498244040586\n",
      "Gradient Descent(455/499): loss=0.2602631353857701, w0=0.08069792941120407, w1=-0.08950939785613315\n",
      "Gradient Descent(456/499): loss=0.2601516454995054, w0=0.0800434531674312, w1=-0.08897425035828697\n",
      "Gradient Descent(457/499): loss=0.2600406466194395, w0=0.0793899508069548, w1=-0.08843954418807524\n",
      "Gradient Descent(458/499): loss=0.25993013622478206, w0=0.07873741909224422, w1=-0.08790528352316157\n",
      "Gradient Descent(459/499): loss=0.2598201118111798, w0=0.07808585480601354, w1=-0.08737147247831323\n",
      "Gradient Descent(460/499): loss=0.2597105708905647, w0=0.0774352547509857, w1=-0.08683811510632683\n",
      "Gradient Descent(461/499): loss=0.2596015109910011, w0=0.07678561574974883, w1=-0.08630521539863266\n",
      "Gradient Descent(462/499): loss=0.2594929296565379, w0=0.07613693464451739, w1=-0.08577277728622565\n",
      "Gradient Descent(463/499): loss=0.2593848244470597, w0=0.0754892082970006, w1=-0.0852408046402225\n",
      "Gradient Descent(464/499): loss=0.25927719293814194, w0=0.07484243358815913, w1=-0.08470930127280314\n",
      "Gradient Descent(465/499): loss=0.25917003272090716, w0=0.07419660741808703, w1=-0.08417827093771614\n",
      "Gradient Descent(466/499): loss=0.25906334140188303, w0=0.07355172670576218, w1=-0.08364771733123656\n",
      "Gradient Descent(467/499): loss=0.2589571166028615, w0=0.07290778838894346, w1=-0.08311764409261438\n",
      "Gradient Descent(468/499): loss=0.25885135596076136, w0=0.07226478942391304, w1=-0.0825880548050556\n",
      "Gradient Descent(469/499): loss=0.2587460571274909, w0=0.0716227267853906, w1=-0.08205895299610717\n",
      "Gradient Descent(470/499): loss=0.25864121776981336, w0=0.07098159746626519, w1=-0.08153034213866962\n",
      "Gradient Descent(471/499): loss=0.2585368355692133, w0=0.07034139847752889, w1=-0.08100222565131049\n",
      "Gradient Descent(472/499): loss=0.25843290822176535, w0=0.06970212684799552, w1=-0.0804746068993181\n",
      "Gradient Descent(473/499): loss=0.25832943343800363, w0=0.06906377962425643, w1=-0.07994748919493412\n",
      "Gradient Descent(474/499): loss=0.258226408942794, w0=0.06842635387038289, w1=-0.07942087579845972\n",
      "Gradient Descent(475/499): loss=0.25812383247520715, w0=0.06778984666790736, w1=-0.07889476991839596\n",
      "Gradient Descent(476/499): loss=0.2580217017883929, w0=0.06715425511550575, w1=-0.07836917471261563\n",
      "Gradient Descent(477/499): loss=0.257920014649457, w0=0.06651957632900818, w1=-0.07784409328839785\n",
      "Gradient Descent(478/499): loss=0.25781876883933896, w0=0.06588580744105649, w1=-0.07731952870368149\n",
      "Gradient Descent(479/499): loss=0.25771796215269105, w0=0.06525294560114929, w1=-0.07679548396697745\n",
      "Gradient Descent(480/499): loss=0.2576175923977593, w0=0.06462098797526944, w1=-0.07627196203872273\n",
      "Gradient Descent(481/499): loss=0.257517657396266, w0=0.06398993174596902, w1=-0.07574896583105052\n",
      "Gradient Descent(482/499): loss=0.25741815498329246, w0=0.06335977411196031, w1=-0.0752264982092673\n",
      "Gradient Descent(483/499): loss=0.2573190830071658, w0=0.06273051228824769, w1=-0.07470456199145721\n",
      "Gradient Descent(484/499): loss=0.25722043932934374, w0=0.062102143505674307, w1=-0.074183159950109\n",
      "Gradient Descent(485/499): loss=0.2571222218243034, w0=0.0614746650111092, w1=-0.07366229481152603\n",
      "Gradient Descent(486/499): loss=0.2570244283794301, w0=0.06084807406694051, w1=-0.07314196925763522\n",
      "Gradient Descent(487/499): loss=0.25692705689490836, w0=0.06022236795132768, w1=-0.07262218592516838\n",
      "Gradient Descent(488/499): loss=0.2568301052836131, w0=0.05959754395763018, w1=-0.0721029474076915\n",
      "Gradient Descent(489/499): loss=0.25673357147100373, w0=0.05897359939473676, w1=-0.07158425625451635\n",
      "Gradient Descent(490/499): loss=0.2566374533950177, w0=0.05835053158641645, w1=-0.07106611497299581\n",
      "Gradient Descent(491/499): loss=0.25654174900596644, w0=0.05772833787173918, w1=-0.07054852602711625\n",
      "Gradient Descent(492/499): loss=0.2564464562664325, w0=0.057107015604333226, w1=-0.0700314918401142\n",
      "Gradient Descent(493/499): loss=0.2563515731511675, w0=0.056486562152914435, w1=-0.06951501479268969\n",
      "Gradient Descent(494/499): loss=0.2562570976469915, w0=0.05586697490043104, w1=-0.0689990972260103\n",
      "Gradient Descent(495/499): loss=0.25616302775269384, w0=0.05524825124472223, w1=-0.06848374143947393\n",
      "Gradient Descent(496/499): loss=0.25606936147893394, w0=0.054630388597527495, w1=-0.06796894969417953\n",
      "Gradient Descent(497/499): loss=0.2559760968481455, w0=0.05401338438529947, w1=-0.0674547242101529\n",
      "Gradient Descent(498/499): loss=0.25588323189443923, w0=0.05339723604805033, w1=-0.06694106717037966\n",
      "Gradient Descent(499/499): loss=0.2557907646635087, w0=0.052781941040348876, w1=-0.06642798071738987\n"
     ]
    }
   ],
   "source": [
    "w_logreg, loss_logreg = f.logistic_regression(y_train_train_lg, tX_train,np.ones(22),500, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9097733667882285\n",
      "F1 score:  0.0506626763574177\n"
     ]
    }
   ],
   "source": [
    "y_pred = tX_train_test.dot(w_logreg)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "y_pred = np.where(y_pred == 1, 1, -1)\n",
    "\n",
    "_,_,_,_,f1 = f.confusion_matrix(y_train_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: \", np.sum(y_pred == y_train_test)/len(y_train_test))\n",
    "\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [ 0.05278194 -0.06642798  0.02117751 -0.00274513  0.49011676  0.18080126\n",
      " -0.32497592 -0.32600825 -0.52893756 -0.28400304 -0.42510498 -0.12187267\n",
      " -0.17498828 -0.1238623  -0.38370667  0.5267693   0.11758655  0.71558617\n",
      "  0.07128298  0.13220975  0.25721627  0.26162381] \n",
      " Loss =  0.2557907646635087 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.005704934219548543\n"
     ]
    }
   ],
   "source": [
    "y_test_logreg = tX_test.dot(w_logreg)\n",
    "y_test_logreg = np.where(y_test_logreg > 0.5, 1, 0)\n",
    "\n",
    "print('weights = \\n', w_logreg,'\\n Loss = ', loss_logreg,'\\n*****************************************************************************',\n",
    "        ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train== 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_logreg == 1)/len(y_test_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=18.207806990644862, w0=0.9078319282799385, w1=0.9078341834706829\n",
      "Gradient Descent(1/99): loss=16.36117617190301, w0=0.8157561433011598, w1=0.815760581290357\n",
      "Gradient Descent(2/99): loss=14.516392331053625, w0=0.7237725585933471, w1=0.7237791043345082\n",
      "Gradient Descent(3/99): loss=12.67345441132835, w0=0.631881095434813, w1=0.6318896713945503\n",
      "Gradient Descent(4/99): loss=10.832366467881576, w0=0.5400817347412534, w1=0.5400922610999396\n",
      "Gradient Descent(5/99): loss=8.99317095889692, w0=0.4483749079165405, w1=0.44838730280129574\n",
      "Gradient Descent(6/99): loss=7.156198218760744, w0=0.35676443230285565, w1=0.35677861205126216\n",
      "Gradient Descent(7/99): loss=5.323926978543616, w0=0.26527949655879657, w1=0.26529537601322084\n",
      "Gradient Descent(8/99): loss=3.5145487817161682, w0=0.17413819403247968, w1=0.17415568688197136\n",
      "Gradient Descent(9/99): loss=1.8472306256148947, w0=0.0849185676753477, w1=0.0849375869953934\n",
      "Gradient Descent(10/99): loss=0.76013464705722, w0=0.007034012888014168, w1=0.00705447298405322\n",
      "Gradient Descent(11/99): loss=0.433963205495897, w0=-0.03801961898097428, w1=-0.037997793297615885\n",
      "Gradient Descent(12/99): loss=0.3550633524257237, w0=-0.059395762457870455, w1=-0.059372615247669694\n",
      "Gradient Descent(13/99): loss=0.3268701664643956, w0=-0.07182392028144263, w1=-0.07179947312691057\n",
      "Gradient Descent(14/99): loss=0.3142037708873347, w0=-0.08001247627934195, w1=-0.079986742265792\n",
      "Gradient Descent(15/99): loss=0.3077443906596993, w0=-0.08579004755469558, w1=-0.08576303574026925\n",
      "Gradient Descent(16/99): loss=0.3041813076570549, w0=-0.09004022281464975, w1=-0.09001193996657422\n",
      "Gradient Descent(17/99): loss=0.3021092235676002, w0=-0.09325407858780918, w1=-0.09322453002142955\n",
      "Gradient Descent(18/99): loss=0.30085785882845695, w0=-0.09573138023247815, w1=-0.09570057027760036\n",
      "Gradient Descent(19/99): loss=0.30008046677697126, w0=-0.09766765730650187, w1=-0.09763558959207405\n",
      "Gradient Descent(20/99): loss=0.2995867532858525, w0=-0.09919682328411746, w1=-0.09916350092392799\n",
      "Gradient Descent(21/99): loss=0.29926754749497875, w0=-0.10041403937238022, w1=-0.10037946509224192\n",
      "Gradient Descent(22/99): loss=0.2990580428160503, w0=-0.1013888826169315, w1=-0.10135305884506338\n",
      "Gradient Descent(23/99): loss=0.298918722349955, w0=-0.10217337218291402, w1=-0.1021363011160253\n",
      "Gradient Descent(24/99): loss=0.29882496711464757, w0=-0.10280709352670828, w1=-0.10276877717940722\n",
      "Gradient Descent(25/99): loss=0.2987611688262856, w0=-0.10332059830492861, w1=-0.10328103854732212\n",
      "Gradient Descent(26/99): loss=0.29871728703685646, w0=-0.10373773464723901, w1=-0.10369693323395236\n",
      "Gradient Descent(27/99): loss=0.29868678273989935, w0=-0.10407728894964188, w1=-0.10403524754247134\n",
      "Gradient Descent(28/99): loss=0.2986653512908656, w0=-0.10435417013316163, w1=-0.10431089031891959\n",
      "Gradient Descent(29/99): loss=0.29865013153744874, w0=-0.10458028122654278, w1=-0.10453576453121995\n",
      "Gradient Descent(30/99): loss=0.2986392048098995, w0=-0.10476517194730102, w1=-0.10471941984737361\n",
      "Gradient Descent(31/99): loss=0.29863127354479374, w0=-0.10491653452463882, w1=-0.10486954845614051\n",
      "Gradient Descent(32/99): loss=0.2986254529294177, w0=-0.10504058514547933, w1=-0.10499236651130886\n",
      "Gradient Descent(33/99): loss=0.2986211345708191, w0=-0.10514236052536162, w1=-0.10509291070118854\n",
      "Gradient Descent(34/99): loss=0.298617896565649, w0=-0.105225950556284, w1=-0.10517527089533396\n",
      "Gradient Descent(35/99): loss=0.29861544374292615, w0=-0.1052946821847646, w1=-0.10524277402170543\n",
      "Gradient Descent(36/99): loss=0.2986135676866159, w0=-0.10535126566138835, w1=-0.10529813031549141\n",
      "Gradient Descent(37/99): loss=0.29861211981771707, w0=-0.10539791147567054, w1=-0.10534355025338632\n",
      "Gradient Descent(38/99): loss=0.2986109931544409, w0=-0.10543642426301437, w1=-0.10538083846007341\n",
      "Gradient Descent(39/99): loss=0.29861010987337877, w0=-0.1054682784940297, w1=-0.10541146939715798\n",
      "Gradient Descent(40/99): loss=0.2986094127704096, w0=-0.10549467966511666, w1=-0.10543664855343743\n",
      "Gradient Descent(41/99): loss=0.2986088593577876, w0=-0.10551661389163278, w1=-0.10545736203781286\n",
      "Gradient Descent(42/99): loss=0.2986084177532454, w0=-0.10553488818495926, w1=-0.10547441685614704\n",
      "Gradient Descent(43/99): loss=0.2986080637943373, w0=-0.10555016321942932, w1=-0.10548847367802253\n",
      "Gradient Descent(44/99): loss=0.29860777899563967, w0=-0.10556298002704874, w1=-0.10550007353132328\n",
      "Gradient Descent(45/99): loss=0.2986075490895667, w0=-0.10557378177053149, w1=-0.10550965957515666\n",
      "Gradient Descent(46/99): loss=0.2986073629741543, w0=-0.10558293151902733, w1=-0.10551759487548804\n",
      "Gradient Descent(47/99): loss=0.2986072119468119, w0=-0.10559072677181225, w1=-0.10552417692875515\n",
      "Gradient Descent(48/99): loss=0.29860708914066614, w0=-0.1055974113325643, w1=-0.10552964953608168\n",
      "Gradient Descent(49/99): loss=0.2986069891057099, w0=-0.1056031850226847, w1=-0.10553421251654736\n",
      "Gradient Descent(50/99): loss=0.298606907494404, w0=-0.10560821163039397, w1=-0.10553802965624225\n",
      "Gradient Descent(51/99): loss=0.29860684082337363, w0=-0.10561262541836705, w1=-0.10554123521586777\n",
      "Gradient Descent(52/99): loss=0.2986067862910868, w0=-0.10561653645286408, w1=-0.10554393925983929\n",
      "Gradient Descent(53/99): loss=0.29860674163713585, w0=-0.1056200349688307, w1=-0.10554623202136357\n",
      "Gradient Descent(54/99): loss=0.2986067050327514, w0=-0.10562319494606805, w1=-0.10554818747858984\n",
      "Gradient Descent(55/99): loss=0.2986066749949876, w0=-0.10562607703953639, w1=-0.105549866284898\n",
      "Gradient Descent(56/99): loss=0.2986066503190157, w0=-0.10562873098075715, w1=-0.10555131817028804\n",
      "Gradient Descent(57/99): loss=0.29860663002439697, w0=-0.10563119754599315, w1=-0.10555258390954984\n",
      "Gradient Descent(58/99): loss=0.2986066133122244, w0=-0.10563351016950513, w1=-0.10555369693551124\n",
      "Gradient Descent(59/99): loss=0.29860659953079255, w0=-0.10563569626598676, w1=-0.105554684661466\n",
      "Gradient Descent(60/99): loss=0.29860658814798663, w0=-0.10563777831466936, w1=-0.10555556956527291\n",
      "Gradient Descent(61/99): loss=0.29860657872900653, w0=-0.10563977474809251, w1=-0.10555637007812162\n",
      "Gradient Descent(62/99): loss=0.29860657091834836, w0=-0.10564170068076512, w1=-0.10555710131318988\n",
      "Gradient Descent(63/99): loss=0.2986065644251919, w0=-0.10564356850658078, w1=-0.10555777566305581\n",
      "Gradient Descent(64/99): loss=0.2986065590115302, w0=-0.10564538838864027, w1=-0.1055584032895177\n",
      "Gradient Descent(65/99): loss=0.2986065544825108, w0=-0.10564716866086704, w1=-0.10555899252520749\n",
      "Gradient Descent(66/99): loss=0.2986065506785648, w0=-0.10564891615730666, w1=-0.10555955020288842\n",
      "Gradient Descent(67/99): loss=0.2986065474689867, w0=-0.10565063648213521, w1=-0.10556008192546197\n",
      "Gradient Descent(68/99): loss=0.2986065447466932, w0=-0.10565233423105524, w1=-0.10556059228736274\n",
      "Gradient Descent(69/99): loss=0.29860654242393936, w0=-0.10565401317283298, w1=-0.10556108505609459\n",
      "Gradient Descent(70/99): loss=0.29860654042881685, w0=-0.10565567639815397, w1=-0.10556156332108559\n",
      "Gradient Descent(71/99): loss=0.2986065387023894, w0=-0.10565732644168199, w1=-0.1055620296157462\n",
      "Gradient Descent(72/99): loss=0.2986065371963486, w0=-0.10565896538214509, w1=-0.10556248601755489\n",
      "Gradient Descent(73/99): loss=0.2986065358710945, w0=-0.10566059492440502, w1=-0.105562934230127\n",
      "Gradient Descent(74/99): loss=0.2986065346941635, w0=-0.1056622164667534, w1=-0.10556337565051067\n",
      "Gradient Descent(75/99): loss=0.2986065336389407, w0=-0.10566383115609432, w1=-0.10556381142436906\n",
      "Gradient Descent(76/99): loss=0.2986065326836064, w0=-0.10566543993319341, w1=-0.10556424249122927\n",
      "Gradient Descent(77/99): loss=0.2986065318102705, w0=-0.10566704356978224, w1=-0.10556466962158646\n",
      "Gradient Descent(78/99): loss=0.29860653100426476, w0=-0.10566864269898386, w1=-0.1055650934473293\n",
      "Gradient Descent(79/99): loss=0.29860653025356204, w0=-0.10567023784026167, w1=-0.10556551448668859\n",
      "Gradient Descent(80/99): loss=0.2986065295483014, w0=-0.10567182941987754, w1=-0.10556593316469544\n",
      "Gradient Descent(81/99): loss=0.2986065288803966, w0=-0.10567341778766738, w1=-0.10556634982995658\n",
      "Gradient Descent(82/99): loss=0.29860652824321743, w0=-0.10567500323079732, w1=-0.10556676476841062\n",
      "Gradient Descent(83/99): loss=0.2986065276313275, w0=-0.10567658598504363, w1=-0.10556717821460779\n",
      "Gradient Descent(84/99): loss=0.29860652704026824, w0=-0.10567816624404264, w1=-0.10556759036095985\n",
      "Gradient Descent(85/99): loss=0.29860652646638464, w0=-0.10567974416687506, w1=-0.10556800136532442\n",
      "Gradient Descent(86/99): loss=0.2986065259066787, w0=-0.10568131988428585, w1=-0.10556841135722471\n",
      "Gradient Descent(87/99): loss=0.2986065253586922, w0=-0.10568289350378415, w1=-0.10556882044294952\n",
      "Gradient Descent(88/99): loss=0.2986065248204095, w0=-0.1056844651138252, w1=-0.10556922870973509\n",
      "Gradient Descent(89/99): loss=0.29860652429017803, w0=-0.10568603478723979, w1=-0.10556963622919452\n",
      "Gradient Descent(90/99): loss=0.2986065237666415, w0=-0.1056876025840462, w1=-0.10557004306012975\n",
      "Gradient Descent(91/99): loss=0.29860652324868864, w0=-0.10568916855375615, w1=-0.10557044925083743\n",
      "Gradient Descent(92/99): loss=0.2986065227354079, w0=-0.10569073273726555, w1=-0.10557085484099968\n",
      "Gradient Descent(93/99): loss=0.29860652222605133, w0=-0.10569229516840514, w1=-0.10557125986323476\n",
      "Gradient Descent(94/99): loss=0.2986065217200061, w0=-0.10569385587521168, w1=-0.10557166434436821\n",
      "Gradient Descent(95/99): loss=0.298606521216769, w0=-0.1056954148809708, w1=-0.10557206830647567\n",
      "Gradient Descent(96/99): loss=0.29860652071592764, w0=-0.10569697220507158, w1=-0.10557247176773756\n",
      "Gradient Descent(97/99): loss=0.2986065202171438, w0=-0.10569852786370783, w1=-0.1055728747431402\n",
      "Gradient Descent(98/99): loss=0.29860651972013974, w0=-0.10570008187045281, w1=-0.10557327724505067\n",
      "Gradient Descent(99/99): loss=0.298606519224688, w0=-0.10570163423673071, w1=-0.10557367928368819\n",
      "Gradient Descent(100/99): loss=0.2986065187306014, w0=-0.10570318497220306, w1=-0.10557408086751058\n"
     ]
    }
   ],
   "source": [
    "w_reg_logreg, loss_reg_logreg = f.reg_logistic_regression(y_train_processed_logreg, tX_train, 0.01, initial_w, 100, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [-0.10570318 -0.10557408 -0.10551638 -0.10546428 -0.10539099 -0.10547108\n",
      " -0.10545289 -0.10547108 -0.10555219 -0.10543358 -0.10580189 -0.10591579\n",
      " -0.10548113 -0.10543551 -0.10544656 -0.10545656 -0.10594989 -0.10540836\n",
      " -0.10546825 -0.10540809 -0.10538928 -0.10540574] \n",
      " Loss =  0.2986065187306014 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.08830207079403295 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.0\n"
     ]
    }
   ],
   "source": [
    "y_test_reg_logreg = tX_test.dot(w_reg_logreg)\n",
    "y_test_reg_logreg = np.where(y_test_reg_logreg > 0.5, 1, 0)\n",
    "\n",
    "print('weights = \\n', w_reg_logreg,'\\n Loss = ', loss_reg_logreg,'\\n*****************************************************************************',\n",
    "        ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train == 1)/len(y_train), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_reg_logreg == 1)/len(y_test_reg_logreg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sub = np.where(y_test_reg_logreg == 1, 1, -1)\n",
    "h.create_csv_submission(test_ids, y_sub, 'submission_reg_logreg6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
