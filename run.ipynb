{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "try:\n",
    "    import importlib\n",
    "    importlib.reload(h)\n",
    "    importlib.reload(f)\n",
    "    importlib.reload(d)\n",
    "except NameError: # It hasn't been imported yet\n",
    "    import helpers as h\n",
    "    import implementations as f\n",
    "    import data_processing as d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing and feature selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this to work, the data folder needs to be one level above the project folder and the folder name needs\n",
    "#to be 'data'\n",
    "data_folder = '../data/'\n",
    "x_train, x_test, y_train, train_ids, test_ids = h.load_csv_data(data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the features names and remove the ID column\n",
    "features_name = np.genfromtxt('../data/x_train.csv', delimiter=',', dtype=str, max_rows=1)[1:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "one paper on internet suggests to use these features : \n",
    "\n",
    " _RFHYPE5, TOLDHI2, _CHOLCHK, _BMI5, SMOKE100, CVDSTRK3, DIABETE3, _TOTINDA, _FRTLT1, _VEGLT1, _RFDRHV5, HLTHPLN1, MEDCOST, GENHLTH, MENTHLTH, PHYSHLTH, DIFFWALK, SEX, _AGEG5YR, EDUCA, and INCOME2\n",
    "\n",
    " then, iterating through them, it removes the missing values, made the data binary when possible, removed the 'don't know, not sure', and ordinal (categorical) variables ares changed to 0,1,2,..., and renamed them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the important features\n",
    "features_list = ['_RFHYPE5', 'TOLDHI2', '_CHOLCHK', '_BMI5', 'SMOKE100', 'CVDSTRK3', 'DIABETE3', '_TOTINDA', '_FRTLT1', '_VEGLT1', '_RFDRHV5', \n",
    "                 'HLTHPLN1', 'MEDCOST', 'GENHLTH', 'MENTHLTH', 'PHYSHLTH', 'DIFFWALK', 'SEX', '_AGEG5YR', 'EDUCA', 'INCOME2']\n",
    "\n",
    "#Create a mask to filter the data\n",
    "mask = np.isin(features_name, features_list)\n",
    "\n",
    "x_train_featured = x_train[:, mask]\n",
    "x_test_featured = x_test[:, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257733, 21) (257733,) (109379, 21) (257733,)\n"
     ]
    }
   ],
   "source": [
    "#remove all missing values on X and remove corresponding lines in Y and ids\n",
    "\n",
    "x_train_featured_clean = x_train_featured[~np.isnan(x_train_featured).any(axis=1)]\n",
    "#x_test_featured_clean = x_test_featured[~np.isnan(x_test_featured).any(axis=1)]\n",
    "\n",
    "y_train_clean = y_train[~np.isnan(x_train_featured).any(axis=1)]\n",
    "\n",
    "train_ids_filtered = train_ids[~np.isnan(x_train_featured).any(axis=1)]\n",
    "#test_ids_filtered = test_ids[~np.isnan(x_test_featured).any(axis=1)]\n",
    "\n",
    "print(x_train_featured_clean.shape, y_train_clean.shape, x_test_featured.shape, train_ids_filtered.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to clean the data for each feature, making them binary for yes/no, etc... and rename them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "\n",
    "x_train_processed, y_train_processed, ids_train_processed = d.feature_processing(x_train_featured_clean, y_train_clean, train_ids_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data\n",
    "x_test_processed = d.feature_processing_test(x_test_featured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the preprocessing has been done, we can format the data to be used by the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_train = np.c_[np.ones((len(y_train_processed), 1)), x_train_processed]\n",
    "tX_test = np.c_[np.ones((len(x_test_featured), 1)), x_test_featured]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And then, we can run the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. MSE gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(499/500): loss=0.36610420165584884\n"
     ]
    }
   ],
   "source": [
    "#Compute gradient descent with MSE as loss function (see functions.py for the function)\n",
    "\n",
    "initial_w = [random.choice([1, -1]) for i in range(len(tX_train[0]))]\n",
    "\n",
    "w_mse_gd, loss_mse_gd = f.mean_squared_error_gd(y_train_processed, tX_train, initial_w, 500, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      "\n",
      " [ 0.9983649  -0.16200943  0.00430593  0.001176    0.79282177  0.74400033\n",
      "  0.31161574  0.76798885 -0.26295255 -0.30374897 -0.12552476 -0.02685832\n",
      " -0.7805071  -0.36720938 -0.54901789 -0.82418507 -0.01715525 -1.00861971\n",
      "  0.79307391  0.21966954  0.38087663 -0.56989337] \n",
      "\n",
      " Loss =  0.36610420165584884 \n",
      "\n",
      "*****************************************************************************  \n",
      "\n",
      " Train sample : \n",
      " Heart attack rate =  0.09400982168649387 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.04727598533539345\n"
     ]
    }
   ],
   "source": [
    "#Test the model on the test sample. Do we need to standardize ?\n",
    "\n",
    "y_test = tX_test.dot(w_mse_gd)\n",
    "y_test_rounded = np.where(y_test > 0, 1, -1) #not sure about this line\n",
    "\n",
    "print('weights = \\n\\n', w_mse_gd,'\\n\\n Loss = ', loss_mse_gd,'\\n\\n*****************************************************************************',\n",
    "      ' \\n\\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train_processed == 1)/len(y_train_processed), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_rounded == 1)/len(y_test_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. MSE SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 499/499: loss=1.9697986341411864, w0=1.1354000674791846, w1=-0.6413476775707898\n"
     ]
    }
   ],
   "source": [
    "w_mse_sgd, loss_mse_sgd = f.mean_squared_error_sgd(y_train_processed, tX_train, initial_w, 500, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [ 1.13540007 -0.64134768 -0.05683514  0.21212907  1.00469463  1.12510268\n",
      "  1.06869715  1.12562852 -0.74617025 -0.91480522 -0.28146503 -0.06965598\n",
      " -0.88484654 -0.89438242 -0.9100599  -0.87255593  0.2335261  -0.96389908\n",
      "  1.13267781  1.02203206  1.00867986 -0.96641201] \n",
      " Loss =  1.9697986341411864 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.09400982168649387 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.13274943087795646\n"
     ]
    }
   ],
   "source": [
    "y_test_sgd = tX_test.dot(w_mse_sgd)\n",
    "y_test_rounded_sgd = np.where(y_test_sgd > 0, 1, -1)\n",
    "\n",
    "print('weights = \\n', w_mse_sgd,'\\n Loss = ', loss_mse_sgd,'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train_processed == 1)/len(y_train_processed), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_rounded_sgd == 1)/len(y_test_rounded_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ls, loss_ls = f.least_squares(y_train_processed, tX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [-4.23458311e-01  6.69229048e-02  1.84255656e-03 -2.88677208e-04\n",
      " -1.72692968e-02 -1.45978253e-02 -7.62584533e-02 -3.73059100e-01\n",
      " -4.33297730e-02 -1.06050366e-01  1.03039949e-03 -6.91952059e-03\n",
      " -9.97705263e-02 -4.55439527e-02 -6.73076101e-02  2.69260672e-02\n",
      "  2.23087967e-02 -2.36533255e-01  3.86932707e-02 -6.06623971e-03\n",
      " -8.09010894e-03 -9.17165058e-03] \n",
      " Loss =  0.14512298742909366 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.09400982168649387 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.0014993737371890399\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_test_ls = tX_test.dot(w_ls)\n",
    "y_test_ls = np.where(y_test_ls > 0, 1, -1)\n",
    "\n",
    "print('weights = \\n', w_ls,'\\n Loss = ', loss_ls,'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train_processed == 1)/len(y_train_processed), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_ls == 1)/len(y_test_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ridge, loss_ridge = f.ridge_regression(y_train_processed, tX_train, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [-0.05446848  0.01909874  0.00402915 -0.00136054 -0.01866968 -0.03347632\n",
      " -0.07015045 -0.10700854 -0.06969889 -0.07622882 -0.04981145 -0.02171525\n",
      " -0.07341898 -0.04008595 -0.06586933 -0.03631186  0.01438435 -0.01918326\n",
      " -0.03391057 -0.01702109 -0.01232494 -0.01216396] \n",
      " Loss =  0.149955122105239 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.09400982168649387 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  1.8285045575476097e-05\n"
     ]
    }
   ],
   "source": [
    "y_test_ridge = tX_test.dot(w_ridge)\n",
    "y_test_ridge = np.where(y_test_ridge > 0, 1, -1)\n",
    "\n",
    "print('weights = \\n', w_ridge,'\\n Loss = ', loss_ridge,'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train_processed == 1)/len(y_train_processed), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_ridge == 1)/len(y_test_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_processed_logreg = np.where(y_train_processed == 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/199): loss=0.7334222943120227, w0=-0.040599017831350614, w1=-0.09388219228446126\n",
      "Gradient Descent(1/199): loss=0.5039173063536879, w0=-0.03159831712146977, w1=-0.06308501345033278\n",
      "Gradient Descent(2/199): loss=0.32940776728793036, w0=-0.023563175321223904, w1=-0.034601898395042616\n",
      "Gradient Descent(3/199): loss=0.32452908449651496, w0=-0.0202363658371515, w1=-0.019933155058361823\n",
      "Gradient Descent(4/199): loss=0.34103798151574394, w0=-0.024910102653780547, w1=-0.031202820035394305\n",
      "Gradient Descent(5/199): loss=0.3164223870335607, w0=-0.02091128744753009, w1=-0.013252799527350114\n",
      "Gradient Descent(6/199): loss=0.3339948863536741, w0=-0.026879583586654387, w1=-0.02700788358035513\n",
      "Gradient Descent(7/199): loss=0.3113204183989352, w0=-0.02311165613296598, w1=-0.009735638290876935\n",
      "Gradient Descent(8/199): loss=0.3283683288358732, w0=-0.029124308305221572, w1=-0.023647992108958425\n",
      "Gradient Descent(9/199): loss=0.3078863184776599, w0=-0.025412727385532043, w1=-0.006680211730566581\n",
      "Gradient Descent(10/199): loss=0.32544869161343665, w0=-0.03141401498295772, w1=-0.020830403075481962\n",
      "Gradient Descent(11/199): loss=0.30483845931345815, w0=-0.027640882460331476, w1=-0.0038160007905841398\n",
      "Gradient Descent(12/199): loss=0.3222860931038805, w0=-0.033576425862603704, w1=-0.017987988992574814\n",
      "Gradient Descent(13/199): loss=0.3024479125772916, w0=-0.029796852226111798, w1=-0.001074858201924954\n",
      "Gradient Descent(14/199): loss=0.3198754037569276, w0=-0.03568140624905693, w1=-0.01530514532830558\n",
      "Gradient Descent(15/199): loss=0.3003965548750508, w0=-0.03188238211753447, w1=0.001558560614706224\n",
      "Gradient Descent(16/199): loss=0.317659836996985, w0=-0.03770974852331917, w1=-0.012681788228975921\n",
      "Gradient Descent(17/199): loss=0.29866872262934074, w0=-0.033904418680162675, w1=0.004104897385641026\n",
      "Gradient Descent(18/199): loss=0.31576507864563086, w0=-0.03967925471768037, w1=-0.010142360736881465\n",
      "Gradient Descent(19/199): loss=0.2971664749495278, w0=-0.03586771975198728, w1=0.006576939051313381\n",
      "Gradient Descent(20/199): loss=0.314068195879943, w0=-0.041591740363340256, w1=-0.007664049978390368\n",
      "Gradient Descent(21/199): loss=0.2958501534888401, w0=-0.03777741080310209, w1=0.008986160464882664\n",
      "Gradient Descent(22/199): loss=0.31255703502688886, w0=-0.04345395980917527, w1=-0.0052440541482349536\n",
      "Gradient Descent(23/199): loss=0.2946781788062239, w0=-0.03963781206587345, w1=0.011341484427510297\n",
      "Gradient Descent(24/199): loss=0.31119220340878984, w0=-0.04526990065341373, w1=-0.0028740014750084156\n",
      "Gradient Descent(25/199): loss=0.29362318548623045, w0=-0.041452898706510956, w1=0.01365031474511326\n",
      "Gradient Descent(26/199): loss=0.3099527054178218, w0=-0.047043654997141034, w1=-0.0005487661276142794\n",
      "Gradient Descent(27/199): loss=0.29266299183024264, w0=-0.043226175568758074, w1=0.015918710168170664\n",
      "Gradient Descent(28/199): loss=0.30881850448114684, w0=-0.04877857853555085, w1=0.0017367082439253\n",
      "Gradient Descent(29/199): loss=0.29178091963665675, w0=-0.04496077772027113, w1=0.018151684262873207\n",
      "Gradient Descent(30/199): loss=0.3077744275038316, w0=-0.05047766370548381, w1=0.003986568981510931\n",
      "Gradient Descent(31/199): loss=0.29096389796111793, w0=-0.046659495695420404, w1=0.020353395204345707\n",
      "Gradient Descent(32/199): loss=0.3068078343972121, w0=-0.052143516747646564, w1=0.006204443421443866\n",
      "Gradient Descent(33/199): loss=0.2902017124460302, w0=-0.04832482191345035, w1=0.02252730371452653\n",
      "Gradient Descent(34/199): loss=0.3059084181593323, w0=-0.053778438229768355, w1=0.008393440014635144\n",
      "Gradient Descent(35/199): loss=0.2894862771578191, w0=-0.04995898575661888, w1=0.024676296456397982\n",
      "Gradient Descent(36/199): loss=0.3050676206090344, w0=-0.05538446255502071, w1=0.010556240094518166\n",
      "Gradient Descent(37/199): loss=0.2888111426202154, w0=-0.05156398651485291, w1=0.026802784897691143\n",
      "Gradient Descent(38/199): loss=0.3042783014045678, w0=-0.056963398368233414, w1=0.012695151141537814\n",
      "Gradient Descent(39/199): loss=0.28817111702692716, w0=-0.05314162172430697, w1=0.028908785092533935\n",
      "Gradient Descent(40/199): loss=0.30353445420369984, w0=-0.058516860474287606, w1=0.014812158231139155\n",
      "Gradient Descent(41/199): loss=0.2875619844620506, w0=-0.05469351196289591, w1=0.03099598255451874\n",
      "Gradient Descent(42/199): loss=0.3028309930718652, w0=-0.060046297010121596, w1=0.016908966577686903\n",
      "Gradient Descent(43/199): loss=0.28698029103366596, w0=-0.05622112233541085, w1=0.03306578542261908\n",
      "Gradient Descent(44/199): loss=0.3021635835653893, w0=-0.06155301214948054, w1=0.018987038451258438\n",
      "Gradient Descent(45/199): loss=0.2864231820834433, w0=-0.057725781100657306, w1=0.035119368317653554\n",
      "Gradient Descent(46/199): loss=0.30152850964963274, w0=-0.0630381852362834, w1=0.021047624979361558\n",
      "Gradient Descent(47/199): loss=0.2858882773340387, w0=-0.059208695812262645, w1=0.037157708744076395\n",
      "Gradient Descent(48/199): loss=0.3009225676853556, w0=-0.06450288693675917, w1=0.02309179371656775\n",
      "Gradient Descent(49/199): loss=0.285373574564675, w0=-0.06067096731707659, w1=0.03918161747148603\n",
      "Gradient Descent(50/199): loss=0.3003429814106146, w0=-0.06594809292595114, w1=0.025120452598959463\n",
      "Gradient Descent(51/199): loss=0.2848773748461739, w0=-0.06211360191062937, w1=0.041191764019049254\n",
      "Gradient Descent(52/199): loss=0.29978733325240015, w0=-0.06737469552481394, w1=0.02713437081079538\n",
      "Gradient Descent(53/199): loss=0.28439822417608834, w0=-0.0635375219094096, w1=0.04318869812874708\n",
      "Gradient Descent(54/199): loss=0.29925350844876014, w0=-0.06878351363031776, w1=0.029134196996569273\n",
      "Gradient Descent(55/199): loss=0.28393486766165993, w0=-0.0649435748630408, w1=0.04517286793197358\n",
      "Gradient Descent(56/199): loss=0.2987396492896656, w0=-0.07017530121933285, w1=0.031120475181345225\n",
      "Gradient Descent(57/199): loss=0.28348621335737917, w0=-0.06633254159715882, w1=0.04714463537392581\n",
      "Gradient Descent(58/199): loss=0.29824411740630574, w0=-0.07155075465716364, w1=0.033093658704900025\n",
      "Gradient Descent(59/199): loss=0.2830513035708537, w0=-0.06770514324978465, w1=0.04910428935119897\n",
      "Gradient Descent(60/199): loss=0.29776546250484753, w0=-0.07291051900096251, w1=0.03505412242880912\n",
      "Gradient Descent(61/199): loss=0.28262929197542747, w0=-0.0690620474399718, w1=0.05105205693250293\n",
      "Gradient Descent(62/199): loss=0.29730239629370875, w0=-0.07425519345518133, w1=0.03700217343757113\n",
      "Gradient Descent(63/199): loss=0.2822194252598636, w0=-0.07040387368705, w1=0.052988112964878556\n",
      "Gradient Descent(64/199): loss=0.29685377062233387, w0=-0.07558533610926109, w1=0.03893806042342504\n",
      "Gradient Descent(65/199): loss=0.28182102833979683, w0=-0.07173119818142669, w1=0.054912588314076485\n",
      "Gradient Descent(66/199): loss=0.2964185590556966, w0=-0.07690146806575056, w1=0.040861981918313244\n",
      "Gradient Descent(67/199): loss=0.28143349237809573, w0=-0.07304455799320685, w1=0.056825576944760724\n",
      "Gradient Descent(68/199): loss=0.2959958412678964, w0=-0.07820407704903015, w1=0.04277409351444216\n",
      "Gradient Descent(69/199): loss=0.2810562650302096, w0=-0.07434445479244721, w1=0.05872714201155564\n",
      "Gradient Descent(70/199): loss=0.29558478976179303, w0=-0.07949362057003756, w1=0.04467451419629226\n",
      "Gradient Descent(71/199): loss=0.28068884245958864, w0=-0.07563135814432607, w1=0.060617321103878075\n",
      "Gradient Descent(72/199): loss=0.2951846585181272, w0=-0.0807705287102357, w1=0.046563331891109935\n",
      "Gradient Descent(73/199): loss=0.2803307627672345, w0=-0.07690570843358188, w1=0.0624961307646104\n",
      "Gradient Descent(74/199): loss=0.29479477325342346, w0=-0.08203520657803276, w1=0.04844060833137609\n",
      "Gradient Descent(75/199): loss=0.27998160055573207, w0=-0.07816791946499878, w1=0.06436357038390927\n",
      "Gradient Descent(76/199): loss=0.2944145230258578, w0=-0.0832880364825665, w1=0.05030638331111748\n",
      "Gradient Descent(77/199): loss=0.27964096240718445, w0=-0.07941838078027601, w1=0.0662196255539805\n",
      "Gradient Descent(78/199): loss=0.2940433529758803, w0=-0.08452937986287809, w1=0.05216067840788247\n",
      "Gradient Descent(79/199): loss=0.2793084831004095, w0=-0.0806574597261365, w1=0.06806427095784669\n",
      "Gradient Descent(80/199): loss=0.2936807580264004, w0=-0.08575957900477169, w1=0.05400350023349582\n",
      "Gradient Descent(81/199): loss=0.27898382242860226, w0=-0.08188550330384896, w1=0.06989747285446522\n",
      "Gradient Descent(82/199): loss=0.29332627739785067, w0=-0.08697895857286807, w1=0.05583484326914477\n",
      "Gradient Descent(83/199): loss=0.2786666625067698, w0=-0.0831028398263394, w1=0.07171919121363632\n",
      "Gradient Descent(84/199): loss=0.2929794898180805, w0=-0.08818782698135609, w1=0.05765469233374419\n",
      "Gradient Descent(85/199): loss=0.2783567054803469, w0=-0.08430978040563887, w1=0.07352938154664328\n",
      "Gradient Descent(86/199): loss=0.29264000932701706, w0=-0.08938647762358029, w1=0.05946302472875864\n",
      "Gradient Descent(87/199): loss=0.27805367156383887, w0=-0.08550662029047268, w1=0.07532799647223856\n",
      "Gradient Descent(88/199): loss=0.2923074815922931, w0=-0.09057518997776723, w1=0.06125981209760458\n",
      "Gradient Descent(89/199): loss=0.2777572973521492, w0=-0.08669364007126595, w1=0.07711498705222444\n",
      "Gradient Descent(90/199): loss=0.29198158066537605, w0=-0.09175423060379904, w1=0.06304502203331357\n",
      "Gradient Descent(91/199): loss=0.277467334358236, w0=-0.0878711067676568, w1=0.07889030392631649\n",
      "Gradient Descent(92/199): loss=0.2916620061187133, w0=-0.0929238540439136, w1=0.06481861946423623\n",
      "Gradient Descent(93/199): loss=0.2771835477394669, w0=-0.0890392748117263, w1=0.08065389827207534\n",
      "Gradient Descent(94/199): loss=0.291348480513431, w0=-0.09408430363848488, w1=0.06658056784412762\n",
      "Gradient Descent(95/199): loss=0.27690571518205664, w0=-0.0901983869385234, w1=0.08240572261235188\n",
      "Gradient Descent(96/199): loss=0.2910407471546847, w0=-0.0952358122665703, w1=0.06833083016991917\n",
      "Gradient Descent(97/199): loss=0.27663362591855833, w0=-0.0913486749940495, w1=0.08414573148982017\n",
      "Gradient Descent(98/199): loss=0.29073856809798965, w0=-0.0963786030196539, w1=0.0700693698478129\n",
      "Gradient Descent(99/199): loss=0.2763670798578981, w0=-0.09249036066964159, w1=0.08587388202568834\n",
      "Gradient Descent(100/199): loss=0.2904417223751107, w0=-0.09751288981594046, w1=0.07179615142596285\n",
      "Gradient Descent(101/199): loss=0.27610588681108206, w0=-0.09362365617062479, w1=0.08759013437753924\n",
      "Gradient Descent(102/199): loss=0.29015000441252253, w0=-0.09863887796163373, w1=0.07351114120991595\n",
      "Gradient Descent(103/199): loss=0.2758498657986209, w0=-0.09474876482617604, w1=0.08929445210938651\n",
      "Gradient Descent(104/199): loss=0.289863222619099, w0=-0.0997567646648312, w1=0.07521430777514296\n",
      "Gradient Descent(105/199): loss=0.27559884442814075, w0=-0.09586588164652884, w1=0.09098680248542244\n",
      "Gradient Descent(106/199): loss=0.2895811981229181, w0=-0.10086673950698974, w1=0.0769056223893377\n",
      "Gradient Descent(107/199): loss=0.27535265833255046, w0=-0.09697519383294087, w1=0.09266715669751695\n",
      "Gradient Descent(108/199): loss=0.28930376363966465, w0=-0.1019689848763155, w1=0.07858505935571959\n",
      "Gradient Descent(109/199): loss=0.2751111506607397, w0=-0.09807688124522468, w1=0.09433549003530352\n",
      "Gradient Descent(110/199): loss=0.2890307624574279, w0=-0.1030636763669211, w1=0.08025259628728779\n",
      "Gradient Descent(111/199): loss=0.2748741716140739, w0=-0.09917111683109932, w1=0.09599178200660818\n",
      "Gradient Descent(112/199): loss=0.28876204752463375, w0=-0.10415098314714435, w1=0.08190821432082437\n",
      "Gradient Descent(113/199): loss=0.27464157802300665, w0=-0.10025806702114191, w1=0.09763601641503679\n",
      "Gradient Descent(114/199): loss=0.28849748062950537, w0=-0.10523106830003377, w1=0.08355189827844321\n",
      "Gradient Descent(115/199): loss=0.274413232959014, w0=-0.10133789209270007, w1=0.09926818140070887\n",
      "Gradient Descent(116/199): loss=0.2882369316608985, w0=-0.10630408913866578, w1=0.08518363678357337\n",
      "Gradient Descent(117/199): loss=0.2741890053777674, w0=-0.10241074650575582, w1=0.10088826944939808\n",
      "Gradient Descent(118/199): loss=0.2879802779415828, w0=-0.10737019749866211, w1=0.0868034223374738\n",
      "Gradient Descent(119/199): loss=0.27396876979007184, w0=-0.103476779213408, w1=0.10249627737470185\n",
      "Gradient Descent(120/199): loss=0.2877274036261302, w0=-0.10842954001001563, w1=0.08841125136166428\n",
      "Gradient Descent(121/199): loss=0.273752405957578, w0=-0.10453613394935114, w1=0.10409220627730015\n",
      "Gradient Descent(122/199): loss=0.2874781991564778, w0=-0.10948225835010324, w1=0.09000712421103538\n",
      "Gradient Descent(123/199): loss=0.2735397986107086, w0=-0.10558894949447761, w1=0.10567606148486758\n",
      "Gradient Descent(124/199): loss=0.2872325607690729, w0=-0.11052848947956588, w1=0.09159104516183364\n",
      "Gradient Descent(125/199): loss=0.2733308371865693, w0=-0.10663535992450385, w1=0.10724785247576478\n",
      "Gradient Descent(126/199): loss=0.28699039004817284, w0=-0.11156836586255639, w1=0.09316302237823185\n",
      "Gradient Descent(127/199): loss=0.2731254155849192, w0=-0.10767549484032479, w1=0.1088075927892495\n",
      "Gradient Descent(128/199): loss=0.2867515935205383, w0=-0.11260201567270237, w1=0.09472306786074586\n",
      "Gradient Descent(129/199): loss=0.2729234319405133, w0=-0.10870947958262332, w1=0.11035529992460899\n",
      "Gradient Descent(130/199): loss=0.28651608228724845, w0=-0.11362956298599092, w1=0.09627119737937892\n",
      "Gradient Descent(131/199): loss=0.2727247884103414, w0=-0.10973743543210664, w1=0.1118909952313139\n",
      "Gradient Descent(132/199): loss=0.286283771688873, w0=-0.11465112796166252, w1=0.09780743039401814\n",
      "Gradient Descent(133/199): loss=0.27252939097446266, w0=-0.11075947979660256, w1=0.1134147037920257\n",
      "Gradient Descent(134/199): loss=0.28605458100064113, w0=-0.11566682701209113, w1=0.0993317899643066\n",
      "Gradient Descent(135/199): loss=0.2723371492492858, w0=-0.11177572638612507, w1=0.11492645430005954\n",
      "Gradient Descent(136/199): loss=0.28582843315461565, w0=-0.11667677296253291, w1=0.10084430265094085\n",
      "Gradient Descent(137/199): loss=0.2721479763122742, w0=-0.11278628537690957, w1=0.11642627893269357\n",
      "Gradient Descent(138/199): loss=0.2856052544861934, w0=-0.11768107520153992, w1=0.10234499841009792\n",
      "Gradient Descent(139/199): loss=0.2719617885371672, w0=-0.11379126356532016, w1=0.11791421322153288\n",
      "Gradient Descent(140/199): loss=0.2853849745025615, w0=-0.1186798398227595, w1=0.10383391048248636\n",
      "Gradient Descent(141/199): loss=0.27177850543890075, w0=-0.11479076451244338, w1=0.11939029592097822\n",
      "Gradient Descent(142/199): loss=0.2851675256709644, w0=-0.11967316975877168, w1=0.10531107527832237\n",
      "Gradient Descent(143/199): loss=0.271598049527501, w0=-0.11578488868010679, w1=0.12085456887570112\n",
      "Gradient Descent(144/199): loss=0.2849528432248835, w0=-0.12066116490755781, w1=0.10677653225936255\n",
      "Gradient Descent(145/199): loss=0.2714203461702846, w0=-0.11677373355898908, w1=0.12230707688790404\n",
      "Gradient Descent(146/199): loss=0.28474086498641415, w0=-0.12164392225213717, w1=0.10823032381897918\n",
      "Gradient Descent(147/199): loss=0.2712453234617798, w0=-0.11775739378942734, w1=0.12374786758503559\n",
      "Gradient Descent(148/199): loss=0.2845315312033079, w0=-0.12262153597386237, w1=0.1096724951611286\n",
      "Gradient Descent(149/199): loss=0.2710729121008175, w0=-0.11873596127547158, w1=0.1251769912885259\n",
      "Gradient Descent(150/199): loss=0.28432478439930703, w0=-0.12359409755981934, w1=0.1111030941789469\n",
      "Gradient Descent(151/199): loss=0.27090304527430664, w0=-0.11970952529268518, w1=0.1265945008840267\n",
      "Gradient Descent(152/199): loss=0.28412056923652657, w0=-0.12456169590473848, w1=0.11252217133360454\n",
      "Gradient Descent(153/199): loss=0.27073565854724885, w0=-0.12067817259014654, w1=0.12800045169356275\n",
      "Gradient Descent(154/199): loss=0.2839188323887897, w0=-0.12552441740779066, w1=0.113929779533958\n",
      "Gradient Descent(155/199): loss=0.27057068975857096, w0=-0.12164198748706606, w1=0.12939490134993414\n",
      "Gradient Descent(156/199): loss=0.2837195224248902, w0=-0.1264823460646069, w1=0.1153259740174607\n",
      "Gradient Descent(157/199): loss=0.2704080789224127, w0=-0.12260105196439643, w1=0.13077790967364877\n",
      "Gradient Descent(158/199): loss=0.2835225897009079, w0=-0.1274355635548352, w1=0.1167108122327164\n",
      "Gradient Descent(159/199): loss=0.27024776813451223, w0=-0.12355544575178146, w1=0.13214953855261613\n",
      "Gradient Descent(160/199): loss=0.28332798626073996, w0=-0.1283841493255198, w1=0.11808435372400493\n",
      "Gradient Descent(161/199): loss=0.2700897014833807, w0=-0.12450524641015945, w1=0.13350985182478436\n",
      "Gradient Descent(162/199): loss=0.283135665744137, w0=-0.12932818067056748, w1=0.1194466600180466\n",
      "Gradient Descent(163/199): loss=0.2699338249659664, w0=-0.12545052941031032, w1=0.13485891516386758\n",
      "Gradient Descent(164/199): loss=0.2829455833015645, w0=-0.1302677328065417, w1=0.12079779451322913\n",
      "Gradient Descent(165/199): loss=0.2697800864075386, w0=-0.12639136820761157, w1=0.13619679596827072\n",
      "Gradient Descent(166/199): loss=0.28275769551530305, w0=-0.13120287894500904, w1=0.12213782237147512\n",
      "Gradient Descent(167/199): loss=0.2696284353855418, w0=-0.12732783431324712, w1=0.13752356325329385\n",
      "Gradient Descent(168/199): loss=0.28257196032624304, w0=-0.1321336903616433, w1=0.1234668104128919\n",
      "Gradient Descent(169/199): loss=0.26947882315718297, w0=-0.12825999736209243, w1=0.13883928754666758\n",
      "Gradient Descent(170/199): loss=0.28238833696588167, w0=-0.1330602364622775, w1=0.12478482701331375\n",
      "Gradient Descent(171/199): loss=0.2693312025905411, w0=-0.12918792517748212, w1=0.14014404078745457\n",
      "Gradient Descent(172/199): loss=0.2822067858930863, w0=-0.13398258484608047, w1=0.12609194200481727\n",
      "Gradient Descent(173/199): loss=0.26918552809899177, w0=-0.13011168383305022, w1=0.14143789622832426\n",
      "Gradient Descent(174/199): loss=0.2820272687352142, w0=-0.13490080136602042, w1=0.127388226579265\n",
      "Gradient Descent(175/199): loss=0.2690417555787626, w0=-0.13103133771181835, w1=0.142720928341199\n",
      "Gradient Descent(176/199): loss=0.28184974823323383, w0=-0.1358149501867675, w1=0.1286737531949141\n",
      "Gradient Descent(177/199): loss=0.26889984234944564, w0=-0.13194694956269332, w1=0.1439932127262511\n",
      "Gradient Descent(178/199): loss=0.2816741881905096, w0=-0.1367250938401753, w1=0.12994859548610466\n",
      "Gradient Descent(179/199): loss=0.2687597470972991, w0=-0.13285858055452449, w1=0.14525482602421935\n",
      "Gradient Descent(180/199): loss=0.28150055342495384, w0=-0.13763129327847215, w1=0.13121282817602928\n",
      "Gradient Descent(181/199): loss=0.2686214298211896, w0=-0.1337662903278594, w1=0.14650584583200188\n",
      "Gradient Descent(182/199): loss=0.28132880972427354, w0=-0.13853360792528388, w1=0.13246652699256892\n",
      "Gradient Descent(183/199): loss=0.26848485178103154, w0=-0.13467013704452618, w1=0.14774635062147695\n",
      "Gradient Descent(184/199): loss=0.28115892380406377, w0=-0.13943209572460152, w1=0.13370976858717237\n",
      "Gradient Descent(185/199): loss=0.2683499754485888, w0=-0.1355701774351627, w1=0.14897641966149192\n",
      "Gradient Descent(186/199): loss=0.28099086326852146, w0=-0.14032681318779952, w1=0.13494263045674207\n",
      "Gradient Descent(187/199): loss=0.26821676446051324, w0=-0.1364664668448028, w1=0.15019613294295742\n",
      "Gradient Descent(188/199): loss=0.28082459657357345, w0=-0.1412178154388033, w1=0.13616519086848392\n",
      "Gradient Descent(189/199): loss=0.26808518357350525, w0=-0.13735905927662348, w1=0.1514055711069768\n",
      "Gradient Descent(190/199): loss=0.2806600929922344, w0=-0.14210515625749867, w1=0.13737752878767126\n",
      "Gradient Descent(191/199): loss=0.26795519862148093, w0=-0.138248007433949, w1=0.15260481537594064\n",
      "Gradient Descent(192/199): loss=0.28049732258202187, w0=-0.14298888812146993, w1=0.1385797238082658\n",
      "Gradient Descent(193/199): loss=0.267826776474648, w0=-0.13913336276060215, w1=0.1537939474875092\n",
      "Gradient Descent(194/199): loss=0.2803362561542706, w0=-0.14386906224614696, w1=0.13977185608633635\n",
      "Gradient Descent(195/199): loss=0.2676998850003894, w0=-0.1400151754796858, w1=0.15497304963140485\n",
      "Gradient Descent(196/199): loss=0.28017686524521124, w0=-0.14474572862343862, w1=0.14095400627620752\n",
      "Gradient Descent(197/199): loss=0.26757449302586245, w0=-0.1408934946308739, w1=0.15614220438893672\n",
      "Gradient Descent(198/199): loss=0.28001912208867386, w0=-0.14561893605892243, w1=0.14212625546927535\n",
      "Gradient Descent(199/199): loss=0.26745057030222974, w0=-0.14176836810628446, w1=0.15730149467517598\n"
     ]
    }
   ],
   "source": [
    "w_logreg, loss_logreg = f.logistic_regression(y_train_processed_logreg, tX_train,np.zeros(22),200, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [-0.14176837  0.15730149  0.03346908  0.00515173 -0.03886945 -0.11844364\n",
      " -0.31634258 -0.27480137 -0.33766603 -0.31785316 -0.29703101 -0.10415193\n",
      " -0.22435663 -0.20535771 -0.30740196 -0.10683725  0.17908248 -0.03747371\n",
      " -0.11498309 -0.04570317 -0.03555813 -0.0252816 ] \n",
      " Loss =  0.26745057030222974 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.09400982168649387 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.009160807833313524\n"
     ]
    }
   ],
   "source": [
    "y_test_logreg = tX_test.dot(w_logreg)\n",
    "y_test_logreg = np.where(y_test_logreg > 0.5, 1, 0)\n",
    "\n",
    "print('weights = \\n', w_logreg,'\\n Loss = ', loss_logreg,'\\n*****************************************************************************',\n",
    "        ' \\n Train sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_train_processed == 1)/len(y_train_processed), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.count_nonzero(y_test_logreg == 1)/len(y_test_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
