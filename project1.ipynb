{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import importlib\n",
    "    importlib.reload(h)\n",
    "except NameError: # It hasn't been imported yet\n",
    "    import helpers as h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing and feature selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.nanmean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.nanstd(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "def build_data(input, output):\n",
    "    \"\"\"build the data such as we have the wanted format :\n",
    "    input = matrix of features\n",
    "    output = vector of output\n",
    "    return the matrix of features with a columns of ones added\"\"\"\n",
    "    # Standardize the data\n",
    "    input, mean_x, std_x = standardize(input)\n",
    "    # Add a column of ones\n",
    "    input = np.c_[np.ones((input.shape[0], 1)), input]\n",
    "    return input, output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "one paper on internet suggests to use these features : \n",
    "\n",
    " _RFHYPE5, TOLDHI2, _CHOLCHK, _BMI5, SMOKE100, CVDSTRK3, DIABETE3, _TOTINDA, _FRTLT1, _VEGLT1, _RFDRHV5, HLTHPLN1, MEDCOST, GENHLTH, MENTHLTH, PHYSHLTH, DIFFWALK, SEX, _AGEG5YR, EDUCA, and INCOME2\n",
    "\n",
    " then, iterating through them, it removes the missing values, made the data binary when possible, removed the 'don't know, not sure', and ordinal (categorical) variables ares changed to 0,1,2,..., and renamed them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y, tx, w):\n",
    "    \"\"\"compute the loss by mse.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    mse = e.dot(e)/(2*len(y))\n",
    "    return mse\n",
    "\n",
    "def compute_gradient_mse(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    grad =-1/(len(y)) * tx.T.dot(e)\n",
    "    return grad\n",
    "\n",
    "def mean_squared_error_gd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\" gradient descent algorithm using mean squared error as the loss function \n",
    "        y = output vector\n",
    "        tx = input matrix\n",
    "        initial_w = initial weights\n",
    "        max_iters = maximum number of iterations\n",
    "        gamma = learning rate\"\"\"\n",
    "        \n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient_mse(y, tx, w)\n",
    "        loss = compute_mse(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "            bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "    return loss, w\n",
    "\n",
    "def compute_y_test(x_test, w):\n",
    "    \"\"\"compute the output vector y_test\"\"\"\n",
    "    x_test_std = standardize(x_test)[0]\n",
    "    y_test = x_test_std.dot(w)\n",
    "    #y_test_abs_rounded = np.where(np.abs(y_test) > 0.5, 1, 0)\n",
    "    print('the number of heart attack predicted in the test sample are :', np.sum(y_test), 'out of', len(y_test), 'samples', 'which is', np.sum(y_test)/len(y_test)*100, '%')\n",
    "    return y_test\n",
    "\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"Generate a minibatch iterator for a dataset.\"\"\"\n",
    "    data_size = len(y)\n",
    "    indices = np.arange(data_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            batch_indices = indices[start_index:end_index]\n",
    "            yield y[batch_indices], tx[batch_indices]\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(e) / len(y)\n",
    "    return grad\n",
    "\n",
    "def mean_squared_error_sgd(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\" stochastic gradient descent algorithm using mean squared error as the loss function \n",
    "        y = output vector\n",
    "        tx = input matrix\n",
    "        initial_w = initial weights\n",
    "        max_iters = maximum number of iterations\n",
    "        gamma = learning rate\"\"\"\n",
    "    \n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size=1):\n",
    "            gradient = compute_stoch_gradient(minibatch_y, minibatch_tx, w)\n",
    "            loss = compute_mse(minibatch_y, minibatch_tx, w)\n",
    "            w = w - gamma * gradient\n",
    "            print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return loss, w\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    return loss, w\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_mse(y, tx, w)\n",
    "    return loss, w\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\n",
    "\n",
    "    Args:\n",
    "        x: numpy array of shape (N,), N is the number of samples.\n",
    "        degree: integer.\n",
    "\n",
    "    Returns:\n",
    "        poly: numpy array of shape (N,d+1)\n",
    "\n",
    "    >>> build_poly(np.array([0.0, 1.5]), 2)\n",
    "    array([[1.  , 0.  , 0.  ],\n",
    "           [1.  , 1.5 , 2.25]])\n",
    "    \"\"\"\n",
    "    poly_x = np.zeros((len(x), degree+1))\n",
    "    for i, z in enumerate(x):\n",
    "        for j in range(degree+1):\n",
    "            poly_x[i,j] = z**j\n",
    "\n",
    "    return poly_x\n",
    "\n",
    "def poly_ridge_ls(y,tx,lambda_=0,ridge_y=False,ls_y = False, degree = 1):\n",
    "    if ridge_y == False and ls_y == False:\n",
    "        print('no algorithm has been chosen')\n",
    "        return\n",
    "    elif ridge_y == True and ls_y == True:\n",
    "        print('both algorithm have been chosen')\n",
    "        return\n",
    "        \n",
    "    elif ls_y == True and ridge_y == False:\n",
    "        tx_poly = build_poly(tx, degree)\n",
    "        weights, mse = least_squares(y, tx_poly)\n",
    "        rmse = np.sqrt(2*mse)\n",
    "        return rmse, weights\n",
    "    \n",
    "    elif ls_y == False and ridge_y == True:\n",
    "        tx_poly = build_poly(tx, degree)\n",
    "        weights, mse = ridge_regression(y, tx_poly, lambda_)\n",
    "        rmse = np.sqrt(2*mse)\n",
    "        return rmse, weights\n",
    "    \n",
    "def compute_loss_logistic(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    pred = tx.dot(w)\n",
    "    loss = np.sum(np.log(1 + np.exp(pred))) - y.T.dot(pred)\n",
    "    return loss\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "def compute_gradient_logistic(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    pred = tx.dot(w)\n",
    "    gradient = tx.T.dot(sigmoid(pred) - y)\n",
    "    return gradient\n",
    "    \n",
    "def logistic_regression(y, x, max_iter, gamma, initial_w):\n",
    "    \"\"\"calculate the loss and the weights using logistic regression.\n",
    "        Args : \n",
    "        x = input matrix of the training set (N,D) where N is the number of samples and D the number of features\n",
    "        y = output vector of the training set(N,) where N is the number of samples\n",
    "        max_iter = maximum number of iterations\n",
    "        gamma = learning rate\n",
    "        initial_w = initial weights\n",
    "        return :\n",
    "        loss = loss of the logistic regression\n",
    "        w = weights of the logistic regression\"\"\"\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iter):\n",
    "        loss = compute_loss_logistic(y, x, w)\n",
    "        gradient = compute_gradient_logistic(y, x, w)\n",
    "        w = w - gamma * gradient\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "            bi=n_iter, ti=max_iter - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return loss, w\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.genfromtxt('x_train.csv', delimiter=',', skip_header=1, filling_values=np.nan)\n",
    "features_name = np.genfromtxt('x_train.csv', delimiter=',', dtype=str, max_rows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.genfromtxt('x_test.csv', delimiter=',', skip_header=1, filling_values=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.genfromtxt('y_train.csv', delimiter=',', skip_header=1, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = np.array([i for i in range(len(Y))])\n",
    "ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the important features\n",
    "features_list = ['_RFHYPE5', 'TOLDHI2', '_CHOLCHK', '_BMI5', 'SMOKE100', 'CVDSTRK3', 'DIABETE3', '_TOTINDA', '_FRTLT1', '_VEGLT1', '_RFDRHV5', \n",
    "                 'HLTHPLN1', 'MEDCOST', 'GENHLTH', 'MENTHLTH', 'PHYSHLTH', 'DIFFWALK', 'SEX', '_AGEG5YR', 'EDUCA', 'INCOME2']\n",
    "\n",
    "\n",
    "mask = np.isin(features_name, features_list)\n",
    "\n",
    "x_featured = x[:, mask]\n",
    "x_test_featured = x_test[:, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257733, 21) (257733,) (85873, 21) (328135,)\n"
     ]
    }
   ],
   "source": [
    "#remove all missing values\n",
    "x_featured_clean = x_featured[~np.isnan(x_featured).any(axis=1)]\n",
    "x_test_featured_clean = x_test_featured[~np.isnan(x_test_featured).any(axis=1)]\n",
    "y_clean = Y[~np.isnan(x_featured).any(axis=1)]\n",
    "ids_1 = ids[~np.isnan(x_featured).any(axis=1)]\n",
    "print(x_featured_clean.shape, y_clean.shape, x_test_featured_clean.shape, ids.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to clean the data for each feature, making them binary for yes/no, etc... and rename them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_processing(x_featured_clean_1, y_clean, ids):\n",
    "    \"\"\"takes as input the matrix with no missing values and the output vector (also with rows removed) and returns the matrix with the features processed)\"\"\"\n",
    "\n",
    "    #1 GenHealth : ordinal variable, 1 = very good physical condition -> 5 = very bad physical condition. 7/9 = missing values that need to be dropped\n",
    "\n",
    "    #remove all missing values\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 0] != 7] \n",
    "    ids = ids[x_featured_clean_1[:, 0] != 7]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 0] != 7]\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 0] != 9]\n",
    "    ids = ids[x_featured_clean_1[:, 0] != 9]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 0] != 9]\n",
    "\n",
    "    print(x_featured_clean_1.shape, y_clean.shape)\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #2 PHYSHLTH : in days, scale = 0-30. 77/99 = missing values that need to be dropped\n",
    "    #change 88 to 0\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 1] == 88, 1] = 0\n",
    "\n",
    "    #remove 77 and 99\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 1] != 77]\n",
    "    ids = ids[x_featured_clean_1[:, 1] != 77]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 1] != 77]\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 1] != 99]\n",
    "    ids = ids[x_featured_clean_1[:, 1] != 99]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 1] != 99]\n",
    "    print(x_featured_clean_1.shape, y_clean.shape)\n",
    "\n",
    "    ###################################################\n",
    "    #3 MENTHLTH same as PHYSHLTH\n",
    "    #change 88 to 0\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 2] == 88, 2] = 0\n",
    "\n",
    "    #remove 77 and 99\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 2] != 77]\n",
    "    ids = ids[x_featured_clean_1[:, 2] != 77]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 2] != 77]\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 2] != 99]\n",
    "    ids = ids[x_featured_clean_1[:, 2] != 99]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 2] != 99]\n",
    "    print(x_featured_clean_1.shape, y_clean.shape)\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #4 HLTHPLN1 : health care access : change 1 (yes) to 0 and 2 (no) to 1, and remove 9 and 7\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 3] == 1, 3] = 0\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 3] == 2, 3] = 1\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 3] != 9]\n",
    "    ids = ids[x_featured_clean_1[:, 3] != 9]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 3] != 9]\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 3] != 7]\n",
    "    ids = ids[x_featured_clean_1[:, 3] != 7]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 3] != 7]\n",
    "    print(x_featured_clean_1.shape, y_clean.shape)\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #5 MEDCOST : binary variable if medical care was delayed due to cost, change 1 (yes) to 0 and 2 (no) to 1, and remove 9 and 7\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 4] == 1, 4] = 0\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 4] == 2, 4] = 1\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 4] != 9]\n",
    "    ids = ids[x_featured_clean_1[:, 4] != 9]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 4] != 9]\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 4] != 7]\n",
    "    ids = ids[x_featured_clean_1[:, 4] != 7]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 4] != 7]\n",
    "    print(x_featured_clean_1.shape, y_clean.shape)\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #6 : TOLDHI2 high blood cholesterol binary variable : change 1 (yes) to 0 and 2 (no) to 1, and remove 9 and 7\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 5] == 1, 5] = 0\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 5] == 2, 5] = 1\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 5] != 9]\n",
    "    ids = ids[x_featured_clean_1[:, 5] != 9]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 5] != 9]\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 5] != 7]\n",
    "    ids = ids[x_featured_clean_1[:, 5] != 7]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 5] != 7]\n",
    "    print(x_featured_clean_1.shape, y_clean.shape)\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #7 CVDSTRK3 ever had a stroke binary variable : change 1 (yes) to 0 and 2 (no) to 1, and remove 9 and 7\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 6] == 1, 6] = 0\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 6] == 2, 6] = 1\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 6] != 9]\n",
    "    ids = ids[x_featured_clean_1[:, 6] != 9]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 6] != 9]\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 6] != 7]\n",
    "    ids = ids[x_featured_clean_1[:, 6] != 7]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 6] != 7]\n",
    "    print(x_featured_clean_1.shape, y_clean.shape)\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #8 : DIABETE3 : ever had diabete : yes (1) or during pregnancy (2) to 0, pre-diabete or borderline (4) to 1, no (3) to 2, and remove 9 and 7\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 7] == 1, 7] = 0\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 7] == 2, 7] = 0\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 7] == 4, 7] = 1\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 7] == 3, 7] = 2\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 7] != 9]\n",
    "    ids = ids[x_featured_clean_1[:, 7] != 9]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 7] != 9]\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 7] != 7]\n",
    "    ids = ids[x_featured_clean_1[:, 7] != 7]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 7] != 7]\n",
    "    x_featured_clean_1.shape\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #9 : SEX : male (1) to 0 and female (2) to 1\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 8] == 1, 8] = 0\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 8] == 2, 8] = 1\n",
    "    x_featured_clean_1.shape\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #10 : EDUCA ordinal variable : 1 = never attended school -> 6 = college graduate or above. 9 = missing values that need to be dropped\n",
    "    #remove all missing values\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 9] != 9]\n",
    "    ids = ids[x_featured_clean_1[:, 9] != 9]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 9] != 9]\n",
    "    x_featured_clean_1.shape\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #11 : INCOME2 remove 77 and 99 (already ordinal)\n",
    "    #remove 77 and 99\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 10] != 77]\n",
    "    ids = ids[x_featured_clean_1[:, 10] != 77]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 10] != 77]\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 10] != 99]\n",
    "    ids = ids[x_featured_clean_1[:, 10] != 99]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 10] != 99]\n",
    "    x_featured_clean_1.shape\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #12 : DIFFWALK : binary :Do you have serious difficulty walking or climbing stairs? 1 = yes, 2 = no, 7 = don't know, 9 = missing values\n",
    "    #remove all missing values\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 11] != 7]\n",
    "    ids = ids[x_featured_clean_1[:, 11] != 7]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 11] != 7]\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 11] != 9]\n",
    "    ids = ids[x_featured_clean_1[:, 11] != 9]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 11] != 9]\n",
    "\n",
    "    #change 1 (yes) to 0 and 2 (no) to 1\n",
    "\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 11] == 1, 11] = 0\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 11] == 2, 11] = 1\n",
    "    x_featured_clean_1.shape\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #13 : SMOKE100 : binary variable : Have you smoked at least 100 cigarettes in your entire life? 1 = yes, 2 = no, 7 = don't know, 9 = missing values\n",
    "    #remove all missing values\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 12] != 7]\n",
    "    ids = ids[x_featured_clean_1[:, 12] != 7]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 12] != 7]\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 12] != 9]\n",
    "    ids = ids[x_featured_clean_1[:, 12] != 9]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 12] != 9]\n",
    "\n",
    "    #change 1 (yes) to 0 and 2 (no) to 1\n",
    "\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 12] == 1, 12] = 0\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 12] == 2, 12] = 1\n",
    "    x_featured_clean_1.shape\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #14 : _RFHYPE5 : Adults who have been told they have high blood pressure by a doctor. Yes (2) to 0, no (1) to 1, 7 = don't know, 9 = missing values\n",
    "\n",
    "    #remove all missing values\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 13] != 7]\n",
    "    ids = ids[x_featured_clean_1[:, 13] != 7]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 13] != 7]\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 13] != 9]\n",
    "    ids = ids[x_featured_clean_1[:, 13] != 9]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 13] != 9]\n",
    "\n",
    "    #change 1 (yes) to 0 and 2 (no) to 1\n",
    "\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 13] == 1, 13] = 1\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 13] == 2, 13] = 0\n",
    "    x_featured_clean_1.shape\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #15 : _CHOLCHK : Cholesterol check within past five years. No = {2,3} to 0, Yes = {1}, remove missing values\n",
    "    #remove all missing values\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 14] != 7]\n",
    "    ids = ids[x_featured_clean_1[:, 14] != 7]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 14] != 7]\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 14] != 9]\n",
    "    ids = ids[x_featured_clean_1[:, 14] != 9]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 14] != 9]\n",
    "\n",
    "    #change 2 and 3 to 0 \n",
    "\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 14] == 2, 14] = 0\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 14] == 3, 14] = 0\n",
    "    x_featured_clean_1.shape\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "\n",
    "    #16 : _AGEG5YR : ordinal varible to represent the age groupe -> remove 14 as missing values\n",
    "    #remove all missing values\n",
    "\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 15] != 14]\n",
    "    ids = ids[x_featured_clean_1[:, 15] != 14]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 15] != 14]\n",
    "    x_featured_clean_1.shape\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #17 : _BMI5 : Body mass index (kg/m**2) : divide by 100\n",
    "    x_featured_clean_1[:, 16] = x_featured_clean_1[:, 16] / 100\n",
    "    x_featured_clean_1.shape\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #18 : _RFDRHV5 : Heavy drinkers, binary. yes (2) to 0 and no (1) to 1, remove missing values\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 17] == 2, 17] = 0\n",
    "\n",
    "    #remove all missing values\n",
    "\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 17] != 9]\n",
    "    ids = ids[x_featured_clean_1[:, 17] != 9]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 17] != 9]\n",
    "    x_featured_clean_1.shape\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #19 : _FRTLT1 : Consume Fruit 1 or more times per day : change 1 = yes to 0, 2 = less than 1 time per day to 1, 9 = missing values\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 18] == 1, 18] = 0\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 18] == 2, 18] = 1\n",
    "\n",
    "    #remove all missing values\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 18] != 9]\n",
    "    ids = ids[x_featured_clean_1[:, 18] != 9]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 18] != 9]\n",
    "    x_featured_clean_1.shape\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #20 : _VEGLT1 : Consume Vegetables 1 or more times per day : change 1 = yes to 0, 2 = less than 1 time per day to 1, 9 = missing values\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 19] == 1, 19] = 0\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 19] == 2, 19] = 1\n",
    "\n",
    "    #remove all missing values\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 19] != 9]\n",
    "    ids = ids[x_featured_clean_1[:, 19] != 9]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 19] != 9]\n",
    "    x_featured_clean_1.shape\n",
    "\n",
    "    ###################################################\n",
    "\n",
    "    #21 : _TOTINDA : Adults who reported doing physical activity or exercise during the past 30 days other than their regular job. \n",
    "    #change 1 = yes to 0, 2 = less than 1 time per day to 1, 9 = missing values\n",
    "\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 20] == 1, 20] = 0\n",
    "    x_featured_clean_1[x_featured_clean_1[:, 20] == 2, 20] = 1\n",
    "\n",
    "    #remove all missing values\n",
    "    y_clean = y_clean[x_featured_clean_1[:, 20] != 9]\n",
    "    ids = ids[x_featured_clean_1[:, 20] != 9]\n",
    "    x_featured_clean_1 = x_featured_clean_1[x_featured_clean_1[:, 20] != 9]\n",
    "    x_featured_clean_1.shape\n",
    "\n",
    "    return x_featured_clean_1, y_clean, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257127, 21) (257127,)\n",
      "(252137, 21) (252137,)\n",
      "(249379, 21) (249379,)\n",
      "(248858, 21) (248858,)\n",
      "(248466, 21) (248466,)\n",
      "(246570, 21) (246570,)\n",
      "(246060, 21) (246060,)\n",
      "the final number of samples (Y) is : 190395 and the number of features (w) is : 21 and the matrix of features (X) is : (190395, 21)\n"
     ]
    }
   ],
   "source": [
    "x_processed, y_processed, ids_processed = feature_processing(x_featured_clean, y_clean, ids_1)\n",
    "print('the final number of samples (Y) is :', len(y_processed), 'and the number of features (w) is :', len(x_processed[0]), 'and the matrix of features (X) is :', x_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85672, 21) (85672,)\n",
      "(84084, 21) (84084,)\n",
      "(83166, 21) (83166,)\n",
      "(83009, 21) (83009,)\n",
      "(82866, 21) (82866,)\n",
      "(82244, 21) (82244,)\n",
      "(82075, 21) (82075,)\n"
     ]
    }
   ],
   "source": [
    "x_test_processed= feature_processing(x_test_featured_clean, np.zeros(len(x_test_featured_clean)), np.arange(len(x_test_featured_clean)))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the preprocessing has been done, we can format the data to be used by the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = np.c_[np.ones((len(y_processed), 1)), x_processed]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And then, we can run the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. MSE gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=0.047004910843246935\n",
      "Gradient Descent(1/99): loss=0.13321256876278395\n",
      "Gradient Descent(2/99): loss=1.4637876479891274\n",
      "Gradient Descent(3/99): loss=21.809446400605346\n",
      "Gradient Descent(4/99): loss=332.8192203016009\n",
      "Gradient Descent(5/99): loss=5086.941494855025\n",
      "Gradient Descent(6/99): loss=77758.81847357457\n",
      "Gradient Descent(7/99): loss=1188626.6579433763\n",
      "Gradient Descent(8/99): loss=18169436.926940143\n",
      "Gradient Descent(9/99): loss=277739393.9674497\n",
      "Gradient Descent(10/99): loss=4245545495.7023726\n",
      "Gradient Descent(11/99): loss=64897731289.70107\n",
      "Gradient Descent(12/99): loss=992031655495.6168\n",
      "Gradient Descent(13/99): loss=15164271322718.281\n",
      "Gradient Descent(14/99): loss=231802204571926.2\n",
      "Gradient Descent(15/99): loss=3543346125962986.5\n",
      "Gradient Descent(16/99): loss=5.416385832724543e+16\n",
      "Gradient Descent(17/99): loss=8.279528571588799e+17\n",
      "Gradient Descent(18/99): loss=1.2656150334340708e+19\n",
      "Gradient Descent(19/99): loss=1.9346287641917643e+20\n",
      "Gradient Descent(20/99): loss=2.9572882403921945e+21\n",
      "Gradient Descent(21/99): loss=4.520533292295839e+22\n",
      "Gradient Descent(22/99): loss=6.910121565980614e+23\n",
      "Gradient Descent(23/99): loss=1.0562864372220987e+25\n",
      "Gradient Descent(24/99): loss=1.614647480230003e+26\n",
      "Gradient Descent(25/99): loss=2.4681624165026392e+27\n",
      "Gradient Descent(26/99): loss=3.772851838451097e+28\n",
      "Gradient Descent(27/99): loss=5.767210010058328e+29\n",
      "Gradient Descent(28/99): loss=8.815801076824659e+30\n",
      "Gradient Descent(29/99): loss=1.3475900563807719e+32\n",
      "Gradient Descent(30/99): loss=2.0599364076286922e+33\n",
      "Gradient Descent(31/99): loss=3.1488344570236283e+34\n",
      "Gradient Descent(32/99): loss=4.8133322956087e+35\n",
      "Gradient Descent(33/99): loss=7.357696348968718e+36\n",
      "Gradient Descent(34/99): loss=1.1247030588978209e+38\n",
      "Gradient Descent(35/99): loss=1.7192296483822872e+39\n",
      "Gradient Descent(36/99): loss=2.628027514011795e+40\n",
      "Gradient Descent(37/99): loss=4.017222842161739e+41\n",
      "Gradient Descent(38/99): loss=6.140757384594684e+42\n",
      "Gradient Descent(39/99): loss=9.386808434097838e+43\n",
      "Gradient Descent(40/99): loss=1.4348746752232338e+45\n",
      "Gradient Descent(41/99): loss=2.1933603397275063e+46\n",
      "Gradient Descent(42/99): loss=3.35278729422213e+47\n",
      "Gradient Descent(43/99): loss=5.125096153463713e+48\n",
      "Gradient Descent(44/99): loss=7.834260952823888e+49\n",
      "Gradient Descent(45/99): loss=1.1975510866359421e+51\n",
      "Gradient Descent(46/99): loss=1.8305856975392003e+52\n",
      "Gradient Descent(47/99): loss=2.798247217535029e+53\n",
      "Gradient Descent(48/99): loss=4.277421975364706e+54\n",
      "Gradient Descent(49/99): loss=6.538499758234433e+55\n",
      "Gradient Descent(50/99): loss=9.994800450985798e+56\n",
      "Gradient Descent(51/99): loss=1.527812797258565e+58\n",
      "Gradient Descent(52/99): loss=2.3354262597977226e+59\n",
      "Gradient Descent(53/99): loss=3.569950339949743e+60\n",
      "Gradient Descent(54/99): loss=5.457053236530425e+61\n",
      "Gradient Descent(55/99): loss=8.341693074292596e+62\n",
      "Gradient Descent(56/99): loss=1.2751175465889671e+64\n",
      "Gradient Descent(57/99): loss=1.9491543780600578e+65\n",
      "Gradient Descent(58/99): loss=2.9794922042080374e+66\n",
      "Gradient Descent(59/99): loss=4.55447444022979e+67\n",
      "Gradient Descent(60/99): loss=6.962004262810258e+68\n",
      "Gradient Descent(61/99): loss=1.0642172657125e+70\n",
      "Gradient Descent(62/99): loss=1.6267706049686145e+71\n",
      "Gradient Descent(63/99): loss=2.4866939171656638e+72\n",
      "Gradient Descent(64/99): loss=3.8011792312832245e+73\n",
      "Gradient Descent(65/99): loss=5.810511478150807e+74\n",
      "Gradient Descent(66/99): loss=8.881992030227108e+75\n",
      "Gradient Descent(67/99): loss=1.357708055851286e+77\n",
      "Gradient Descent(68/99): loss=2.075402858559377e+78\n",
      "Gradient Descent(69/99): loss=3.172476591527431e+79\n",
      "Gradient Descent(70/99): loss=4.849471842192507e+80\n",
      "Gradient Descent(71/99): loss=7.412939534691806e+81\n",
      "Gradient Descent(72/99): loss=1.1331475742758908e+83\n",
      "Gradient Descent(73/99): loss=1.7321379988036364e+84\n",
      "Gradient Descent(74/99): loss=2.647759316624478e+85\n",
      "Gradient Descent(75/99): loss=4.047385025681497e+86\n",
      "Gradient Descent(76/99): loss=6.18686352768452e+87\n",
      "Gradient Descent(77/99): loss=9.457286635028675e+88\n",
      "Gradient Descent(78/99): loss=1.4456480266110863e+90\n",
      "Gradient Descent(79/99): loss=2.209828566582506e+91\n",
      "Gradient Descent(80/99): loss=3.377960751021609e+92\n",
      "Gradient Descent(81/99): loss=5.163576490953271e+93\n",
      "Gradient Descent(82/99): loss=7.893082289325532e+94\n",
      "Gradient Descent(83/99): loss=1.2065425608629465e+96\n",
      "Gradient Descent(84/99): loss=1.8443301334162303e+97\n",
      "Gradient Descent(85/99): loss=2.8192570667331307e+98\n",
      "Gradient Descent(86/99): loss=4.309537790613614e+99\n",
      "Gradient Descent(87/99): loss=6.587592237641377e+100\n",
      "Gradient Descent(88/99): loss=1.0069843588319922e+102\n",
      "Gradient Descent(89/99): loss=1.5392839482963209e+103\n",
      "Gradient Descent(90/99): loss=2.3529611485038282e+104\n",
      "Gradient Descent(91/99): loss=3.5967543041660123e+105\n",
      "Gradient Descent(92/99): loss=5.498025980056123e+106\n",
      "Gradient Descent(93/99): loss=8.404324321613966e+107\n",
      "Gradient Descent(94/99): loss=1.284691406681037e+109\n",
      "Gradient Descent(95/99): loss=1.96378905339907e+110\n",
      "Gradient Descent(96/99): loss=3.0018628802173406e+111\n",
      "Gradient Descent(97/99): loss=4.588670425690342e+112\n",
      "Gradient Descent(98/99): loss=7.014276506220918e+113\n",
      "Gradient Descent(99/99): loss=1.0722076405895081e+115\n"
     ]
    }
   ],
   "source": [
    "loss_mse_gd, w_mse_gd = mean_squared_error_gd(y_processed, tX, np.zeros(len(tX[0])), 100, 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      "\n",
      " [-2.02003827e-05  2.54989475e-02  2.60793352e-03  1.93497726e-04\n",
      " -4.81263741e-04 -8.86109403e-04 -1.20183967e-02 -8.02635297e-03\n",
      " -1.44006578e-02 -1.28237454e-02 -3.49899156e-03 -6.26742058e-03\n",
      " -7.42836588e-03 -7.47218454e-03 -1.21558621e-02  1.06884607e-03\n",
      "  1.64853185e-02  3.07174567e-04  9.55504128e-04  1.39608518e-03\n",
      "  9.92309863e-04  1.39735929e-03] \n",
      "\n",
      " Loss =  0.037641090448382625 \n",
      "\n",
      "*****************************************************************************  \n",
      "\n",
      " Train sample : \n",
      " Heart attack rate =  0.09400982168649387 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.0\n"
     ]
    }
   ],
   "source": [
    "tX_test = np.c_[np.ones((x_test_processed.shape[0], 1)), x_test_processed]\n",
    "y_test = tX_test.dot(w_mse_gd)\n",
    "y_test_rounded = np.where(y_test > 0.5, 1, 0)\n",
    "\n",
    "print('weights = \\n\\n', w_mse_gd,'\\n\\n Loss = ', loss_mse_gd,'\\n\\n*****************************************************************************',\n",
    "      ' \\n\\n Train sample : \\n', 'Heart attack rate = ', np.sum(y_processed)/len(y_processed), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.sum(y_test_rounded)/len(y_test_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. MSE SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(1/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(2/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(3/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(4/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(5/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(6/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(7/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(8/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(9/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(10/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(11/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(12/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(13/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(14/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(15/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(16/299): loss=0.0, w0=0.0, w1=0.0\n",
      "Gradient Descent(17/299): loss=0.5, w0=0.05, w1=0.15000000000000002\n",
      "Gradient Descent(18/299): loss=16.27227965858765, w0=-0.23523919487500006, w1=-0.7057175846250001\n",
      "Gradient Descent(19/299): loss=355.62541303755563, w0=1.0982251148040323, w1=3.294675344412097\n",
      "Gradient Descent(20/299): loss=28272.39032766903, w0=-10.791348117164606, w1=-8.594897887556542\n",
      "Gradient Descent(21/299): loss=1327076.4658438028, w0=70.66650803691678, w1=154.3208144206062\n",
      "Gradient Descent(22/299): loss=66900071.95417752, w0=-507.6935526267564, w1=-1002.3993069067401\n",
      "Gradient Descent(23/299): loss=1913442302.609502, w0=2585.397379280599, w1=5183.7825569079705\n",
      "Gradient Descent(24/299): loss=30168680142.41445, w0=-9696.434760594635, w1=-31661.713862717726\n",
      "Gradient Descent(25/299): loss=1068378498660.9255, w0=63391.81664347596, w1=187603.04034949408\n",
      "Gradient Descent(26/299): loss=77463786794227.12, w0=-558957.7105284627, w1=-2924144.5955101997\n",
      "Gradient Descent(27/299): loss=3361293944032953.5, w0=3540611.744934485, w1=5274994.315415695\n",
      "Gradient Descent(28/299): loss=2.1511947699036342e+17, w0=-29255689.646408677, w1=-60317608.46727063\n",
      "Gradient Descent(29/299): loss=1.371945632687157e+19, w0=232655137.9922322, w1=463504046.81001115\n",
      "Gradient Descent(30/299): loss=5.787656074808718e+20, w0=-1468470382.7746391, w1=-1237621473.9568603\n",
      "Gradient Descent(31/299): loss=1.5319885011794493e+22, w0=7283639366.250019, w1=25018707773.117115\n",
      "Gradient Descent(32/299): loss=3.3230129790853657e+24, w0=-121615797775.37148, w1=-490579040793.36884\n",
      "Gradient Descent(33/299): loss=1.850879161621009e+27, w0=2920488047938.1577, w1=8635732496347.219\n",
      "Gradient Descent(34/299): loss=5.971778236649421e+28, w0=-14359237407996.352, w1=-8643992959587.291\n",
      "Gradient Descent(35/299): loss=7.144946121060143e+29, w0=45410931242678.14, w1=170666512992436.2\n",
      "Gradient Descent(36/299): loss=3.2054287284975183e+31, w0=-354928220508528.9, w1=-1030350942261184.5\n",
      "Gradient Descent(37/299): loss=1.3350797428349366e+33, w0=2228751077348440.5, w1=4137007653452754.5\n",
      "Gradient Descent(38/299): loss=6.581816599305569e+34, w0=-1.5912109706122628e+16, w1=-5.028557469696045e+16\n",
      "Gradient Descent(39/299): loss=6.134475323560886e+36, w0=1.592231997486085e+17, w1=2.999850442125018e+17\n",
      "Gradient Descent(40/299): loss=2.5617746757215554e+38, w0=-9.725397338469294e+17, w1=-4.2270666901696497e+18\n",
      "Gradient Descent(41/299): loss=1.1182362582448792e+40, w0=6.504880470491193e+18, w1=3.250353514168473e+18\n",
      "Gradient Descent(42/299): loss=4.602969018371598e+41, w0=-4.1468909344217e+19, w1=-9.269722611524792e+19\n",
      "Gradient Descent(43/299): loss=2.0967579118055355e+43, w0=2.8231789642506813e+20, w1=2.3108957965403724e+20\n",
      "Gradient Descent(44/299): loss=3.1405234525264526e+44, w0=-9.707829479857983e+20, w1=-2.275112109167696e+21\n",
      "Gradient Descent(45/299): loss=6.48086779861387e+45, w0=4.72169796924786e+21, w1=2.0494811559766937e+22\n",
      "Gradient Descent(46/299): loss=2.491163696257543e+47, w0=-3.057110367823787e+22, w1=-8.538359338269025e+22\n",
      "Gradient Descent(47/299): loss=4.159317997462944e+48, w0=1.136391247638346e+23, w1=2.0303686350145468e+23\n",
      "Gradient Descent(48/299): loss=2.5263983787941487e+50, w0=-1.0102822200806676e+24, w1=-4.292648515876554e+24\n",
      "Gradient Descent(49/299): loss=2.353721923264877e+52, w0=9.838040054042824e+24, w1=2.825231830649392e+25\n",
      "Gradient Descent(50/299): loss=2.5715864125583255e+54, w0=-1.0355478186512289e+26, w1=-1.985333255318375e+26\n",
      "Gradient Descent(51/299): loss=2.007620670152559e+56, w0=8.983485742906889e+26, w1=1.8052733867797862e+27\n",
      "Gradient Descent(52/299): loss=1.5867774424013762e+58, w0=-8.0088884900334e+27, w1=-3.382367487051657e+28\n",
      "Gradient Descent(53/299): loss=9.708984601140474e+59, w0=6.166529826926963e+28, w1=1.7519888540739252e+29\n",
      "Gradient Descent(54/299): loss=2.5386642555695216e+61, w0=-2.946115783007319e+29, w1=-5.373548677326104e+29\n",
      "Gradient Descent(55/299): loss=7.49291121997629e+62, w0=1.6409647208898077e+30, w1=5.269374029839008e+30\n",
      "Gradient Descent(56/299): loss=3.2205962335167046e+64, w0=-1.104878750978181e+31, w1=-4.548963489284746e+31\n",
      "Gradient Descent(57/299): loss=5.200033356639043e+66, w0=1.5019688462809362e+32, w1=5.9949305365865426e+32\n",
      "Gradient Descent(58/299): loss=4.2771555319382227e+68, w0=-1.3121908088270692e+33, w1=-2.3252823332516715e+33\n",
      "Gradient Descent(59/299): loss=3.263071017947172e+70, w0=1.1460966627662885e+34, w1=2.3221032539728237e+34\n",
      "Gradient Descent(60/299): loss=1.4392274799217578e+72, w0=-7.336908347021689e+34, w1=-2.3126911775391103e+35\n",
      "Gradient Descent(61/299): loss=6.722728471183232e+74, w0=1.7600332825126617e+36, w1=8.93574271216048e+36\n",
      "Gradient Descent(62/299): loss=4.526058776548256e+76, w0=-1.3283335317219675e+37, w1=-3.6194363087036525e+37\n",
      "Gradient Descent(63/299): loss=1.800226371351529e+78, w0=8.159095970918695e+37, w1=3.4330281701859e+38\n",
      "Gradient Descent(64/299): loss=3.7510187636478174e+79, w0=-3.514805565354061e+38, w1=-9.559117317151893e+38\n",
      "Gradient Descent(65/299): loss=6.727054139269783e+80, w0=1.4825115562161433e+39, w1=2.7120724937879094e+39\n",
      "Gradient Descent(66/299): loss=5.1250496719403285e+82, w0=-1.452537661171583e+40, w1=-6.131948017793998e+40\n",
      "Gradient Descent(67/299): loss=4.815286475383997e+84, w0=1.4064044558157512e+41, w1=9.384634201535098e+40\n",
      "Gradient Descent(68/299): loss=3.5529356146935825e+86, w0=-1.1922015583134068e+42, w1=-2.571837665774613e+42\n",
      "Gradient Descent(69/299): loss=2.2787508373244854e+88, w0=9.48195142289425e+42, w1=4.012477425905601e+43\n",
      "Gradient Descent(70/299): loss=1.3204518591963044e+90, w0=-7.177233643923352e+43, w1=-4.1129513603071758e+43\n",
      "Gradient Descent(71/299): loss=5.581424219506651e+91, w0=4.564995736367544e+44, w1=4.871423964729161e+44\n",
      "Gradient Descent(72/299): loss=1.5638957195391333e+93, w0=-2.3398334890894415e+45, w1=-2.3091906662532796e+45\n",
      "Gradient Descent(73/299): loss=3.649723661077105e+94, w0=1.1168911199217289e+46, w1=3.8217043398666915e+46\n",
      "Gradient Descent(74/299): loss=1.1308319487945377e+96, w0=-6.40252358068675e+46, w1=-1.8736539761958744e+47\n",
      "Gradient Descent(75/299): loss=6.801376901224923e+97, w0=5.191289848425643e+47, w1=9.789430436792761e+47\n",
      "Gradient Descent(76/299): loss=3.4515805472993044e+99, w0=-3.635134215352691e+48, w1=-7.329583356711235e+48\n",
      "Gradient Descent(77/299): loss=3.545199685328312e+100, w0=9.678767702057383e+48, w1=4.5926024312929057e+49\n",
      "Gradient Descent(78/299): loss=3.8732688683065545e+101, w0=-3.4328441186410847e+49, w1=1.9188154244608265e+48\n",
      "Gradient Descent(79/299): loss=1.6301022770468238e+103, w0=2.5116256366722933e+50, w1=1.1438828348390215e+51\n",
      "Gradient Descent(80/299): loss=4.3573671103735525e+106, w0=-1.450920177867465e+52, w1=-1.361648150750286e+52\n",
      "Gradient Descent(81/299): loss=3.193271721722409e+109, w0=3.8507005955269185e+53, w1=1.584700563817963e+54\n",
      "Gradient Descent(82/299): loss=1.513690041190024e+111, w0=-2.3660115833561696e+54, w1=-6.668544364908623e+54\n",
      "Gradient Descent(83/299): loss=3.5261390495768013e+112, w0=1.0912051245190774e+55, w1=1.9887581292185264e+55\n",
      "Gradient Descent(84/299): loss=8.27688350981147e+112, w0=-9.431109165388834e+54, w1=-4.555791183943434e+53\n",
      "Gradient Descent(85/299): loss=9.241946741441292e+113, w0=5.854663256468519e+55, w1=6.752216261167968e+55\n",
      "Gradient Descent(86/299): loss=8.898335656816104e+115, w0=-6.084741938274187e+56, w1=-3.2675819693488395e+57\n",
      "Gradient Descent(87/299): loss=4.522176469394611e+117, w0=4.146615957470773e+57, w1=1.0997688484545734e+58\n",
      "Gradient Descent(88/299): loss=3.868338373210478e+119, w0=-3.9832574423949836e+58, w1=-7.696069227829549e+58\n",
      "Gradient Descent(89/299): loss=6.686886584249761e+120, w0=1.4301827441533998e+59, w1=2.8874100540028414e+59\n",
      "Gradient Descent(90/299): loss=4.7414400887059955e+121, w0=-3.4388213063203707e+59, w1=-1.1719602097418468e+60\n",
      "Gradient Descent(91/299): loss=8.581505973457312e+122, w0=1.727531151822398e+60, w1=2.9708663551670227e+60\n",
      "Gradient Descent(92/299): loss=5.601535246487049e+125, w0=-5.1194747930276147e+61, w1=-1.5579597089112858e+62\n",
      "Gradient Descent(93/299): loss=2.3484262046363043e+128, w0=1.0324163918787202e+63, w1=2.011426308726864e+63\n",
      "Gradient Descent(94/299): loss=1.66032755359241e+130, w0=-8.078915984871702e+63, w1=-2.5322570821524404e+64\n",
      "Gradient Descent(95/299): loss=4.755385997696105e+131, w0=4.0682677496556406e+64, w1=7.220061614133181e+64\n",
      "Gradient Descent(96/299): loss=2.322820904371633e+133, w0=-3.001120484288629e+65, w1=-1.6317730134857648e+66\n",
      "Gradient Descent(97/299): loss=1.8399593842843107e+135, w0=2.733004652494719e+66, w1=4.4344603883614e+66\n",
      "Gradient Descent(98/299): loss=9.372898896893013e+136, w0=-1.8915204161276702e+67, w1=-6.051016605295287e+67\n",
      "Gradient Descent(99/299): loss=4.238274589506331e+138, w0=1.2665736518999792e+68, w1=6.673526807034201e+68\n",
      "Gradient Descent(100/299): loss=5.8205856450436816e+140, w0=-1.5793006713753338e+69, w1=-4.4505214289925746e+69\n",
      "Gradient Descent(101/299): loss=4.591997061543534e+141, w0=3.212357215798757e+69, w1=1.9507768006877883e+70\n",
      "Gradient Descent(102/299): loss=1.9915074552578595e+142, w0=-6.766388835829823e+69, w1=9.529021955249303e+69\n",
      "Gradient Descent(103/299): loss=7.87623504626654e+143, w0=5.598803350342266e+70, w1=1.3503786663375427e+71\n",
      "Gradient Descent(104/299): loss=5.756099363177727e+145, w0=-4.804865401334706e+71, w1=-1.4743858542769254e+72\n",
      "Gradient Descent(105/299): loss=1.270891838829775e+147, w0=2.0403187268692933e+72, w1=6.088029946731367e+72\n",
      "Gradient Descent(106/299): loss=1.3000411387299995e+148, w0=-6.022066586206482e+72, w1=-1.974355366344409e+72\n",
      "Gradient Descent(107/299): loss=3.445033382197953e+149, w0=3.5481146435184725e+73, w1=8.103207067643801e+73\n",
      "Gradient Descent(108/299): loss=9.048630973309283e+151, w0=-6.371491745179107e+74, w1=-3.282119534089039e+75\n",
      "Gradient Descent(109/299): loss=8.723619451223619e+153, w0=5.9672506662032804e+75, w1=1.6531079988074536e+76\n",
      "Gradient Descent(110/299): loss=7.7134975715421375e+155, w0=-5.613547701761966e+76, w1=-1.0767437537957135e+77\n",
      "Gradient Descent(111/299): loss=2.758147865951259e+157, w0=3.1522337005292334e+77, w1=6.350433187615146e+77\n",
      "Gradient Descent(112/299): loss=1.7255720285308158e+159, w0=-2.6220985637252047e+78, w1=-8.17692248257287e+78\n",
      "Gradient Descent(113/299): loss=5.656078046542967e+161, w0=5.055721154153551e+79, w1=2.577196280437307e+80\n",
      "Gradient Descent(114/299): loss=1.7380379882102505e+163, w0=-2.4423406955495917e+80, w1=-3.707165305276399e+79\n",
      "Gradient Descent(115/299): loss=9.233683601595855e+164, w0=1.9044496672471623e+81, w1=2.1116120837493573e+81\n",
      "Gradient Descent(116/299): loss=7.413888071534161e+166, w0=-1.7348976094853155e+82, w1=-5.564866520255158e+82\n",
      "Gradient Descent(117/299): loss=7.020458196731116e+168, w0=1.7000707794038633e+83, w1=3.190634428679274e+83\n",
      "Gradient Descent(118/299): loss=1.170885549507114e+171, w0=-2.249585397567746e+84, w1=-9.359306459164601e+84\n",
      "Gradient Descent(119/299): loss=5.804736746482772e+172, w0=1.4786753332456925e+85, w1=4.174970973090941e+85\n",
      "Gradient Descent(120/299): loss=1.653897472519938e+174, w0=-7.614996841733836e+85, w1=-1.4012373376868115e+86\n",
      "Gradient Descent(121/299): loss=8.30181997045859e+175, w0=5.681255925990093e+86, w1=5.041518272476666e+86\n",
      "Gradient Descent(122/299): loss=3.7690526565118792e+177, w0=-3.7729875460509614e+87, w1=-1.2519187588702246e+88\n",
      "Gradient Descent(123/299): loss=3.430531004035153e+179, w0=3.7642776385302245e+88, w1=1.1172810420535735e+89\n",
      "Gradient Descent(124/299): loss=1.6457637734933484e+181, w0=-2.4921640278571093e+89, w1=-7.488494333076822e+89\n",
      "Gradient Descent(125/299): loss=2.8059663061176466e+182, w0=9.352594952632208e+89, w1=3.9890541588880445e+90\n",
      "Gradient Descent(126/299): loss=4.688544525721612e+183, w0=-3.9065090482142616e+90, w1=-5.694482928066921e+90\n",
      "Gradient Descent(127/299): loss=1.0636249153241408e+185, w0=1.9154550286995613e+91, w1=4.042763574235283e+91\n",
      "Gradient Descent(128/299): loss=3.5432413222633532e+186, w0=-1.1394769090869863e+92, w1=-3.5887908784472987e+92\n",
      "Gradient Descent(129/299): loss=9.453850419318932e+187, w0=5.735786874695762e+92, w1=1.0161736689118198e+93\n",
      "Gradient Descent(130/299): loss=2.553478356232712e+189, w0=-2.9995700333596533e+93, w1=-9.70327249357587e+93\n",
      "Gradient Descent(131/299): loss=5.5172281971965286e+190, w0=1.3609506095667707e+94, w1=2.3514879764478847e+94\n",
      "Gradient Descent(132/299): loss=1.871426716900492e+192, w0=-8.312277178768651e+94, w1=-1.699496760022296e+95\n",
      "Gradient Descent(133/299): loss=6.9354112642889446e+193, w0=5.057495091140785e+95, w1=1.0077948858013004e+96\n",
      "Gradient Descent(134/299): loss=2.6146905553414656e+195, w0=-3.1099734859046674e+96, w1=-9.839374099254939e+96\n",
      "Gradient Descent(135/299): loss=1.546546697994678e+197, w0=2.4697819468541377e+97, w1=7.35840047640832e+97\n",
      "Gradient Descent(136/299): loss=3.4210247160823294e+198, w0=-1.060887378697862e+98, w1=-5.720255257424439e+97\n",
      "Gradient Descent(137/299): loss=9.9130673421274e+199, w0=5.979378007680481e+98, w1=2.0548770633392589e+99\n",
      "Gradient Descent(138/299): loss=3.4655023388620824e+201, w0=-3.5646949832173965e+99, w1=-6.27038850463163e+99\n",
      "Gradient Descent(139/299): loss=1.592632334290519e+203, w0=2.4654379543514686e+100, w1=1.0660590960229671e+101\n",
      "Gradient Descent(140/299): loss=5.367471598506802e+204, w0=-1.3916673628876748e+101, w1=-5.7215206229985455e+100\n",
      "Gradient Descent(141/299): loss=1.0843629295200437e+206, w0=5.971629892826112e+101, w1=6.791145193413933e+101\n",
      "Gradient Descent(142/299): loss=6.062747356565744e+207, w0=-4.908628214788673e+102, w1=-2.6849841501015032e+103\n",
      "Gradient Descent(143/299): loss=1.5843979451614517e+210, w0=8.409693186887833e+103, w1=3.2917239883365304e+104\n",
      "Gradient Descent(144/299): loss=3.3838245617227524e+211, w0=-3.2723168139080925e+104, w1=-8.215621442603455e+103\n",
      "Gradient Descent(145/299): loss=4.2716844903675084e+212, w0=1.1342204201793224e+105, w1=1.379295887144097e+105\n",
      "Gradient Descent(146/299): loss=2.971058284567196e+214, w0=-1.105400809049276e+106, w1=-3.5185389644872153e+106\n",
      "Gradient Descent(147/299): loss=3.057871237635757e+216, w0=1.1259612813336583e+107, w1=3.3576501902670366e+107\n",
      "Gradient Descent(148/299): loss=3.413934657151424e+217, w0=-3.005584831897697e+107, w1=-4.9054420361956746e+107\n",
      "Gradient Descent(149/299): loss=1.303185351320887e+218, w0=5.066544228396394e+107, w1=3.1666870240984163e+107\n",
      "Gradient Descent(150/299): loss=1.8256051140787898e+219, w0=-2.514607825028617e+108, w1=-8.747118041194928e+108\n",
      "Gradient Descent(151/299): loss=1.6351530329490157e+221, w0=2.6078687131560558e+109, w1=7.703276682857258e+109\n",
      "Gradient Descent(152/299): loss=9.317330858065876e+222, w0=-1.8976073070107116e+110, w1=-1.3880665100405914e+110\n",
      "Gradient Descent(153/299): loss=4.116084078516795e+224, w0=1.2448270286424965e+111, w1=5.599544386370212e+111\n",
      "Gradient Descent(154/299): loss=1.063681259611244e+226, w0=-6.047913401440163e+111, w1=-8.985936473795107e+111\n",
      "Gradient Descent(155/299): loss=4.2093812742828156e+227, w0=-5.192482085106462e+112, w1=-2.383704737219174e+113\n",
      "Gradient Descent(156/299): loss=4.617241525103333e+229, w0=4.2855626749182394e+113, w1=7.225917029638596e+113\n",
      "Gradient Descent(157/299): loss=1.0686940510833798e+231, w0=-1.8830384754112005e+114, w1=-1.5890030399391648e+114\n",
      "Gradient Descent(158/299): loss=2.1553000269678683e+232, w0=8.497953834979624e+114, w1=3.993496620162413e+115\n",
      "Gradient Descent(159/299): loss=3.3681234042234488e+233, w0=-3.2539367009731042e+115, w1=-4.21396754877972e+115\n",
      "Gradient Descent(160/299): loss=9.61548340524502e+234, w0=1.8672626419184396e+116, w1=3.963915869153528e+116\n",
      "Gradient Descent(161/299): loss=4.817437622674898e+236, w0=-1.3652785074945867e+117, w1=-4.259622728143939e+117\n",
      "Gradient Descent(162/299): loss=2.5430994299216294e+238, w0=9.911022814130402e+117, w1=2.9569281236731032e+118\n",
      "Gradient Descent(163/299): loss=1.4722099798869051e+240, w0=-7.588553830246987e+118, w1=-1.4202384099646952e+119\n",
      "Gradient Descent(164/299): loss=1.4529086607135769e+242, w0=7.764373643329038e+119, w1=2.4149448669096512e+120\n",
      "Gradient Descent(165/299): loss=8.610224571241345e+243, w0=-5.78489808328801e+120, w1=-1.7269061475953087e+121\n",
      "Gradient Descent(166/299): loss=2.5071502337766005e+245, w0=2.9620964664120086e+121, w1=5.3542664018863105e+121\n",
      "Gradient Descent(167/299): loss=8.206049333433152e+246, w0=-1.729382777000973e+122, w1=-3.515758207095717e+122\n",
      "Gradient Descent(168/299): loss=1.3548797138360139e+248, w0=6.501290688263015e+122, w1=2.9406935653960234e+123\n",
      "Gradient Descent(169/299): loss=2.869377303672652e+249, w0=-3.137599342552115e+123, w1=-4.63476325736081e+123\n",
      "Gradient Descent(170/299): loss=8.22914258528992e+250, w0=1.7146806718866e+124, w1=9.678726704972977e+124\n",
      "Gradient Descent(171/299): loss=2.9677869703397434e+252, w0=-1.0466836003675076e+125, w1=-2.686582332171206e+125\n",
      "Gradient Descent(172/299): loss=3.754150579782789e+253, w0=3.285839094196705e+125, w1=1.0310985751521433e+126\n",
      "Gradient Descent(173/299): loss=7.571984147901366e+256, w0=-1.9129042035054344e+127, w1=-7.679940520274392e+127\n",
      "Gradient Descent(174/299): loss=3.774822737485175e+258, w0=1.1825404894569872e+128, w1=1.9796677675876225e+128\n",
      "Gradient Descent(175/299): loss=2.694735695560566e+260, w0=-1.0425077042923562e+129, w1=-4.4450802361934573e+129\n",
      "Gradient Descent(176/299): loss=9.147740518443819e+262, w0=2.0344100860933476e+130, w1=8.110135402470987e+130\n",
      "Gradient Descent(177/299): loss=5.036289072614495e+264, w0=-1.3834252542834294e+131, w1=-7.758527226456655e+130\n",
      "Gradient Descent(178/299): loss=2.4637816088524917e+266, w0=9.71563238331016e+131, w1=3.25213201901351e+132\n",
      "Gradient Descent(179/299): loss=1.0490103332416382e+268, w0=-6.27070965504175e+132, w1=-1.123241376773202e+133\n",
      "Gradient Descent(180/299): loss=2.4499482772238116e+269, w0=2.8728920894607e+133, w1=5.876684733156547e+133\n",
      "Gradient Descent(181/299): loss=1.9184559027199978e+271, w0=-2.809851329292385e+134, w1=-5.606612603161256e+134\n",
      "Gradient Descent(182/299): loss=1.3967187830592373e+273, w0=2.361663900767099e+135, w1=4.724636807076549e+135\n",
      "Gradient Descent(183/299): loss=5.361486706195647e+274, w0=-1.4011311861438816e+136, w1=-2.802131471733528e+136\n",
      "Gradient Descent(184/299): loss=2.036680499756606e+276, w0=8.690153419288189e+136, w1=7.289153133698544e+136\n",
      "Gradient Descent(185/299): loss=5.659691119552179e+277, w0=-4.4506139306578426e+137, w1=-1.522997250439013e+138\n",
      "Gradient Descent(186/299): loss=1.2612000185025173e+279, w0=2.066113649123148e+138, w1=8.521702918316717e+138\n",
      "Gradient Descent(187/299): loss=6.519396033585343e+280, w0=-1.5988520151629145e+139, w1=-9.532930882435576e+138\n",
      "Gradient Descent(188/299): loss=1.240834463111502e+282, w0=6.27780483629222e+139, w1=1.480002061466671e+140\n",
      "Gradient Descent(189/299): loss=2.0472283298585416e+283, w0=-2.5716165922731596e+140, w1=-4.918792090338092e+140\n",
      "Gradient Descent(190/299): loss=8.427726140846287e+284, w0=1.7956079504520423e+141, w1=1.5608904006455488e+141\n",
      "Gradient Descent(191/299): loss=3.1687057780360063e+286, w0=-1.0791500093169428e+142, w1=-2.3613325686597392e+142\n",
      "Gradient Descent(192/299): loss=1.121962705143128e+288, w0=6.410718831801465e+142, w1=5.128536272458669e+142\n",
      "Gradient Descent(193/299): loss=4.200645258311822e+289, w0=-3.941855815525029e+143, w1=-8.653001770164484e+143\n",
      "Gradient Descent(194/299): loss=1.1077565178792316e+291, w0=1.95927624932476e+144, w1=1.0902008977369866e+145\n",
      "Gradient Descent(195/299): loss=2.746628726160154e+292, w0=-9.759573366060178e+144, w1=-1.253569025340001e+145\n",
      "Gradient Descent(196/299): loss=9.748629395384176e+293, w0=6.005671912854375e+145, w1=5.728060224120392e+145\n",
      "Gradient Descent(197/299): loss=3.9897483711181105e+295, w0=-3.865834252336771e+146, w1=-8.35999686483238e+146\n",
      "Gradient Descent(198/299): loss=8.576666308201198e+296, w0=1.6842456730022445e+147, w1=3.305658509988605e+147\n",
      "Gradient Descent(199/299): loss=1.6641521573252833e+298, w0=-7.43757475201513e+147, w1=-1.4937982340046144e+148\n",
      "Gradient Descent(200/299): loss=2.7713429073000576e+299, w0=2.9787033421208514e+148, w1=1.3396045035284844e+149\n",
      "Gradient Descent(201/299): loss=3.643782154203053e+300, w0=-1.0519041199473966e+149, w1=-1.016995063099737e+147\n",
      "Gradient Descent(202/299): loss=6.115800687642824e+301, w0=4.477924398028014e+149, w1=2.2109144121270646e+150\n",
      "Gradient Descent(203/299): loss=1.0341857317446442e+303, w0=-1.8261751630278155e+150, w1=-2.337020793534169e+150\n",
      "Gradient Descent(204/299): loss=6.799067261086409e+303, w0=4.004376809592963e+150, w1=9.324083151707387e+150\n",
      "Gradient Descent(205/299): loss=6.302701231211918e+305, w0=-5.2132514942675966e+151, w1=-1.0294970035283048e+152\n",
      "Gradient Descent(206/299): loss=2.448200580703277e+307, w0=2.9773893149896186e+152, w1=5.967931925304453e+152\n",
      "Gradient Descent(207/299): loss=inf, w0=-2.09470037906578e+153, w1=-4.188085428599038e+153\n",
      "Gradient Descent(208/299): loss=inf, w0=1.767775965509583e+154, w1=5.512929467388579e+154\n",
      "Gradient Descent(209/299): loss=inf, w0=-7.354783666525092e+154, w1=-1.273218979668077e+155\n",
      "Gradient Descent(210/299): loss=inf, w0=2.5687179279383673e+155, w1=1.194356619869543e+156\n",
      "Gradient Descent(211/299): loss=inf, w0=-1.0965821278358832e+156, w1=-1.5125512213898968e+156\n",
      "Gradient Descent(212/299): loss=inf, w0=6.449711138727595e+156, w1=2.112632857830054e+157\n",
      "Gradient Descent(213/299): loss=inf, w0=-4.258617920195289e+157, w1=-2.790956176237995e+157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\croge\\AppData\\Local\\Temp\\ipykernel_24832\\1574709863.py:4: RuntimeWarning: overflow encountered in square\n",
      "  mse = 1/(2*len(y)) * np.sum(e**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(214/299): loss=inf, w0=3.4217564749466915e+158, w1=1.1263759183274862e+159\n",
      "Gradient Descent(215/299): loss=inf, w0=-2.441455395930598e+159, w1=-4.440886168523048e+159\n",
      "Gradient Descent(216/299): loss=inf, w0=1.2403730376787408e+160, w1=4.009467114963097e+160\n",
      "Gradient Descent(217/299): loss=inf, w0=-6.297010938849546e+160, w1=-1.860268481462176e+161\n",
      "Gradient Descent(218/299): loss=inf, w0=8.621528828074275e+161, w1=2.589342128441551e+162\n",
      "Gradient Descent(219/299): loss=inf, w0=-6.270627596738209e+162, w1=-1.1676218830649722e+163\n",
      "Gradient Descent(220/299): loss=inf, w0=4.561299712717303e+163, w1=1.9585828006499521e+164\n",
      "Gradient Descent(221/299): loss=inf, w0=-2.4104767817407695e+164, w1=-6.641237458387548e+164\n",
      "Gradient Descent(222/299): loss=inf, w0=1.9988044825656256e+165, w1=3.81558057564065e+165\n",
      "Gradient Descent(223/299): loss=inf, w0=-1.2779278106404563e+166, w1=-1.0962502013329538e+166\n",
      "Gradient Descent(224/299): loss=inf, w0=6.084068532282987e+166, w1=2.0989738827437375e+167\n",
      "Gradient Descent(225/299): loss=inf, w0=-4.203449577274113e+167, w1=-1.714845183926591e+168\n",
      "Gradient Descent(226/299): loss=inf, w0=3.730401674623983e+168, w1=2.4359014484248034e+168\n",
      "Gradient Descent(227/299): loss=inf, w0=-1.0514157087406489e+169, w1=-4.029777483766661e+169\n",
      "Gradient Descent(228/299): loss=inf, w0=6.172167406324938e+169, w1=3.2088138091561274e+170\n",
      "Gradient Descent(229/299): loss=inf, w0=-2.880287461945771e+170, w1=-2.886903934221369e+169\n",
      "Gradient Descent(230/299): loss=inf, w0=1.0446879032767118e+171, w1=2.636564259600364e+171\n",
      "Gradient Descent(231/299): loss=inf, w0=-2.2110114955194647e+171, w1=-6.191351391958123e+170\n",
      "Gradient Descent(232/299): loss=inf, w0=-9.728464609479827e+170, w1=1.8571949299471518e+171\n",
      "Gradient Descent(233/299): loss=inf, w0=-1.671506356947971e+172, w1=-6.111167350417975e+172\n",
      "Gradient Descent(234/299): loss=inf, w0=3.419438084758099e+173, w1=1.7321826867222683e+174\n",
      "Gradient Descent(235/299): loss=inf, w0=-2.47012394736821e+174, w1=-1.0798850691217518e+174\n",
      "Gradient Descent(236/299): loss=inf, w0=2.228510598634233e+175, w1=2.367534486458879e+175\n",
      "Gradient Descent(237/299): loss=inf, w0=-2.3237238132192077e+176, w1=-7.402971170602005e+176\n",
      "Gradient Descent(238/299): loss=inf, w0=1.6088520896644358e+177, w1=4.783376295898869e+177\n",
      "Gradient Descent(239/299): loss=inf, w0=-3.4561280341176293e+177, w1=-2.816038278831961e+176\n",
      "Gradient Descent(240/299): loss=inf, w0=7.704237724169813e+177, w1=4.4359859205266566e+178\n",
      "Gradient Descent(241/299): loss=inf, w0=-1.614871144575361e+179, w1=-6.32405549521557e+179\n",
      "Gradient Descent(242/299): loss=inf, w0=1.060665331289837e+180, w1=5.89746896225816e+179\n",
      "Gradient Descent(243/299): loss=inf, w0=-4.257532446587643e+180, w1=-4.728450881651664e+180\n",
      "Gradient Descent(244/299): loss=inf, w0=2.8023056602422077e+181, w1=5.9832727216367776e+181\n",
      "Gradient Descent(245/299): loss=inf, w0=-2.0712568475899615e+182, w1=-8.807622382293052e+182\n",
      "Gradient Descent(246/299): loss=inf, w0=9.489985528350399e+182, w1=3.743734712146839e+183\n",
      "Gradient Descent(247/299): loss=inf, w0=1.4808969537998016e+183, w1=4.807531514076363e+183\n",
      "Gradient Descent(248/299): loss=inf, w0=-8.0936700865802e+183, w1=-2.3916169607063647e+184\n",
      "Gradient Descent(249/299): loss=inf, w0=8.080384029232564e+184, w1=1.5387885115074802e+185\n",
      "Gradient Descent(250/299): loss=inf, w0=-1.9110370025984746e+185, w1=-1.1802868940142509e+185\n",
      "Gradient Descent(251/299): loss=inf, w0=1.9466311620736172e+185, w1=1.0392717600002025e+186\n",
      "Gradient Descent(252/299): loss=inf, w0=-3.083973238536136e+186, w1=-2.2393645947432953e+186\n",
      "Gradient Descent(253/299): loss=inf, w0=1.733174123217951e+187, w1=7.94234932881193e+187\n",
      "Gradient Descent(254/299): loss=inf, w0=-1.3982079957886333e+188, w1=-3.9203412914500927e+188\n",
      "Gradient Descent(255/299): loss=inf, w0=1.0365355804709288e+189, w1=5.489747771103952e+189\n",
      "Gradient Descent(256/299): loss=inf, w0=-4.81211121115373e+189, w1=-6.207545812145366e+189\n",
      "Gradient Descent(257/299): loss=inf, w0=2.8343136522737466e+190, w1=6.010294965563702e+190\n",
      "Gradient Descent(258/299): loss=inf, w0=-1.942167011304637e+191, w1=-3.850167256507654e+191\n",
      "Gradient Descent(259/299): loss=inf, w0=1.2094784868225473e+192, w1=5.229764026161278e+192\n",
      "Gradient Descent(260/299): loss=inf, w0=-8.751597405849949e+192, w1=-2.465346365185621e+193\n",
      "Gradient Descent(261/299): loss=inf, w0=7.590444948721605e+193, w1=2.293146770273418e+194\n",
      "Gradient Descent(262/299): loss=inf, w0=-7.538063460611808e+194, w1=-6.003961185210551e+194\n",
      "Gradient Descent(263/299): loss=inf, w0=8.792179415610498e+195, w1=1.8491575404822303e+196\n",
      "Gradient Descent(264/299): loss=inf, w0=-4.559759532185431e+196, w1=-1.4467774880757212e+197\n",
      "Gradient Descent(265/299): loss=inf, w0=1.823812363504173e+197, w1=8.330108286469949e+196\n",
      "Gradient Descent(266/299): loss=inf, w0=-1.0174803307235814e+198, w1=-3.516283618357297e+198\n",
      "Gradient Descent(267/299): loss=inf, w0=4.933471208759898e+198, w1=8.385619460609662e+198\n",
      "Gradient Descent(268/299): loss=inf, w0=-2.920900410377549e+199, w1=-5.989933116446111e+199\n",
      "Gradient Descent(269/299): loss=inf, w0=2.147548565462651e+200, w1=6.719922507856605e+200\n",
      "Gradient Descent(270/299): loss=inf, w0=-1.1583553762907819e+201, w1=-2.0742282148884334e+201\n",
      "Gradient Descent(271/299): loss=inf, w0=8.534231984547139e+201, w1=7.618359145949488e+201\n",
      "Gradient Descent(272/299): loss=inf, w0=-6.185413695242708e+202, w1=-2.0354674766497317e+203\n",
      "Gradient Descent(273/299): loss=inf, w0=4.286576067923418e+203, w1=7.774767398245645e+203\n",
      "Gradient Descent(274/299): loss=inf, w0=-5.611700466847622e+204, w1=-2.338395555473529e+205\n",
      "Gradient Descent(275/299): loss=inf, w0=3.8534235893981263e+205, w1=6.490791716692248e+205\n",
      "Gradient Descent(276/299): loss=inf, w0=1.0329314468694798e+206, w1=2.5918464354582265e+206\n",
      "Gradient Descent(277/299): loss=inf, w0=-6.53462835657505e+206, w1=-2.0110832974875365e+207\n",
      "Gradient Descent(278/299): loss=inf, w0=6.458144682931794e+207, w1=5.100524221101763e+207\n",
      "Gradient Descent(279/299): loss=inf, w0=-5.769691354087076e+208, w1=-1.873646504503059e+209\n",
      "Gradient Descent(280/299): loss=inf, w0=4.569672779315542e+209, w1=1.871292115439394e+210\n",
      "Gradient Descent(281/299): loss=inf, w0=-2.6744645446007793e+210, w1=-1.2601397070929396e+210\n",
      "Gradient Descent(282/299): loss=inf, w0=1.7119846730200213e+211, w1=7.7917105392111025e+211\n",
      "Gradient Descent(283/299): loss=inf, w0=-5.194988343862602e+211, w1=-1.2929208511436766e+212\n",
      "Gradient Descent(284/299): loss=inf, w0=3.0581721468034577e+212, w1=5.862421111235759e+212\n",
      "Gradient Descent(285/299): loss=inf, w0=-1.6100426710947442e+213, w1=-7.077197431976783e+213\n",
      "Gradient Descent(286/299): loss=inf, w0=1.010775511256459e+214, w1=2.8076195919001215e+214\n",
      "Gradient Descent(287/299): loss=inf, w0=-5.073707072587585e+214, w1=-2.1530310743476055e+215\n",
      "Gradient Descent(288/299): loss=inf, w0=9.047209842642637e+215, w1=3.6065291125257977e+216\n",
      "Gradient Descent(289/299): loss=inf, w0=-5.907896326645234e+216, w1=-3.2060881983837e+216\n",
      "Gradient Descent(290/299): loss=inf, w0=3.8316633590065636e+217, w1=1.294675015517489e+218\n",
      "Gradient Descent(291/299): loss=inf, w0=-2.2108095030168575e+218, w1=-9.081228340152567e+218\n",
      "Gradient Descent(292/299): loss=inf, w0=5.771523055316341e+219, w1=1.7069689182838825e+220\n",
      "Gradient Descent(293/299): loss=inf, w0=-4.808045700835217e+220, w1=-1.4448625100816672e+221\n",
      "Gradient Descent(294/299): loss=inf, w0=4.236295793593252e+221, w1=2.2140639308302202e+222\n",
      "Gradient Descent(295/299): loss=inf, w0=-1.602769634603951e+222, w1=-3.8651337110596087e+222\n",
      "Gradient Descent(296/299): loss=inf, w0=3.1053465864937794e+222, w1=1.4967331173331312e+223\n",
      "Gradient Descent(297/299): loss=inf, w0=5.427347592859555e+223, w1=2.7080797788384016e+224\n",
      "Gradient Descent(298/299): loss=inf, w0=-4.9007550958742544e+225, w1=-2.4504334881130415e+226\n",
      "Gradient Descent(299/299): loss=inf, w0=3.654641765870627e+226, w1=5.839001062803063e+226\n"
     ]
    }
   ],
   "source": [
    "loss_mse_sgd, w_mse_sgd = mean_squared_error_sgd(y_processed, tX, np.zeros(len(tX[0])), 300, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [ 3.65464177e+226  5.83900106e+226 -1.47101686e+227 -6.42822596e+226\n",
      "  9.55458055e+215  4.15034726e+226  3.65417035e+226  3.64952498e+226\n",
      "  7.30918924e+226  3.65464119e+226  2.24129594e+227  1.82787638e+227\n",
      "  4.14498066e+226  3.64952497e+226  4.14498605e+226  3.65464177e+226\n",
      "  1.26617631e+227  1.00459101e+226  3.65464177e+226  4.15014941e+226\n",
      "  5.43272905e+223  3.65464657e+226] \n",
      " Loss =  inf \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.09400982168649387 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.9091569882278581\n"
     ]
    }
   ],
   "source": [
    "y_test_sgd = tX_test.dot(w_mse_sgd)\n",
    "y_test_rounded_sgd = np.where(y_test_sgd > 0.5, 1, 0)\n",
    "\n",
    "print('weights = \\n', w_mse_sgd,'\\n Loss = ', loss_mse_sgd,'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.sum(y_processed)/len(y_processed), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.sum(y_test_rounded_sgd)/len(y_test_rounded_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ls, w_ls = least_squares(y_processed, tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [ 2.88270844e-01  3.34614524e-02  9.21278281e-04 -1.44338604e-04\n",
      " -8.63464839e-03 -7.29891263e-03 -3.81292266e-02 -1.86529550e-01\n",
      " -2.16648865e-02 -5.30251829e-02  5.15199746e-04 -3.45976029e-03\n",
      " -4.98852632e-02 -2.27719764e-02 -3.36538051e-02  1.34630336e-02\n",
      "  1.11543984e-02 -1.18266627e-01  1.93466354e-02 -3.03311985e-03\n",
      " -4.04505447e-03 -4.58582529e-03] \n",
      " Loss =  0.036280746857273415 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.09400982168649387 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.0036185509994469465\n"
     ]
    }
   ],
   "source": [
    "y_test_ls = tX_test.dot(w_ls)\n",
    "y_test_ls = np.where(y_test_ls > 0.5, 1, 0)\n",
    "\n",
    "print('weights = \\n', w_ls,'\\n Loss = ', loss_ls,'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.sum(y_processed)/len(y_processed), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.sum(y_test_ls)/len(y_test_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ridge, w_ridge = ridge_regression(y_processed, tX, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights = \n",
      " [ 3.11737536e-03  3.46261813e-02  1.98772957e-03  1.35255466e-04\n",
      " -1.07511427e-03 -5.01489595e-04 -2.27049733e-02 -2.85666601e-02\n",
      " -1.75867462e-02 -2.91627752e-02  1.83612644e-03 -4.09446645e-03\n",
      " -1.72798416e-02 -1.44256542e-02 -2.09755449e-02  5.41065993e-03\n",
      "  1.49365519e-02 -7.29866288e-05  6.12963375e-03  7.61692000e-04\n",
      "  5.64658737e-04  1.23394440e-05] \n",
      " Loss =  0.037031163716580585 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.09400982168649387 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.0\n"
     ]
    }
   ],
   "source": [
    "y_test_ridge = tX_test.dot(w_ridge)\n",
    "y_test_ridge = np.where(y_test_ridge > 0.5, 1, 0)\n",
    "\n",
    "print('weights = \\n', w_ridge,'\\n Loss = ', loss_ridge,'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.sum(y_processed)/len(y_processed), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.sum(y_test_ridge)/len(y_test_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ridge & least square polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\croge\\OneDrive\\Bureau\\ML\\project1\\dataset\\exploratory_p1.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/croge/OneDrive/Bureau/ML/project1/dataset/exploratory_p1.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loss_polyridge, w_polyridge \u001b[39m=\u001b[39m poly_ridge_ls(y_processed, tX, lambda_\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, ridge_y\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, degree \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/croge/OneDrive/Bureau/ML/project1/dataset/exploratory_p1.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y_test_polyridge \u001b[39m=\u001b[39m tX_test\u001b[39m.\u001b[39mdot(w_polyridge)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/croge/OneDrive/Bureau/ML/project1/dataset/exploratory_p1.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y_test_polyridge \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(y_test_ridge \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\croge\\OneDrive\\Bureau\\ML\\project1\\dataset\\exploratory_p1.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/croge/OneDrive/Bureau/ML/project1/dataset/exploratory_p1.ipynb#X51sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m rmse, weights\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/croge/OneDrive/Bureau/ML/project1/dataset/exploratory_p1.ipynb#X51sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m \u001b[39melif\u001b[39;00m ls_y \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m ridge_y \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/croge/OneDrive/Bureau/ML/project1/dataset/exploratory_p1.ipynb#X51sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m     tx_poly \u001b[39m=\u001b[39m build_poly(tx, degree)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/croge/OneDrive/Bureau/ML/project1/dataset/exploratory_p1.ipynb#X51sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m     weights, mse \u001b[39m=\u001b[39m ridge_regression(y, tx_poly, lambda_)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/croge/OneDrive/Bureau/ML/project1/dataset/exploratory_p1.ipynb#X51sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m     rmse \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mmse)\n",
      "\u001b[1;32mc:\\Users\\croge\\OneDrive\\Bureau\\ML\\project1\\dataset\\exploratory_p1.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/croge/OneDrive/Bureau/ML/project1/dataset/exploratory_p1.ipynb#X51sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, z \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(x):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/croge/OneDrive/Bureau/ML/project1/dataset/exploratory_p1.ipynb#X51sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(degree\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/croge/OneDrive/Bureau/ML/project1/dataset/exploratory_p1.ipynb#X51sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m         poly_x[i,j] \u001b[39m=\u001b[39m z\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mj\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/croge/OneDrive/Bureau/ML/project1/dataset/exploratory_p1.ipynb#X51sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m \u001b[39mreturn\u001b[39;00m poly_x\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "loss_polyridge, w_polyridge = poly_ridge_ls(y_processed, tX, lambda_=0.1, ridge_y=True, degree = 5)\n",
    "y_test_polyridge = tX_test.dot(w_polyridge)\n",
    "y_test_polyridge = np.where(y_test_ridge > 0.5, 1, 0)\n",
    "\n",
    "print('weights = \\n', w_polyridge,'\\n Loss = ', loss_polyridge,'\\n*****************************************************************************',\n",
    "      ' \\n Train sample : \\n', 'Heart attack rate = ', np.sum(y_processed)/len(y_processed), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.sum(y_test_polyridge)/len(y_test_polyridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=131971.75744271078, w0=-3864.925, w1=-8937.35\n",
      "Gradient Descent(1/99): loss=13220108440.281887, w0=-2969.9750000000004, w1=-5921.35\n",
      "Gradient Descent(2/99): loss=8796184655.56768, w0=-2075.0250000000005, w1=-2905.3500000000004\n",
      "Gradient Descent(3/99): loss=inf, w0=-1181.1750000000006, w1=107.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\croge\\AppData\\Local\\Temp\\ipykernel_24832\\1574709863.py:153: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n",
      "C:\\Users\\croge\\AppData\\Local\\Temp\\ipykernel_24832\\1574709863.py:148: RuntimeWarning: overflow encountered in exp\n",
      "  loss = np.sum(np.log(1 + np.exp(pred))) - y.T.dot(pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4/99): loss=inf, w0=-1773.76871250558, w1=-2479.2658548514974\n",
      "Gradient Descent(5/99): loss=6315118752.377287, w0=-878.8187125055799, w1=536.7341451485026\n",
      "Gradient Descent(6/99): loss=inf, w0=-10.11861971797964, w1=3467.6344235113033\n",
      "Gradient Descent(7/99): loss=inf, w0=-8205.589528150862, w1=-16649.306846080854\n",
      "Gradient Descent(8/99): loss=27561854441.0885, w0=-7310.639528150862, w1=-13633.306846080854\n",
      "Gradient Descent(9/99): loss=23137930656.37429, w0=-6415.689528150862, w1=-10617.306846080854\n",
      "Gradient Descent(10/99): loss=18714006871.660084, w0=-5520.7395281508625, w1=-7601.306846080854\n",
      "Gradient Descent(11/99): loss=14290083086.945877, w0=-4625.789528150863, w1=-4585.306846080854\n",
      "Gradient Descent(12/99): loss=9866159302.23167, w0=-3730.839528150863, w1=-1569.3068460808536\n",
      "Gradient Descent(13/99): loss=inf, w0=-3105.290240828228, w1=318.391122284851\n",
      "Gradient Descent(14/99): loss=inf, w0=-3005.294878538777, w1=106.22256529202349\n",
      "Gradient Descent(15/99): loss=5262963072.999373, w0=-2110.3448785387773, w1=3122.2225652920233\n",
      "Gradient Descent(16/99): loss=inf, w0=-2271.4435844527848, w1=1946.5755472207713\n",
      "Gradient Descent(17/99): loss=inf, w0=-1392.8019614142881, w1=4908.400426265672\n",
      "Gradient Descent(18/99): loss=inf, w0=-3156.885515333374, w1=401.38098748237735\n",
      "Gradient Descent(19/99): loss=5463175408.186721, w0=-2261.9355153333736, w1=3417.3809874823773\n",
      "Gradient Descent(20/99): loss=inf, w0=-2313.8054503210215, w1=2579.7223297480923\n",
      "Gradient Descent(21/99): loss=inf, w0=-1425.2554503210213, w1=5572.822329748093\n",
      "Gradient Descent(22/99): loss=inf, w0=-3740.8944073926664, w1=-560.7061808592061\n",
      "Gradient Descent(23/99): loss=7752278723.412512, w0=-2845.944407392666, w1=2455.293819140794\n",
      "Gradient Descent(24/99): loss=inf, w0=-1954.394407392666, w1=5455.043819140794\n",
      "Gradient Descent(25/99): loss=inf, w0=-3188.795061715685, w1=851.1412622036287\n",
      "Gradient Descent(26/99): loss=7999101676.485986, w0=-2293.845061715685, w1=3867.1412622036287\n",
      "Gradient Descent(27/99): loss=inf, w0=-1422.8950617156847, w1=6800.341262203629\n",
      "Gradient Descent(28/99): loss=inf, w0=-5216.54535133784, w1=-2948.55383035411\n",
      "Gradient Descent(29/99): loss=13000115717.496353, w0=-4321.59535133784, w1=67.44616964589022\n",
      "Gradient Descent(30/99): loss=8576191932.782145, w0=-3426.64535133784, w1=3083.44616964589\n",
      "Gradient Descent(31/99): loss=inf, w0=-2623.245351339284, w1=5688.646169644402\n",
      "Gradient Descent(32/99): loss=inf, w0=-3083.1956616322236, w1=3479.0955325853656\n",
      "Gradient Descent(33/99): loss=6244745304.290678, w0=-2188.2456616322233, w1=6495.095532585366\n",
      "Gradient Descent(34/99): loss=inf, w0=-1540.4449037841146, w1=8715.847091900301\n",
      "Gradient Descent(35/99): loss=inf, w0=-6509.617217053037, w1=-4965.211967029007\n",
      "Gradient Descent(36/99): loss=19315786330.927223, w0=-5614.667217053037, w1=-1949.2119670290067\n",
      "Gradient Descent(37/99): loss=14891862546.213017, w0=-4719.7172170530375, w1=1066.7880329709933\n",
      "Gradient Descent(38/99): loss=10467938761.49881, w0=-3824.7672170530377, w1=4082.7880329709933\n",
      "Gradient Descent(39/99): loss=6044014976.784602, w0=-2929.817217053038, w1=7098.788032970993\n",
      "Gradient Descent(40/99): loss=inf, w0=-2757.383088782206, w1=7099.157547772722\n",
      "Gradient Descent(41/99): loss=inf, w0=-1873.9330732841308, w1=10071.457609765022\n",
      "Gradient Descent(42/99): loss=inf, w0=-5272.046150718505, w1=-111.49260755746582\n",
      "Gradient Descent(43/99): loss=14444287626.680359, w0=-4377.096150718505, w1=2904.507392442534\n",
      "Gradient Descent(44/99): loss=10020363841.966152, w0=-3482.146150718505, w1=5920.507392442534\n",
      "Gradient Descent(45/99): loss=5596440057.2519455, w0=-2587.196150718505, w1=8936.507392442534\n",
      "Gradient Descent(46/99): loss=inf, w0=-1972.9461092770807, w1=10812.307578571272\n",
      "Gradient Descent(47/99): loss=inf, w0=-5540.463529670069, w1=69.61439886011249\n",
      "Gradient Descent(48/99): loss=15081786515.701265, w0=-4645.513529670069, w1=3085.6143988601125\n",
      "Gradient Descent(49/99): loss=10657862730.98706, w0=-3750.5635296700693, w1=6101.6143988601125\n",
      "Gradient Descent(50/99): loss=6233938946.272853, w0=-2855.6135296700695, w1=9117.614398860112\n",
      "Gradient Descent(51/99): loss=inf, w0=-2026.5136749313615, w1=11849.563678803635\n",
      "Gradient Descent(52/99): loss=inf, w0=-5957.371417693441, w1=237.9495645053703\n",
      "Gradient Descent(53/99): loss=15715135546.794668, w0=-5062.421417693441, w1=3253.9495645053703\n",
      "Gradient Descent(54/99): loss=11291211762.080462, w0=-4167.471417693441, w1=6269.94956450537\n",
      "Gradient Descent(55/99): loss=6867287977.366255, w0=-3272.5214176934414, w1=9285.94956450537\n",
      "Gradient Descent(56/99): loss=inf, w0=-2437.7214007726157, w1=12028.999632188674\n",
      "Gradient Descent(57/99): loss=inf, w0=-4373.046897357912, w1=5298.338406449865\n",
      "Gradient Descent(58/99): loss=9623176012.769674, w0=-3478.096897357912, w1=8314.338406449864\n",
      "Gradient Descent(59/99): loss=5199252228.055467, w0=-2583.146897357912, w1=11330.338406449864\n",
      "Gradient Descent(60/99): loss=inf, w0=-2584.468210288614, w1=11465.339937093673\n",
      "Gradient Descent(61/99): loss=inf, w0=-3953.5875000444144, w1=6331.676083034748\n",
      "Gradient Descent(62/99): loss=8072488118.586774, w0=-3058.637500044414, w1=9347.676083034748\n",
      "Gradient Descent(63/99): loss=inf, w0=-2181.187500044407, w1=12296.276083034778\n",
      "Gradient Descent(64/99): loss=inf, w0=-6186.623741933889, w1=2320.411234083691\n",
      "Gradient Descent(65/99): loss=12380440163.712187, w0=-5291.673741933889, w1=5336.411234083691\n",
      "Gradient Descent(66/99): loss=7956516378.997982, w0=-4396.723741933889, w1=8352.411234083691\n",
      "Gradient Descent(67/99): loss=inf, w0=-4086.561117728606, w1=8879.563368286676\n",
      "Gradient Descent(68/99): loss=4106236703.308034, w0=-3191.6111177286057, w1=11895.563368286676\n",
      "Gradient Descent(69/99): loss=inf, w0=-3844.019597519079, w1=8970.542871287991\n",
      "Gradient Descent(70/99): loss=6260890764.032343, w0=-2949.0695975190793, w1=11986.542871287991\n",
      "Gradient Descent(71/99): loss=inf, w0=-2469.020374238728, w1=13642.342781464706\n",
      "Gradient Descent(72/99): loss=inf, w0=-6601.929696205954, w1=1766.3911926194633\n",
      "Gradient Descent(73/99): loss=16480535336.62592, w0=-5706.9796962059545, w1=4782.391192619463\n",
      "Gradient Descent(74/99): loss=12056611551.91171, w0=-4812.029696205955, w1=7798.391192619463\n",
      "Gradient Descent(75/99): loss=7632687767.197505, w0=-3917.079696205955, w1=10814.391192619463\n",
      "Gradient Descent(76/99): loss=3208763982.483298, w0=-3022.129696205955, w1=13830.391192619463\n",
      "Gradient Descent(77/99): loss=inf, w0=-4406.3284367264405, w1=8579.493410175546\n",
      "Gradient Descent(78/99): loss=8226081536.410397, w0=-3511.3784367264407, w1=11595.493410175546\n",
      "Gradient Descent(79/99): loss=inf, w0=-2631.078436725844, w1=14553.693410177933\n",
      "Gradient Descent(80/99): loss=inf, w0=-5701.50405854499, w1=6402.203170938539\n",
      "Gradient Descent(81/99): loss=9940506554.332188, w0=-4806.55405854499, w1=9418.20317093854\n",
      "Gradient Descent(82/99): loss=5516582769.617982, w0=-3911.60405854499, w1=12434.20317093854\n",
      "Gradient Descent(83/99): loss=inf, w0=-4180.087817452211, w1=10809.635930389812\n",
      "Gradient Descent(84/99): loss=5682728159.252517, w0=-3285.1378174522115, w1=13825.635930389812\n",
      "Gradient Descent(85/99): loss=inf, w0=-2641.2415204405656, w1=15941.72483351055\n",
      "Gradient Descent(86/99): loss=inf, w0=-7672.344242551502, w1=1785.6482007156483\n",
      "Gradient Descent(87/99): loss=19226113133.951717, w0=-6777.394242551502, w1=4801.648200715648\n",
      "Gradient Descent(88/99): loss=14802189349.23751, w0=-5882.444242551503, w1=7817.648200715648\n",
      "Gradient Descent(89/99): loss=10378265564.523304, w0=-4987.494242551503, w1=10833.648200715648\n",
      "Gradient Descent(90/99): loss=5954341779.809097, w0=-4092.544242551503, w1=13849.648200715648\n",
      "Gradient Descent(91/99): loss=inf, w0=-3990.694976448848, w1=13555.445284675628\n",
      "Gradient Descent(92/99): loss=inf, w0=-3101.8449764488482, w1=16546.59528467563\n",
      "Gradient Descent(93/99): loss=inf, w0=-5514.665233349021, w1=8664.244894905605\n",
      "Gradient Descent(94/99): loss=10857854726.144102, w0=-4619.715233349021, w1=11680.244894905605\n",
      "Gradient Descent(95/99): loss=6433930941.429899, w0=-3724.7652333490214, w1=14696.244894905605\n",
      "Gradient Descent(96/99): loss=inf, w0=-2893.3652333490227, w1=17469.694894905602\n",
      "Gradient Descent(97/99): loss=inf, w0=-7541.980053653177, w1=4138.649517340264\n",
      "Gradient Descent(98/99): loss=18072235090.18782, w0=-6647.030053653177, w1=7154.649517340264\n",
      "Gradient Descent(99/99): loss=13648311305.473618, w0=-5752.080053653177, w1=10170.649517340264\n",
      "[-462047.37221369 -494920.46334305 -450852.05233523 ... -730529.10882283\n",
      " -366122.91949441 -418350.52820739]\n",
      "weights = \n",
      " [ -5752.08005365  10170.64951734  -9189.23461679  -9497.35384674\n",
      "  -1160.1785399   -5471.73755975 -14733.53539755 -12980.87872664\n",
      " -20589.66361899 -15326.12115531 -23403.51286278 -21122.2409379\n",
      " -11235.25128689  -9656.74133377 -14211.64123565  -4388.83248728\n",
      " -11637.00561508  -1381.88874771  -4540.99359641  -1183.92999036\n",
      "   -719.61127051   -418.09412618] \n",
      " Loss =  13648311305.473618 \n",
      "*****************************************************************************  \n",
      " Train sample : \n",
      " Heart attack rate =  0.09400982168649387 \n",
      " \n",
      " Test sample : \n",
      " Heart attack rate =  0.0\n"
     ]
    }
   ],
   "source": [
    "loss_log, w_log = logistic_regression(y_processed, tX, 100, 0.05, np.zeros(len(tX[0])))\n",
    "\n",
    "y_test_log = tX_test.dot(w_log)\n",
    "print(y_test_log)\n",
    "y_test_log = np.where(y_test_log > 0.5, 1, 0)\n",
    "\n",
    "print('weights = \\n', w_log,'\\n Loss = ', loss_log,'\\n*****************************************************************************',\n",
    "        ' \\n Train sample : \\n', 'Heart attack rate = ', np.sum(y_processed)/len(y_processed), '\\n \\n Test sample : \\n', 'Heart attack rate = ', np.sum(y_test_log)/len(y_test_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
